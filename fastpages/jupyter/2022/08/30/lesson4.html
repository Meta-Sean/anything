<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lesson 4 / Chapter 10 questions | Terps’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lesson 4 / Chapter 10 questions" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Chapter 10 Q/A" />
<meta property="og:description" content="Chapter 10 Q/A" />
<link rel="canonical" href="https://terpsfi.xyz/anything/fastpages/jupyter/2022/08/30/lesson4.html" />
<meta property="og:url" content="https://terpsfi.xyz/anything/fastpages/jupyter/2022/08/30/lesson4.html" />
<meta property="og:site_name" content="Terps’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-30T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lesson 4 / Chapter 10 questions" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-30T00:00:00-05:00","datePublished":"2022-08-30T00:00:00-05:00","description":"Chapter 10 Q/A","headline":"Lesson 4 / Chapter 10 questions","mainEntityOfPage":{"@type":"WebPage","@id":"https://terpsfi.xyz/anything/fastpages/jupyter/2022/08/30/lesson4.html"},"url":"https://terpsfi.xyz/anything/fastpages/jupyter/2022/08/30/lesson4.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/anything/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://terpsfi.xyz/anything/feed.xml" title="Terps's Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HC729WK5N2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HC729WK5N2');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/anything/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/anything/">Terps&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/anything/about/">About Me</a><a class="page-link" href="/anything/search/">Search</a><a class="page-link" href="/anything/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lesson 4 / Chapter 10 questions</h1><p class="page-description">Chapter 10 Q/A</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-30T00:00:00-05:00" itemprop="datePublished">
        Aug 30, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/anything/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/anything/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/Meta-Sean/anything/tree/master/_notebooks/2022-08-30-lesson4.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/anything/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/Meta-Sean/anything/master?filepath=_notebooks%2F2022-08-30-lesson4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/anything/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/Meta-Sean/anything/blob/master/_notebooks/2022-08-30-lesson4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/anything/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2FMeta-Sean%2Fanything%2Fblob%2Fmaster%2F_notebooks%2F2022-08-30-lesson4.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/anything/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-08-30-lesson4.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "self-supervised learning"?</p>
<ul>
<li>Training s model using 'labels' that are naturally part of the input data, rather than requiring separate external labels.</li>
<li>For instance: training a model to predict the next word in text.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is a "language model"?</p>
<ul>
<li>A model trained to predict the next word in text</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why is a language model considered self-supervised?</p>
<ul>
<li>We are not using external labels, but the input data itself.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are self-supervised models usually used for?</p>
<ul>
<li>NLP, Computer Vision </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we fine-tune language models?</p>
<ul>
<li>Task specific corpus of information</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are the three steps to create a state-of-the-art text classifier?</p>
<ul>
<li>Pipeline: Transfer Learning &gt; Train on our task specific data &gt; classification </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?</p>
<ul>
<li>To fine-tune our language model to predict the next word of a movie review</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are the three steps to prepare your data for a language model?</p>
<ul>
<li>Training Set, Validation Set, and Testing Set</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "tokenization"? Why do we need it?</p>
<ul>
<li>Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Name three different approaches to tokenization.</p>
<ul>
<li>Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning "don't" into "do n't"). Generally, punctuation marks are also split into separate tokens.</li>
<li>Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, "occasion" might be tokenized as "o c ca sion."</li>
<li>Character-based: Split a sentence into its individual characters.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is xxbos?</p>
<ul>
<li>Denotes the 'beginning of stream' aka the start of the sentence or text block.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>List four rules that fastai applies to text during tokenization.</p>
<ul>
<li>use defaults.text_proc_rules to check out the default rules</li>
<li>fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses)</li>
<li>replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it's repeated, then the character</li>
<li>replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it's repeated, then the word.</li>
<li>spec_add_spaces: Adds spaces around / and #</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?</p>
<ul>
<li>Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "numericalization"?</p>
<ul>
<li>Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why might there be words that are replaced with the "unknown word" token?</p>
<ul>
<li>All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with "unknown word"</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)</p>
<ul>
<li>a. The dataset is split into 64 mini-streams (batch size)</li>
<li>b. Each batch has 64 rows (batch size) and 64 columns (sequence length)</li>
<li>c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64)</li>
<li>d. The second row of the first batch contains the beginning of the second mini-stream</li>
<li>e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we need padding for text classification? Why don't we need it for language modeling?</p>
<ul>
<li>To collate the batch, in language models it is not needed since all documents are concatenated.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What does an embedding matrix for NLP contain? What is its shape?</p>
<ul>
<li>Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "perplexity"?</p>
<ul>
<li>Metric in NLP for language models. It is the exponential of the loss.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we have to pass the vocabulary of the language model to the classifier data block?</p>
<ul>
<li>To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "gradual unfreezing"?</p>
<ul>
<li>This refers to unfreezing one laying at a time and fine-tuning the pre-trained model.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why is text generation always likely to be ahead of automatic identification of machine-generated texts?</p>
<ul>
<li>The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead.</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Meta-Sean/anything"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/anything/fastpages/jupyter/2022/08/30/lesson4.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/anything/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/anything/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/anything/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Meta-Sean" target="_blank" title="Meta-Sean"><svg class="svg-icon grey"><use xlink:href="/anything/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
