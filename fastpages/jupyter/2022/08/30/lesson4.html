<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lesson 4 / Chapter 10 questions</h1><p class="page-description">NLP Deep Dive: RNNs</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-30T00:00:00-05:00" itemprop="datePublished">
        Aug 30, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/Meta-Sean/anything/tree/master/_notebooks/2022-08-30-lesson4.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/Meta-Sean/anything/master?filepath=_notebooks%2F2022-08-30-lesson4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/Meta-Sean/anything/blob/master/_notebooks/2022-08-30-lesson4.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2FMeta-Sean%2Fanything%2Fblob%2Fmaster%2F_notebooks%2F2022-08-30-lesson4.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-08-30-lesson4.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "self-supervised learning"?</p>
<ul>
<li>Training s model using 'labels' that are naturally part of the input data, rather than requiring separate external labels.</li>
<li>For instance: training a model to predict the next word in text.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">test</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>4</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is a "language model"?</p>
<ul>
<li>A model trained to predict the next word in text</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why is a language model considered self-supervised?</p>
<ul>
<li>We are not using external labels, but the input data itself.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are self-supervised models usually used for?</p>
<ul>
<li>NLP, Computer Vision </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we fine-tune language models?</p>
<ul>
<li>Task specific corpus of information</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are the three steps to create a state-of-the-art text classifier?</p>
<ul>
<li>Pipeline: Transfer Learning &gt; Train on our task specific data &gt; classification </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?</p>
<ul>
<li>To fine-tune our language model to predict the next word of a movie review</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What are the three steps to prepare your data for a language model?</p>
<ul>
<li>Training Set, Validation Set, and Testing Set</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "tokenization"? Why do we need it?</p>
<ul>
<li>Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Name three different approaches to tokenization.</p>
<ul>
<li>Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning "don't" into "do n't"). Generally, punctuation marks are also split into separate tokens.</li>
<li>Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, "occasion" might be tokenized as "o c ca sion."</li>
<li>Character-based: Split a sentence into its individual characters.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is xxbos?</p>
<ul>
<li>Denotes the 'beginning of stream' aka the start of the sentence or text block.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>List four rules that fastai applies to text during tokenization.</p>
<ul>
<li>use defaults.text_proc_rules to check out the default rules</li>
<li>fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses)</li>
<li>replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it's repeated, then the character</li>
<li>replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it's repeated, then the word.</li>
<li>spec_add_spaces: Adds spaces around / and #</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?</p>
<ul>
<li>Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "numericalization"?</p>
<ul>
<li>Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why might there be words that are replaced with the "unknown word" token?</p>
<ul>
<li>All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with "unknown word"</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)</p>
<ul>
<li>a. The dataset is split into 64 mini-streams (batch size)</li>
<li>b. Each batch has 64 rows (batch size) and 64 columns (sequence length)</li>
<li>c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64)</li>
<li>d. The second row of the first batch contains the beginning of the second mini-stream</li>
<li>e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we need padding for text classification? Why don't we need it for language modeling?</p>
<ul>
<li>To collate the batch, in language models it is not needed since all documents are concatenated.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What does an embedding matrix for NLP contain? What is its shape?</p>
<ul>
<li>Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "perplexity"?</p>
<ul>
<li>Metric in NLP for language models. It is the exponential of the loss.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do we have to pass the vocabulary of the language model to the classifier data block?</p>
<ul>
<li>To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is "gradual unfreezing"?</p>
<ul>
<li>This refers to unfreezing one laying at a time and fine-tuning the pre-trained model.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why is text generation always likely to be ahead of automatic identification of machine-generated texts?</p>
<ul>
<li>The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead.</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Meta-Sean/anything"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastpages/jupyter/2022/08/30/lesson4.html" hidden></a>
</article>
