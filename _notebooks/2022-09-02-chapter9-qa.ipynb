{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Lesson 5 / Chapter 9 questions\"\n",
    "> \"Tabular Modeling Deep Dive\"\n",
    "\n",
    "- toc: false\n",
    "- hide: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a continuous variable?\n",
    "- numerical data that have a wide range of 'continuous' values (ex: age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a categorical variable?\n",
    "- discrete levels that correspond to different categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide two of the words that are used for the possible values of a categorical variable.\n",
    "- Levels or categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"dense layer\"?\n",
    "- linear layer and one-hot encoding layers represent inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do entity embeddings reduce memory usage and speed up neural networks?\n",
    "- Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of datasets are entity embeddings especially useful for?\n",
    "- When the datasets have features with high levels of cardinality (high dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the two main families of machine learning algorithms?\n",
    "- Ensembles of decision trees\n",
    "- Multilayered neural networks learned with SGD(i.e, shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\n",
    " - The data is ordinal\n",
    " - df.cat.set_categories(['columns']*, ordered=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize what a decision tree algorithm does.\n",
    "- Asks a series of binary questions about the data and splits until a leaf node where the prediction is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\n",
    "- While a date can be treated as ordinal, some dates are qualitatively different from others in ways that might be useful to the systems we are modeling.\n",
    "- We replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\n",
    "- No, for time-series data we need our model to be able to predict the future\n",
    "- We want the validation set to be later in time than the training set.\n",
    "- To do this is use np.where, a useful function that returns(as the first element of a tuple) the indices of all True values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is pickle and what is it useful for?\n",
    "- Python's system to save a Python object\n",
    "- Allows of to 'freeze' a file \n",
    "- Side note: not a secure process and should trust files you unpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are mse, samples, and values calculated in the decision tree drawn in this chapter?\n",
    "- mse is the square root of the mean of the pred minus actual squared\n",
    "- samples where just 500 random permutations of indexes \n",
    "- Values is average value for all the records in the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we deal with outliers, before building a decision tree?\n",
    "- Check for errors in your data, the outliers might be an error(ie. in the bulldozer competition a large number of reported sales happened during 1000)\n",
    "- We can take the log of the numerical / continuous column to smooth the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we handle categorical variables in a decision tree?\n",
    "- Decision trees naturally split data in a way that gives the categorical variables context, but we can also use pandas get_dummies to replace a single categorical variable with multiple one_hot_encoded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is bagging?\n",
    "- An ensemble technique that aggregators multiple predictions from many models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between max_samples and max_features when creating a random forest?\n",
    "- max_samples defines how many rows to sample for training each tree\n",
    "- max_features defines how many columns to sample at each split point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\n",
    "- n_estimators is the number of trees we want\n",
    "- You can set the number as high as you have time to train, it will get more accurate with diminishing returns\n",
    "- Yes it can lead to over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section \"Creating a Random Forest\", just after <>, why did preds.mean(0) give the same result as our random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"out-of-bag-error\"?\n",
    "- OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row's error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of reasons why a model's validation set error might be worse than the OOB error. How could you test your hypotheses?\n",
    "- Beneficial when we have only a small amount of training data\n",
    "- We can compare then to the training labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why random forests are well suited to answering each of the following question:\n",
    "- How confident are we in our predictions using a particular row of data?\n",
    "    - We can use the standard deviation of predictions across the trees, instead of just the means\n",
    "- For predicting with a particular row of data, what were the most important factors, and how did they influence that - prediction?\n",
    "    - Year Made and Product Size - use treeinterpreter package to check how the prediction changes as it goes - waterfall plots \n",
    "- Which columns are the strongest predictors?\n",
    "    - The feature importance for our model show that the first few important columns have much higher importance scores that the rest. \n",
    "- How do predictions vary as we vary these columns?\n",
    "    - Look at partial dependence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the purpose of removing unimportant variables?\n",
    "-  Makes for a simpler, more interpretable model that is easier to maintain and roll out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a good type of plot for showing tree interpreter results?\n",
    "- cluster_columns\n",
    "- waterfall plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the \"extrapolation problem\"?\n",
    "- Hard fora model to extrapolate to data that's outside the domain of the training data. This is particularly important to random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you tell if your test or validation set is distributed in a different way than your training set?\n",
    "- We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\n",
    "- This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"boosting\"?\n",
    "- We train a model that under-fits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of al the models to get the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we use embeddings with a random forest? Would we expect this to help?\n",
    "- Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might we not always use a neural net for tabular modeling?\n",
    "- We might not use a NN because they are the hardest and longest to train, and more opaque. Instead, random forests should be the first choice/baseline, and neural networks could be tried to improve these results or add to an ensemble. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
