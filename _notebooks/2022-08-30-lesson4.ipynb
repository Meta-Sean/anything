{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"self-supervised learning\"?\n",
    "- Training s model using 'labels' that are naturally part of the input data, rather than requiring separate external labels.\n",
    "- For instance: training a model to predict the next word in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"language model\"?\n",
    "- A model trained to predict the next word in text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a language model considered self-supervised?\n",
    "- We are not using external labels, but the input data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are self-supervised models usually used for?\n",
    "- NLP, Computer Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we fine-tune language models?\n",
    "- Task specific corpus of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the three steps to create a state-of-the-art text classifier?\n",
    "- Pipeline: Transfer Learning > Train on our task specific data > classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
    "- To fine-tune our language model to predict the next word of a movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the three steps to prepare your data for a language model?\n",
    "- Training Set, Validation Set, and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"tokenization\"? Why do we need it?\n",
    "- Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name three different approaches to tokenization.\n",
    "- Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n",
    "- Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n",
    "- Character-based: Split a sentence into its individual characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is xxbos?\n",
    "- Denotes the 'beginning of stream' aka the start of the sentence or text block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List four rules that fastai applies to text during tokenization.\n",
    "- use defaults.text_proc_rules to check out the default rules\n",
    "- fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses)\n",
    "- replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it's repeated, then the character\n",
    "- replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it's repeated, then the word.\n",
    "- spec_add_spaces: Adds spaces around / and #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
    "- Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"numericalization\"?\n",
    "- Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might there be words that are replaced with the \"unknown word\" token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need padding for text classification? Why don't we need it for language modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an embedding matrix for NLP contain? What is its shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"perplexity\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have to pass the vocabulary of the language model to the classifier data block?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"gradual unfreezing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is text generation always likely to be ahead of automatic identification of machine-generated texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
