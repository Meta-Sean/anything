{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Lesson 4 / Chapter 10 questions\"\n",
    "> \"NLP Deep Dive: RNNs\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"self-supervised learning\"?\n",
    "- Training s model using 'labels' that are naturally part of the input data, rather than requiring separate external labels.\n",
    "- For instance: training a model to predict the next word in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 4\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"language model\"?\n",
    "- A model trained to predict the next word in text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a language model considered self-supervised?\n",
    "- We are not using external labels, but the input data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are self-supervised models usually used for?\n",
    "- NLP, Computer Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we fine-tune language models?\n",
    "- Task specific corpus of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the three steps to create a state-of-the-art text classifier?\n",
    "- Pipeline: Transfer Learning > Train on our task specific data > classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
    "- To fine-tune our language model to predict the next word of a movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the three steps to prepare your data for a language model?\n",
    "- Training Set, Validation Set, and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"tokenization\"? Why do we need it?\n",
    "- Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name three different approaches to tokenization.\n",
    "- Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n",
    "- Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n",
    "- Character-based: Split a sentence into its individual characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is xxbos?\n",
    "- Denotes the 'beginning of stream' aka the start of the sentence or text block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List four rules that fastai applies to text during tokenization.\n",
    "- use defaults.text_proc_rules to check out the default rules\n",
    "- fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses)\n",
    "- replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it's repeated, then the character\n",
    "- replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it's repeated, then the word.\n",
    "- spec_add_spaces: Adds spaces around / and #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
    "- Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"numericalization\"?\n",
    "- Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might there be words that are replaced with the \"unknown word\" token?\n",
    "- All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with \"unknown word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)\n",
    "- a. The dataset is split into 64 mini-streams (batch size)\n",
    "- b. Each batch has 64 rows (batch size) and 64 columns (sequence length)\n",
    "- c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64)\n",
    "- d. The second row of the first batch contains the beginning of the second mini-stream\n",
    "- e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need padding for text classification? Why don't we need it for language modeling?\n",
    "- To collate the batch, in language models it is not needed since all documents are concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an embedding matrix for NLP contain? What is its shape?\n",
    "- Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"perplexity\"?\n",
    "- Metric in NLP for language models. It is the exponential of the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
    "- To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"gradual unfreezing\"?\n",
    "- This refers to unfreezing one laying at a time and fine-tuning the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\n",
    "- The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
