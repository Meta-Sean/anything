{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Lesson 3 / Chapter 4 questions\"\n",
    "> \"Under the Hood: Training a Digit Classifier\"\n",
    "\n",
    "> shout out to [this forum post](https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253) for helping me fill in the gaps\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is a grayscale image represented on a computer? How about a color image?\n",
    "- Images are represented by arrays of pixels values representing content for the image\n",
    "-  Greyscale values range from 0-255, from white-black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n",
    "- Train & Valid -> 3 & 7\n",
    "- To test our model on out of sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "- Create an archetype for the class we want to identify (Defined as the pixel wise mean of all the values in the training set)\n",
    "- Calculate the pixel mean wise absolute difference -> classify image based upon lowest distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a list comprehension? Create one now that selects odd numbers from a list and doubles them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_double = [x * 2 for x in [1,2,3,4] if x % 2 == 0 ]\n",
    "odd_double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"rank-3 tensor\"?\n",
    "- Rank is the dimensionality of the tensor\n",
    "- The easiest way to identify the dimensionality is the number of indices you would need to reference a number within a tensor\n",
    "- a \"rank-3 tensor\" is a cuboid or stack of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "- Rank is the number of axis or dimensions in a tensor while shape is the size of each axis of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you get the rank from the shape?\n",
    "- The length of the tensor's shape is its rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are RMSE and L1 norm?\n",
    "- Root Mean Squared Error aka the L2 norm, and Mean absolute difference aka L1 norm, are two commonly used methods of measuring distance, focusing on the magnitudes of the differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "-  Pytorch and fastai libraries utilize C and CUDA to provide these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 3Ã—3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is broadcasting?\n",
    "-  dimension matching to allow operations, PyTorch implements these features to make it easier to perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "- Validation set, you want to test on out of sample data to see how the model will perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is SGD?\n",
    "- Stochastic Gradient Decent is the process we use to traverse a function to find the local minimum for space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does SGD use mini-batches?\n",
    "- computationally efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the seven steps in SGD for machine learning?\n",
    "- Initialize the parameters\n",
    "- Calculate the predictions\n",
    "- Calculate the loss\n",
    "- Calculate the gradients\n",
    "- Step the weights\n",
    "- Repeat the process\n",
    "- Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we initialize the weights in a model?\n",
    "- Randomness is recommended "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"loss\"?\n",
    "- A loss is a function that measures your predictions versus the results of the data, large deviations would indicate a large loss while the opposite would also be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we always use a high learning rate?\n",
    "- A high learning rate might skip over the minima for our loss space, while fast and computationally less expensive, it may be useful to dynamically adjust your step **** might be explaining step size fix later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"gradient\"?\n",
    "- The first derivative of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you need to know how to calculate gradients yourself?\n",
    "- No we use our handy tools provided by PyTorch, but it is helpful to understand whats happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we use accuracy as a loss function?\n",
    "- A loss for binomial outcomes (0, 1) would have create a loss function with a derivative 0\n",
    "- We need to be able to turn the dials and knobs on the weights and see the effect on the predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the sigmoid function. What is special about its shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "# plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between a loss function and a metric?\n",
    "- The metric is the accuracy of the actual result we want to achieve, the loss tells us how wel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the function to calculate new weights using a learning rate?\n",
    "- The optimizer step method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the DataLoader class do?\n",
    "- Takes any python collection and turns it into an iterator over many batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write pseudocode showing the basic steps taken in each epoch for SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in dl:\n",
    "#     pred = model(x)\n",
    "#     loss = loss_func(pred, y)\n",
    "#     loss.backward()\n",
    "#     parameters -= parameters.grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuple(a,b):\n",
    "    return list(zip(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does view do in PyTorch?\n",
    "- Changes the shape of the tensor without changing its content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "- The margin or padding on the for our output, keeps our model from predicting a zero value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the @ operator do in Python?\n",
    "- Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the backward method do?\n",
    "- Return the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have to zero the gradients?\n",
    "- To avoid adding the previously stored gradient to the current gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information do we have to pass to Learner?\n",
    "- DataLoader, the model, optimization function, and the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Python or pseudocode for the basic steps of a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show Python or pseudocode for the basic steps of a training loop.\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "for i in range(20):\n",
    "    train_epoch(model, lr, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is \"ReLU\"? Draw a plot of it for values from -2 to +2.\n",
    "- Rectified Linear Unit, basically just replaces any negative numbers with 0s in a linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an \"activation function\"?\n",
    "- function that adds non-linearity to our model to decouple each linear equation from the rest, allowing for more complex functions, see universal approximation theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between F.relu and nn.ReLU?\n",
    "- Python function versus a PyTorch module. Can be called as a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "- Allows for deeper models with less parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
