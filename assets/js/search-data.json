{
  
    
        "post0": {
            "title": "Pytorch üî•",
            "content": "Quick shout-out to FSDL for the awesome course, check them out if you want to get right into the weeds with professional full-stack deep learning. .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/12/pytorch.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/12/pytorch.html",
            "date": " ‚Ä¢ Oct 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch Lightning üå©Ô∏è",
            "content": "",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/12/lightning.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/12/lightning.html",
            "date": " ‚Ä¢ Oct 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Micrograd üîß",
            "content": "A blog implementation of Andrej Karpathy&#39;s micrograd video . Work in progress will continually add to this blog . Content: . 1. Derivative of a function . 2. Core Value Object . 3. Manual Backpropagation . 4. Backward function . 5. More operations . 6. Pytorch . . Lets get an intuitive understanding of what a derivative is . import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline . Lets use a simple quadratic function as an example . $f(x) = 3x^2 - 4x + 5$ . def f(x): return 3*x**2 - 4*x + 5 . f(3.0) . 20.0 . Take a look at the shape of the function, we can expect a parabola since we know its a quadratic function . xs = np.arange(-5, 5, 0.25) ys = f(xs) plt.plot(xs, ys) . [&lt;matplotlib.lines.Line2D at 0x7f572624d290&gt;] . We know want to think through what is the derivative of this function at different points x, let refresh with the definition of a derivative . $f&#39;(x) = lim_{h to 0} frac{f(x+h)-f(x)}{h}$ . You are basically trying to see the level of sensitivty the function responds with by bumping any x value at any point slightly by this small number h &lt;/br&gt; Intuitively how would you expect this function to respond if we nudged x = 3.0 by this small postitive number h? The amount the x value responds tells you the strength of the slope . h = 0.0001 x = 3.0 print(f&#39;slope of function at x = {x}, slope = {(f(x + h) - f(x)) / h}&#39;) . slope of function at x = 3.0, slope = 14.000300000063248 . Lets do a hacky implementation with more variables &lt;/br&gt; Look at the function a*b + c in relation to the variables we assigned, imagine if you nudged each variables by a tiny amount would that result in our output being increased or decreased? &lt;/br&gt; If we were to slightly nudge each of our input varibles by the tiny amount h(amount approaching 0) we can approximate the instataneous rate of change by looking at the difference before and after over the amount we nudged by, this will give us the slope. . h = 0.0001 #inputs a = 2.0 b = -3.0 c = 10.0 #We wanna find the derivative of d with respect to a,b,c d1 = a*b + c a += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 4.0 d2 3.999699999999999 slope -3.000000000010772 . Lets do it with b now . d1 = a*b + c b += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 3.999699999999999 d2 3.99990001 slope 2.0001000000124947 . And c... . d1 = a*b + c c += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 3.99990001 d2 4.00000001 slope 0.9999999999976694 . Hopefully this has helped build an inuitive sense of what this derivative is telling you about the function, but now we want to move to neural networks, which will be massive mathmatical expressions, so we need some structures to maintain these expressions, so we will build out a value object that can keep track of state and allow us to do expressions . . Core Value Object . class Value: def __init__(self, data): self.data = data def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data) return out def __mul__(self, other): out = Value(self.data * other.data) return out a = Value(2.0) b = Value(-3.0) a + b a * b . Value(data=-6.0) . We use double underscore or dunder methods so python knows what to interally when we use operators such as print, +, -, &lt;/br&gt; So when we call a + b above what is happening interally is a.__add__(b) with self as a and b as other. Similarly when we do a b, python is callinga.mul(b) . Cool so now we can do basic arthmetic now we need to add pointers to what see values produce other values and by what operations . class Value: def __init__(self, data, _children=(), _op=&#39;&#39;, label=&#39;&#39;): self.data = data self._prev = set(_children) self._op = _op self.label = label self.grad = 0.0 def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data, (self, other), &#39;+&#39;) return out def __mul__(self, other): out = Value(self.data * other.data, (self, other), &#39;*&#39;) return out a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L . Value(data=-8.0) . Now that we have the a way to store the parents, labels and operations we can visualize them with an expression graph, code below allows us to do that using a library called graphviz . from graphviz import Digraph def trace(root): # builds a set of all nodes and edges in a graph nodes, edges = set(), set() def build(v): if v not in nodes: nodes.add(v) for child in v._prev: edges.add((child, v)) build(child) build(root) return nodes, edges def draw_dot(root): dot = Digraph(format=&#39;svg&#39;, graph_attr={&#39;rankdir&#39;: &#39;LR&#39;}) # LR = left to right nodes, edges = trace(root) for n in nodes: uid = str(id(n)) # for any value in the graph, create a rectangular (&#39;record&#39;) node for it dot.node(name = uid, label = &quot;{ %s | data %.4f | grad %.4f}&quot; % (n.label, n.data, n.grad), shape=&#39;record&#39;) if n._op: # if this value is a result of some operation, create an op node for it dot.node(name = uid + n._op, label = n._op) # and connect this node to it dot.edge(uid + n._op, uid) for n1, n2 in edges: # connect n1 to the op node of n2 dot.edge(str(id(n1)), str(id(n2)) + n2._op) return dot . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140012270732304 f data &#45;2.0000 grad 0.0000 140012271510288* * 140012270732304&#45;&gt;140012271510288* 140012271199888 d data 4.0000 grad 0.0000 140012271199888&#45;&gt;140012271510288* 140012271199888+ + 140012271199888+&#45;&gt;140012271199888 140012132103120 c data 10.0000 grad 0.0000 140012132103120&#45;&gt;140012271199888+ 140012271510288 L data &#45;8.0000 grad 0.0000 140012271510288*&#45;&gt;140012271510288 140012271516432 a data 2.0000 grad 0.0000 140012132103568* * 140012271516432&#45;&gt;140012132103568* 140012132103568 e data &#45;6.0000 grad 0.0000 140012132103568&#45;&gt;140012271199888+ 140012132103568*&#45;&gt;140012132103568 140012271517136 b data &#45;3.0000 grad 0.0000 140012271517136&#45;&gt;140012132103568* Lets recap: . we are able to build out mathematical expressions using + and * | Added and tracked grad so we can calculate and update this state later when we do backpropagtion | Forward pass that produces output L and visualized | Now we want to do backpropagation | . . Manual Backpropagation . Lets manually nudge the variable a and manually calculate the derivative of L with respect to a, lets create a gating function lol so we don&#39;t pollute the global scope. We can do this for each variable to calculate their derivative with respect to L . . $L = d * f$ . $ frac{dL}{dd} =? f$ . $lim_{h to 0} frac{(d+h)*f - d*f}{h}$ . $lim_{h to 0} frac{d*f + h*f - d*f}{h}$ . $lim_{h to 0} frac{h*f}{h}$ . $f$ . So we can see that d.grad is just the value of f which is -2.0 and by the property of symmetry f.grad is just the value of d which is 4.0, lets go ahead and manually set these . f.grad = 4.0 d.grad = -2.0 . L.grad = 1 . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data # this is the variable we are nudging by h a = Value(2.0 , label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; d.data += h f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . -1.9999999999953388 . So we have just derived the derivates of f and d with respect to L in a step by step way, now next will uncover the core of backpropagation, we want derive the derivative of L with respect to c and e. &lt;/br&gt; We now know how L is sensitive to d and we know how e and c are sensitive to d, we can know put that together to figure out how L is sensitive to e and c. . If a variable z depends on the variable y, which itself depends on the variable x (that is, y and z are dependent variables), then z depends on x as well, via the intermediate variable y. In this case, the chain rule is expressed as . $ frac{dz}{dx} = frac{dz}{dy} * frac{dy}{dx}$ . The chain rule is fundamentally telling you how we chain these derivatives together correctly so to differentiate through a function composition we have to apply a multiplication of those derivatives &lt;/br&gt; The inuitive explanation here is that knowing the instantaneous rate of change of z with respect to y and y relative to x allows one to calculate the instantaneous rate of change of z . If a car travels twice as fast as a bicyle and the cycle is four times as fast as a walking man then the car is 2 * 4 = 8 times faster than the man &lt;/br&gt; . We know the derivative of $ frac{dL}{dd}$ and $ frac{dd}{dc}$ and want to find $ frac{dL}{dc}$ the chain rule tells us that $ frac{dL}{dc} = frac{dL}{dd} * frac{dd}{dc}$ $1.0 * -2.0 = -2.0$ . The chain rule is telling us for plus nodes &quot;+&quot; we are just routing the gradient because the local derivative is just 1.0 . e.grad = -2.0 c.grad = -2.0 . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140012270732304 f data &#45;2.0000 grad 4.0000 140012271510288* * 140012270732304&#45;&gt;140012271510288* 140012271199888 d data 4.0000 grad &#45;2.0000 140012271199888&#45;&gt;140012271510288* 140012271199888+ + 140012271199888+&#45;&gt;140012271199888 140012132103120 c data 10.0000 grad &#45;2.0000 140012132103120&#45;&gt;140012271199888+ 140012271510288 L data &#45;8.0000 grad 1.0000 140012271510288*&#45;&gt;140012271510288 140012271516432 a data 2.0000 grad 0.0000 140012132103568* * 140012271516432&#45;&gt;140012132103568* 140012132103568 e data &#45;6.0000 grad &#45;2.0000 140012132103568&#45;&gt;140012271199888+ 140012132103568*&#45;&gt;140012132103568 140012271517136 b data &#45;3.0000 grad 0.0000 140012271517136&#45;&gt;140012132103568* Lets manually check our work by nudging c . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data a = Value(2.0 , label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) # this is the variable we are nudging by h c = Value(10.0, label=&#39;c&#39;) c.data += h e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . -1.9999999999953388 . As we expected c.grad equals -2.0* . Now we will recurse our way backwards again and going to do our second application of the chain rule . $ frac{dL}{de} = -2.0$ &lt;/br&gt; . $ frac{de}{da} = b$ &lt;/br&gt; . $ frac{dL}{da} = frac{dL}{de} * frac{de}{da}$ . We are multiplying the derivative of e with respect to L with the local gradients . a.grad = -2.0 * -3.0 b.grad = -2.0 * 2.0 . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140012270732304 f data &#45;2.0000 grad 4.0000 140012271510288* * 140012270732304&#45;&gt;140012271510288* 140012271199888 d data 4.0000 grad &#45;2.0000 140012271199888&#45;&gt;140012271510288* 140012271199888+ + 140012271199888+&#45;&gt;140012271199888 140012132103120 c data 10.0000 grad &#45;2.0000 140012132103120&#45;&gt;140012271199888+ 140012271510288 L data &#45;8.0000 grad 1.0000 140012271510288*&#45;&gt;140012271510288 140012271516432 a data 2.0000 grad 6.0000 140012132103568* * 140012271516432&#45;&gt;140012132103568* 140012132103568 e data &#45;6.0000 grad &#45;2.0000 140012132103568&#45;&gt;140012271199888+ 140012132103568*&#45;&gt;140012132103568 140012271517136 b data &#45;3.0000 grad &#45;4.0000 140012271517136&#45;&gt;140012132103568* Lets verify . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data # this is the variable we are nudging by h a = Value(2.0 , label=&#39;a&#39;) a.data += h b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . 6.000000000021544 . Checks out . We know know what back propagation is; a recursive application of the chain rule backwards through the computational graph . Neuron Example . One step optimization . a.data += 0.01 * a.grad b.data += 0.01 * b.grad c.data += 0.01 * c.grad f.data += 0.01 * f.grad e = a * b d = e + c L = d * f print(L.data) . -7.286496 . . For our model of neurons we have input axis and these synapses that have weights on them so the w&#39;s are the weights and then the synapse interacts with the input multiplicatively so what flows to the cell body of this neuron is w times x but there&#39;s multiple inputs so there&#39;s many w times x&#39;s flowing into the cell body, the cell body also has some bias which is a sort of trigger happiness of this neuron, making it more or less prone to firing. Then we take it through an activation function which is generally some kinda of squashing function like a sigmoid or tanh. Lets go over an example of a tanh activation function . plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid(); . You can see that the inputs that come in get squashed here on the y axis, the function gets capped at 1.00 and -1.00 . x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron #6.8813735870195432 b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; draw_dot(n) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013933902864 x1*w1 data &#45;6.0000 grad 0.0000 140013933902544+ + 140013933902864&#45;&gt;140013933902544+ 140013933902864* * 140013933902864*&#45;&gt;140013933902864 140013933902352 x2 data 0.0000 grad 0.0000 140013933902992* * 140013933902352&#45;&gt;140013933902992* 140013933902416 x1 data 2.0000 grad 0.0000 140013933902416&#45;&gt;140013933902864* 140013933902480 w1 data &#45;3.0000 grad 0.0000 140013933902480&#45;&gt;140013933902864* 140013933902992 x2*w2 data 0.0000 grad 0.0000 140013933902992&#45;&gt;140013933902544+ 140013933902992*&#45;&gt;140013933902992 140013933902544 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013933902608+ + 140013933902544&#45;&gt;140013933902608+ 140013933902544+&#45;&gt;140013933902544 140013933902608 n data 0.8814 grad 0.0000 140013933902608+&#45;&gt;140013933902608 140013933902672 w2 data 1.0000 grad 0.0000 140013933902672&#45;&gt;140013933902992* 140013933902800 b data 6.8814 grad 0.0000 140013933902800&#45;&gt;140013933902608+ We need to add more operations to our Value class to be able to calculate our activation function tanh, lets just do a cheeky implementation of tanh on our value class for now . class Value: def __init__(self, data, _children=(), _op=&#39;&#39;, label=&#39;&#39;): self.data = data self._prev = set(_children) self._op = _op self.label = label self.grad = 0.0 def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data, (self, other), &#39;+&#39;) return out def __mul__(self, other): out = Value(self.data * other.data, (self, other), &#39;*&#39;) return out def tanh(self): x = self.data t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1) out = Value(t, (self, ), &#39;tanh&#39;) return out . x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron #6.8813735870195432 b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; draw_dot(n) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013933930512 x1*w1 data &#45;6.0000 grad 0.0000 140013933930896+ + 140013933930512&#45;&gt;140013933930896+ 140013933930512* * 140013933930512*&#45;&gt;140013933930512 140013933931536 x2 data 0.0000 grad 0.0000 140013933931856* * 140013933931536&#45;&gt;140013933931856* 140013933931088 n data 0.8814 grad 0.0000 140013933931088+ + 140013933931088+&#45;&gt;140013933931088 140013933931216 w2 data 1.0000 grad 0.0000 140013933931216&#45;&gt;140013933931856* 140013933931280 w1 data &#45;3.0000 grad 0.0000 140013933931280&#45;&gt;140013933930512* 140013933930768 b data 6.8814 grad 0.0000 140013933930768&#45;&gt;140013933931088+ 140013933929808 x1 data 2.0000 grad 0.0000 140013933929808&#45;&gt;140013933930512* 140013933931856 x2*w2 data 0.0000 grad 0.0000 140013933931856&#45;&gt;140013933930896+ 140013933931856*&#45;&gt;140013933931856 140013933930896 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013933930896&#45;&gt;140013933931088+ 140013933930896+&#45;&gt;140013933930896 o = n.tanh(); o.label = &#39;o&#39; . draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013933930512 x1*w1 data &#45;6.0000 grad 0.0000 140013933930896+ + 140013933930512&#45;&gt;140013933930896+ 140013933930512* * 140013933930512*&#45;&gt;140013933930512 140013933931536 x2 data 0.0000 grad 0.0000 140013933931856* * 140013933931536&#45;&gt;140013933931856* 140013933900880 o data 0.7071 grad 0.0000 140013933900880tanh tanh 140013933900880tanh&#45;&gt;140013933900880 140013933931088 n data 0.8814 grad 0.0000 140013933931088&#45;&gt;140013933900880tanh 140013933931088+ + 140013933931088+&#45;&gt;140013933931088 140013933931216 w2 data 1.0000 grad 0.0000 140013933931216&#45;&gt;140013933931856* 140013933931280 w1 data &#45;3.0000 grad 0.0000 140013933931280&#45;&gt;140013933930512* 140013933930768 b data 6.8814 grad 0.0000 140013933930768&#45;&gt;140013933931088+ 140013933929808 x1 data 2.0000 grad 0.0000 140013933929808&#45;&gt;140013933930512* 140013933931856 x2*w2 data 0.0000 grad 0.0000 140013933931856&#45;&gt;140013933930896+ 140013933931856*&#45;&gt;140013933931856 140013933930896 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013933930896&#45;&gt;140013933931088+ 140013933930896+&#45;&gt;140013933930896 Awesome n goes through tanh to produce the last output, our activation function is working great, now all we need to know is the derivative of tanh and we can use backpropagation. . o.grad = 1.0 . Lets calculte the gradient of n . 1 - o.data**2 . 0.4999999999999999 . n.grad = 0.5 . Now we can easily get the gradients for x1w1x2w1, b, x1w1, x2w2 since we used addition as an operation the local derivatives are just 1 so we just take the value 0.5 . x1w1x2w2.grad = 0.5 b.grad = 0.5 x1w1.grad = 0.5 x2w2.grad = 0.5 . We can know calculate the gradients for x2, w2, x1, and w1, but unlike the last gradients we used multiplication as our operation, so our local derivative is just the other term used in the operation so lets calculate the gradients . x2.grad = w2.data * x2w2.grad w2.grad = x2.data * x2w2.grad x1.grad = w1.data * x1w1.grad w1.grad = x1.data * x1w1.grad . draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013933930512 x1*w1 data &#45;6.0000 grad 0.5000 140013933930896+ + 140013933930512&#45;&gt;140013933930896+ 140013933930512* * 140013933930512*&#45;&gt;140013933930512 140013933931536 x2 data 0.0000 grad 0.5000 140013933931856* * 140013933931536&#45;&gt;140013933931856* 140013933900880 o data 0.7071 grad 1.0000 140013933900880tanh tanh 140013933900880tanh&#45;&gt;140013933900880 140013933931088 n data 0.8814 grad 0.5000 140013933931088&#45;&gt;140013933900880tanh 140013933931088+ + 140013933931088+&#45;&gt;140013933931088 140013933931216 w2 data 1.0000 grad 0.0000 140013933931216&#45;&gt;140013933931856* 140013933931280 w1 data &#45;3.0000 grad 1.0000 140013933931280&#45;&gt;140013933930512* 140013933930768 b data 6.8814 grad 0.5000 140013933930768&#45;&gt;140013933931088+ 140013933929808 x1 data 2.0000 grad &#45;1.5000 140013933929808&#45;&gt;140013933930512* 140013933931856 x2*w2 data 0.0000 grad 0.5000 140013933931856&#45;&gt;140013933930896+ 140013933931856*&#45;&gt;140013933931856 140013933930896 x1*w1 + x2*w2 data &#45;6.0000 grad 0.5000 140013933930896&#45;&gt;140013933931088+ 140013933930896+&#45;&gt;140013933930896 Nice!, we have manually used backpropagation to calculate our gradients, now lets implement a backward function for each operation . . Backward Function . Lets add a backward methods to our Value object for each operation that we can call to calculate our gradients for us using backpropagation . class Value: def __init__(self, data, _children=(), _op=&#39;&#39;, label=&#39;&#39;): self.data = data self._prev = set(_children) self._op = _op self.label = label self.grad = 0.0 self._backward = lambda: None def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data, (self, other), &#39;+&#39;) def _backward(): self.grad += 1.0 * out.grad other.grad += 1.0 * out.grad out._backward = _backward return out def __mul__(self, other): out = Value(self.data * other.data, (self, other), &#39;*&#39;) def _backward(): self.grad += other.data * out.grad other.grad += self.data * out.grad out._backward = _backward return out def tanh(self): x = self.data t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1) out = Value(t, (self, ), &#39;tanh&#39;) def _backward(): self.grad += (1 - t**2) * out.grad out._backward = _backward return out . x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron #6.8813735870195432 b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; o = n.tanh(); o.label = &#39;o&#39; draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013934006800 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013934006352+ + 140013934006800&#45;&gt;140013934006352+ 140013934006800+ + 140013934006800+&#45;&gt;140013934006800 140013934006352 n data 0.8814 grad 0.0000 140013934009552tanh tanh 140013934006352&#45;&gt;140013934009552tanh 140013934006352+&#45;&gt;140013934006352 140013934007376 x1*w1 data &#45;6.0000 grad 0.0000 140013934007376&#45;&gt;140013934006800+ 140013934007376* * 140013934007376*&#45;&gt;140013934007376 140013934008400 x1 data 2.0000 grad 0.0000 140013934008400&#45;&gt;140013934007376* 140013934009488 x2*w2 data 0.0000 grad 0.0000 140013934009488&#45;&gt;140013934006800+ 140013934009488* * 140013934009488*&#45;&gt;140013934009488 140013934009552 o data 0.7071 grad 0.0000 140013934009552tanh&#45;&gt;140013934009552 140013934007632 w2 data 1.0000 grad 0.0000 140013934007632&#45;&gt;140013934009488* 140012133001552 x2 data 0.0000 grad 0.0000 140012133001552&#45;&gt;140013934009488* 140013934009168 b data 6.8814 grad 0.0000 140013934009168&#45;&gt;140013934006352+ 140013934008720 w1 data &#45;3.0000 grad 0.0000 140013934008720&#45;&gt;140013934007376* We initialize the gradient of o to 1.0, then call _backward to recursively calculate the gradients . o.grad = 1.0 . o._backward() . draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013934006800 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013934006352+ + 140013934006800&#45;&gt;140013934006352+ 140013934006800+ + 140013934006800+&#45;&gt;140013934006800 140013934006352 n data 0.8814 grad 0.5000 140013934009552tanh tanh 140013934006352&#45;&gt;140013934009552tanh 140013934006352+&#45;&gt;140013934006352 140013934007376 x1*w1 data &#45;6.0000 grad 0.0000 140013934007376&#45;&gt;140013934006800+ 140013934007376* * 140013934007376*&#45;&gt;140013934007376 140013934008400 x1 data 2.0000 grad 0.0000 140013934008400&#45;&gt;140013934007376* 140013934009488 x2*w2 data 0.0000 grad 0.0000 140013934009488&#45;&gt;140013934006800+ 140013934009488* * 140013934009488*&#45;&gt;140013934009488 140013934009552 o data 0.7071 grad 1.0000 140013934009552tanh&#45;&gt;140013934009552 140013934007632 w2 data 1.0000 grad 0.0000 140013934007632&#45;&gt;140013934009488* 140012133001552 x2 data 0.0000 grad 0.0000 140012133001552&#45;&gt;140013934009488* 140013934009168 b data 6.8814 grad 0.0000 140013934009168&#45;&gt;140013934006352+ 140013934008720 w1 data &#45;3.0000 grad 0.0000 140013934008720&#45;&gt;140013934007376* n._backward() b._backward() x1w1x2w2._backward() x2w2._backward() x1w1._backward() . draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013934006800 x1*w1 + x2*w2 data &#45;6.0000 grad 0.5000 140013934006352+ + 140013934006800&#45;&gt;140013934006352+ 140013934006800+ + 140013934006800+&#45;&gt;140013934006800 140013934006352 n data 0.8814 grad 0.5000 140013934009552tanh tanh 140013934006352&#45;&gt;140013934009552tanh 140013934006352+&#45;&gt;140013934006352 140013934007376 x1*w1 data &#45;6.0000 grad 0.5000 140013934007376&#45;&gt;140013934006800+ 140013934007376* * 140013934007376*&#45;&gt;140013934007376 140013934008400 x1 data 2.0000 grad &#45;1.5000 140013934008400&#45;&gt;140013934007376* 140013934009488 x2*w2 data 0.0000 grad 0.5000 140013934009488&#45;&gt;140013934006800+ 140013934009488* * 140013934009488*&#45;&gt;140013934009488 140013934009552 o data 0.7071 grad 1.0000 140013934009552tanh&#45;&gt;140013934009552 140013934007632 w2 data 1.0000 grad 0.0000 140013934007632&#45;&gt;140013934009488* 140012133001552 x2 data 0.0000 grad 0.5000 140012133001552&#45;&gt;140013934009488* 140013934009168 b data 6.8814 grad 0.5000 140013934009168&#45;&gt;140013934006352+ 140013934008720 w1 data &#45;3.0000 grad 1.0000 140013934008720&#45;&gt;140013934007376* Awesome, this works great, we never want to call backward() on a node before we have calculated the gradients for everthing after it since it depends on their gradients. We will use topological sort which lays the graph such that all the edges go only from left to right. . . x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron #6.8813735870195432 b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; o = n.tanh(); o.label = &#39;o&#39; draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013950641680 n data 0.8814 grad 0.0000 140013950641936tanh tanh 140013950641680&#45;&gt;140013950641936tanh 140013950641680+ + 140013950641680+&#45;&gt;140013950641680 140013950640656 w2 data 1.0000 grad 0.0000 140013950641168* * 140013950640656&#45;&gt;140013950641168* 140013950641168 x2*w2 data 0.0000 grad 0.0000 140013950641424+ + 140013950641168&#45;&gt;140013950641424+ 140013950641168*&#45;&gt;140013950641168 140013950640784 b data 6.8814 grad 0.0000 140013950640784&#45;&gt;140013950641680+ 140013950640336 x2 data 0.0000 grad 0.0000 140013950640336&#45;&gt;140013950641168* 140013950641936 o data 0.7071 grad 0.0000 140013950641936tanh&#45;&gt;140013950641936 140013950641424 x1*w1 + x2*w2 data &#45;6.0000 grad 0.0000 140013950641424&#45;&gt;140013950641680+ 140013950641424+&#45;&gt;140013950641424 140013950640400 x1 data 2.0000 grad 0.0000 140013950640592* * 140013950640400&#45;&gt;140013950640592* 140013950640464 w1 data &#45;3.0000 grad 0.0000 140013950640464&#45;&gt;140013950640592* 140013950640592 x1*w1 data &#45;6.0000 grad 0.0000 140013950640592&#45;&gt;140013950641424+ 140013950640592*&#45;&gt;140013950640592 topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(o) topo . [Value(data=1.0), Value(data=0.0), Value(data=0.0), Value(data=2.0), Value(data=-3.0), Value(data=-6.0), Value(data=-6.0), Value(data=6.881373587019543), Value(data=0.8813735870195432), Value(data=0.7071067811865476)] . Our Value objects are now ordered properly . o.grad = 1.0 . for node in reversed(topo): node._backward() . draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013950641680 n data 0.8814 grad 0.5000 140013950641936tanh tanh 140013950641680&#45;&gt;140013950641936tanh 140013950641680+ + 140013950641680+&#45;&gt;140013950641680 140013950640656 w2 data 1.0000 grad 0.0000 140013950641168* * 140013950640656&#45;&gt;140013950641168* 140013950641168 x2*w2 data 0.0000 grad 0.5000 140013950641424+ + 140013950641168&#45;&gt;140013950641424+ 140013950641168*&#45;&gt;140013950641168 140013950640784 b data 6.8814 grad 0.5000 140013950640784&#45;&gt;140013950641680+ 140013950640336 x2 data 0.0000 grad 0.5000 140013950640336&#45;&gt;140013950641168* 140013950641936 o data 0.7071 grad 1.0000 140013950641936tanh&#45;&gt;140013950641936 140013950641424 x1*w1 + x2*w2 data &#45;6.0000 grad 0.5000 140013950641424&#45;&gt;140013950641680+ 140013950641424+&#45;&gt;140013950641424 140013950640400 x1 data 2.0000 grad &#45;1.5000 140013950640592* * 140013950640400&#45;&gt;140013950640592* 140013950640464 w1 data &#45;3.0000 grad 1.0000 140013950640464&#45;&gt;140013950640592* 140013950640592 x1*w1 data &#45;6.0000 grad 0.5000 140013950640592&#45;&gt;140013950641424+ 140013950640592*&#45;&gt;140013950640592 . Lets implement this as a method in the Value object and add more operations . class Value: def __init__(self, data, _children=(), _op=&#39;&#39;, label=&#39;&#39;): self.data = data self._prev = set(_children) self._op = _op self.label = label self.grad = 0.0 self._backward = lambda: None def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): other = other if isinstance(other, Value) else Value(other) out = Value(self.data + other.data, (self, other), &#39;+&#39;) def _backward(): self.grad += 1.0 * out.grad other.grad += 1.0 * out.grad out._backward = _backward return out def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) out = Value(self.data * other.data, (self, other), &#39;*&#39;) def _backward(): self.grad += other.data * out.grad other.grad += self.data * out.grad out._backward = _backward return out def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __neg__(self): # -self return self * -1 def __sub__(self, other): # self - other return self + (-other) def __radd__(self, other): # other + self return self + other def __pow__(self, other): assert isinstance(other, (int, float)), &quot;only supporting int/float powers for now&quot; out = Value(self.data**other, (self,), f&#39;**{other}&#39;) def _backward(): self.grad += other * (self.data ** (other - 1)) * out.grad out._backward = _backward return out def tanh(self): x = self.data t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1) out = Value(t, (self, ), &#39;tanh&#39;) def _backward(): self.grad += (1 - t**2) * out.grad out._backward = _backward return out def exp(self): x = self.data out = Value(math.exp(x), (self, ), &#39;exp&#39;) def _backward(): self.grad += out.data * out.grad out._backward = _backward return out # Now we can call backward on our Value object def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(self) self.grad = 1.0 for node in reversed(topo): node._backward() . a = Value(2.0) b = Value(4.0) a - b . Value(data=-2.0) . a = Value(2.0) a.exp() . Value(data=7.38905609893065) . x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; o = n.tanh(); o.label = &#39;o&#39; o.backward() draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013950682640 x2*w2 data 0.0000 grad 0.5000 140013950682000+ + 140013950682640&#45;&gt;140013950682000+ 140013950682640* * 140013950682640*&#45;&gt;140013950682640 140013950683024 n data 0.8814 grad 0.5000 140013950681488tanh tanh 140013950683024&#45;&gt;140013950681488tanh 140013950683024+ + 140013950683024+&#45;&gt;140013950683024 140013933931024 w1 data &#45;3.0000 grad 1.0000 140013950681808* * 140013933931024&#45;&gt;140013950681808* 140013933929552 w2 data 1.0000 grad 0.0000 140013933929552&#45;&gt;140013950682640* 140013933929040 b data 6.8814 grad 0.5000 140013933929040&#45;&gt;140013950683024+ 140013933928656 x2 data 0.0000 grad 0.5000 140013933928656&#45;&gt;140013950682640* 140013950681808 x1*w1 data &#45;6.0000 grad 0.5000 140013950681808&#45;&gt;140013950682000+ 140013950681808*&#45;&gt;140013950681808 140013933932240 x1 data 2.0000 grad &#45;1.5000 140013933932240&#45;&gt;140013950681808* 140013950681488 o data 0.7071 grad 1.0000 140013950681488tanh&#45;&gt;140013950681488 140013950682000 x1*w1 + x2*w2 data &#45;6.0000 grad 0.5000 140013950682000&#45;&gt;140013950683024+ 140013950682000+&#45;&gt;140013950682000 x1 = Value(2.0, label=&#39;x1&#39;) x2 = Value(0.0, label=&#39;x2&#39;) # weights w1,w2 w1 = Value(-3.0, label=&#39;w1&#39;) w2 = Value(1.0, label=&#39;w2&#39;) # bias of the neuron b = Value(6.8813735870195432, label=&#39;b&#39;) # x1*w1 + x2*w2 + b x1w1 = x1*w1; x1w1.label = &#39;x1*w1&#39; x2w2 = x2*w2; x2w2.label = &#39;x2*w2&#39; x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = &#39;x1*w1 + x2*w2&#39; n = x1w1x2w2 + b; n.label = &#39;n&#39; # - e = (2*n).exp() o = (e - 1) / (e + 1) # - o.label = &#39;o&#39; o.backward() draw_dot(o) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140013950711312 data 4.8284 grad 0.1464 140013950712208* * 140013950711312&#45;&gt;140013950712208* 140013950711312+ + 140013950711312+&#45;&gt;140013950711312 140013950608400 x1*w1 data &#45;6.0000 grad 0.5000 140013950710224+ + 140013950608400&#45;&gt;140013950710224+ 140013950608400* * 140013950608400*&#45;&gt;140013950608400 140013950709840 x2*w2 data 0.0000 grad 0.5000 140013950709840&#45;&gt;140013950710224+ 140013950709840* * 140013950709840*&#45;&gt;140013950709840 140013950711376 data &#45;1.0000 grad 0.1464 140013950711376&#45;&gt;140013950711312+ 140013950711888 data 1.0000 grad &#45;0.1036 140013950711952+ + 140013950711888&#45;&gt;140013950711952+ 140013950609552 x2 data 0.0000 grad 0.5000 140013950609552&#45;&gt;140013950709840* 140013950609040 w2 data 1.0000 grad 0.0000 140013950609040&#45;&gt;140013950709840* 140013950711952 data 6.8284 grad &#45;0.1036 140013950710544**&#45;1 **&#45;1 140013950711952&#45;&gt;140013950710544**&#45;1 140013950711952+&#45;&gt;140013950711952 140013950710480 n data 0.8814 grad 0.5000 140013950711120* * 140013950710480&#45;&gt;140013950711120* 140013950710480+ + 140013950710480+&#45;&gt;140013950710480 140013950607760 b data 6.8814 grad 0.5000 140013950607760&#45;&gt;140013950710480+ 140013950608272 x1 data 2.0000 grad &#45;1.5000 140013950608272&#45;&gt;140013950608400* 140013950609104 w1 data &#45;3.0000 grad 1.0000 140013950609104&#45;&gt;140013950608400* 140013950711056 data 2.0000 grad 0.2203 140013950711056&#45;&gt;140013950711120* 140013950711568 data 5.8284 grad 0.0429 140013950711568&#45;&gt;140013950711312+ 140013950711568&#45;&gt;140013950711952+ 140013950711568exp exp 140013950711568exp&#45;&gt;140013950711568 140013950710544 data 0.1464 grad 4.8284 140013950710544&#45;&gt;140013950712208* 140013950710544**&#45;1&#45;&gt;140013950710544 140013950711120 data 1.7627 grad 0.2500 140013950711120&#45;&gt;140013950711568exp 140013950711120*&#45;&gt;140013950711120 140013950712208 o data 0.7071 grad 1.0000 140013950712208*&#45;&gt;140013950712208 140013950710224 x1*w1 + x2*w2 data &#45;6.0000 grad 0.5000 140013950710224&#45;&gt;140013950710480+ 140013950710224+&#45;&gt;140013950710224 . Pytorch comparison . import torch x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True n = x1*w1 + x2*w2 + b o = torch.tanh(n) print(o.data.item()) o.backward() print(&#39;&#39;) print(&#39;x2&#39;, x2.grad.item()) print(&#39;w2&#39;, w2.grad.item()) print(&#39;x1&#39;, x1.grad.item()) print(&#39;w1&#39;, w1.grad.item()) . 0.7071066904050358 x2 0.5000001283844369 w2 0.0 x1 -1.5000003851533106 w1 1.0000002567688737 . o . tensor([0.7071], dtype=torch.float64, grad_fn=&lt;TanhBackward0&gt;) . o.item() . 0.7071066904050358 . . import random class Neuron: def __init__(self, nin): self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] self.b = Value(random.uniform(-1,1)) def __call__(self, x): # w * x + b act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b) out = act.tanh() return out def parameters(self): return self.w + [self.b] class Layer: def __init__(self, nin, nout): self.neurons = [Neuron(nin) for _ in range(nout)] def __call__(self, x): outs = [n(x) for n in self.neurons] return outs[0] if len(outs) == 1 else outs def parameters(self): return [p for neuron in self.neurons for p in neuron.parameters()] # params = [] # for neuron in self.neurons: # ps = neuron.parameters() # params.extend(ps) # return params class MLP: def __init__(self, nin, nouts): sz = [nin] + nouts self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))] def __call__(self, x): for layer in self.layers: x = layer(x) return x def parameters(self): return [p for layer in self.layers for p in layer.parameters()] . x = [2.0, 3.0, -1.0] n = MLP(3, [4, 4, 1]) n(x) . Value(data=-0.7028959990425087) . len(n.parameters()) . 41 . xs = [ [2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0] ] ys = [1.0, -1.0, -1.0, 1.0] ypred = [n(x) for x in xs] ypred . [Value(data=-0.7028959990425087), Value(data=0.1757758058598642), Value(data=0.39624177470697325), Value(data=0.16264413106842704)] . So how do we tune the weights to better predict the desired targets? We calculate a single number that measures the total performance of the neural net, this is called the loss. . loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) loss . Value(data=6.932959473871423) . loss.backward() . Now that we have called backward on the loss, we can take a look at the gradient of a single neuron in one of our layers to get a look on how it impacts our loss, this will be useful soon when we try to update our weights to decrease the loss . n.layers[0].neurons[0].w[0].data . 0.28438888706081467 . The gradient for this neuron is positive, so the weights is increasing our loss . n.layers[0].neurons[0].w[0].grad . 1.711566891777295 . n.layers[0].neurons[0].b.grad . 1.0849775541180198 . We can now also call draw_dot on our loss, we can see the DAG has increased tremendously in compelexity . draw_dot(loss) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140012867125392 data 0.5000 grad 0.0138 140012867125456* * 140012867125392&#45;&gt;140012867125456* 140012867223696 data &#45;0.2920 grad &#45;1.2578 140012867235152* * 140012867223696&#45;&gt;140012867235152* 140012867223696tanh tanh 140012867223696tanh&#45;&gt;140012867223696 140012867387600 data 0.2986 grad 0.0046 140012867161360* * 140012867387600&#45;&gt;140012867161360* 140012867234384* * 140012867387600&#45;&gt;140012867234384* 140012867084240* * 140012867387600&#45;&gt;140012867084240* 140012866791312* * 140012867387600&#45;&gt;140012866791312* 140012867125456 data 0.1566 grad 0.0441 140012867125712+ + 140012867125456&#45;&gt;140012867125712+ 140012867125456*&#45;&gt;140012867125456 140012867158288 data &#45;0.4037 grad &#45;0.0501 140012867158800+ + 140012867158288&#45;&gt;140012867158800+ 140012867158288+ + 140012867158288+&#45;&gt;140012867158288 140012867387792 data 0.0591 grad &#45;0.0089 140012867160080+ + 140012867387792&#45;&gt;140012867160080+ 140012867233104+ + 140012867387792&#45;&gt;140012867233104+ 140012867082960+ + 140012867387792&#45;&gt;140012867082960+ 140012866790032+ + 140012867387792&#45;&gt;140012866790032+ 140012867420624 data &#45;0.0117 grad 0.5672 140012867098896* * 140012867420624&#45;&gt;140012867098896* 140012866805968* * 140012867420624&#45;&gt;140012866805968* 140012867176016* * 140012867420624&#45;&gt;140012867176016* 140012867249040* * 140012867420624&#45;&gt;140012867249040* 140012867125712 data 2.3308 grad 0.0441 140012867125840tanh tanh 140012867125712&#45;&gt;140012867125840tanh 140012867125712+&#45;&gt;140012867125712 140012867387920 data &#45;0.3473 grad &#45;0.8261 140012867224336+ + 140012867387920&#45;&gt;140012867224336+ 140012867036112+ + 140012867387920&#45;&gt;140012867036112+ 140012866777168+ + 140012867387920&#45;&gt;140012866777168+ 140012867143120+ + 140012867387920&#45;&gt;140012867143120+ 140012867158544 data &#45;0.6273 grad &#45;0.0501 140012867158544&#45;&gt;140012867158800+ 140012867158544* * 140012867158544*&#45;&gt;140012867158544 140012867224080 data 0.0272 grad &#45;1.4466 140012867224080&#45;&gt;140012867224336+ 140012867224080* * 140012867224080*&#45;&gt;140012867224080 140012867125840 data 0.9813 grad 1.1885 140012867125840&#45;&gt;140012867158544* 140012867160848* * 140012867125840&#45;&gt;140012867160848* 140012867129232* * 140012867125840&#45;&gt;140012867129232* 140012867143888* * 140012867125840&#45;&gt;140012867143888* 140012867125840tanh&#45;&gt;140012867125840 140012867420880 data &#45;0.2072 grad 1.2794 140012867097616+ + 140012867420880&#45;&gt;140012867097616+ 140012867235408+ + 140012867420880&#45;&gt;140012867235408+ 140012867174736+ + 140012867420880&#45;&gt;140012867174736+ 140012866792336+ + 140012867420880&#45;&gt;140012866792336+ 140012867420944 data &#45;0.8644 grad &#45;1.5505 140012867097872* * 140012867420944&#45;&gt;140012867097872* 140012867235664* * 140012867420944&#45;&gt;140012867235664* 140012867174992* * 140012867420944&#45;&gt;140012867174992* 140012866792592* * 140012867420944&#45;&gt;140012866792592* 140012867158800 data &#45;1.0310 grad &#45;0.0501 140012867159312+ + 140012867158800&#45;&gt;140012867159312+ 140012867158800+&#45;&gt;140012867158800 140012867224336 data &#45;0.3200 grad &#45;1.4466 140012867224848+ + 140012867224336&#45;&gt;140012867224848+ 140012867224336+&#45;&gt;140012867224336 140012867421008 data &#45;0.2827 grad &#45;1.8925 140012867098384* * 140012867421008&#45;&gt;140012867098384* 140012867175504* * 140012867421008&#45;&gt;140012867175504* 140012867248528* * 140012867421008&#45;&gt;140012867248528* 140012866793104* * 140012867421008&#45;&gt;140012866793104* 140012867126224 data 3.0000 grad &#45;0.0141 140012867126288* * 140012867126224&#45;&gt;140012867126288* 140012867126288 data &#45;2.2485 grad 0.0189 140012867126544+ + 140012867126288&#45;&gt;140012867126544+ 140012867126288*&#45;&gt;140012867126288 140012867159056 data &#45;0.9194 grad &#45;0.0501 140012867159056&#45;&gt;140012867159312+ 140012867159056* * 140012867159056*&#45;&gt;140012867159056 140012867224592 data &#45;0.1241 grad &#45;1.4466 140012867224592&#45;&gt;140012867224848+ 140012867224592* * 140012867224592*&#45;&gt;140012867224592 140012867388496 data &#45;0.0257 grad 1.0154 140012867225616* * 140012867388496&#45;&gt;140012867225616* 140012866778448* * 140012867388496&#45;&gt;140012866778448* 140012867144400* * 140012867388496&#45;&gt;140012867144400* 140012867704592* * 140012867388496&#45;&gt;140012867704592* 140012867126544 data &#45;1.9131 grad 0.0189 140012867127120+ + 140012867126544&#45;&gt;140012867127120+ 140012867126544+&#45;&gt;140012867126544 140012867159312 data &#45;1.9504 grad &#45;0.0501 140012867159440tanh tanh 140012867159312&#45;&gt;140012867159440tanh 140012867159312+&#45;&gt;140012867159312 140012867224848 data &#45;0.4441 grad &#45;1.4466 140012867225360+ + 140012867224848&#45;&gt;140012867225360+ 140012867224848+&#45;&gt;140012867224848 140012867388752 data &#45;0.2507 grad &#45;0.0475 140012867388752&#45;&gt;140012867160848* 140012867233872* * 140012867388752&#45;&gt;140012867233872* 140012867083728* * 140012867388752&#45;&gt;140012867083728* 140012866790800* * 140012867388752&#45;&gt;140012866790800* 140012867388816 data &#45;0.5827 grad 0.4721 140012867226384* * 140012867388816&#45;&gt;140012867226384* 140012866779216* * 140012867388816&#45;&gt;140012866779216* 140012867145168* * 140012867388816&#45;&gt;140012867145168* 140012867702992* * 140012867388816&#45;&gt;140012867702992* 140012867159440 data &#45;0.9604 grad &#45;0.6443 140012867159440&#45;&gt;140012867175504* 140012867159440tanh&#45;&gt;140012867159440 140012867388880 data &#45;0.2595 grad 0.0134 140012867160336* * 140012867388880&#45;&gt;140012867160336* 140012867233360* * 140012867388880&#45;&gt;140012867233360* 140012867083216* * 140012867388880&#45;&gt;140012867083216* 140012866790288* * 140012867388880&#45;&gt;140012866790288* 140012867126800 data &#45;1.0000 grad 0.0164 140012867126864* * 140012867126800&#45;&gt;140012867126864* 140012867225104 data &#45;0.1334 grad &#45;1.4466 140012867225104&#45;&gt;140012867225360+ 140012867225104* * 140012867225104*&#45;&gt;140012867225104 140012867389008 data &#45;0.3994 grad 0.3803 140012867128720* * 140012867389008&#45;&gt;140012867128720* 140012867205840* * 140012867389008&#45;&gt;140012867205840* 140012867081680* * 140012867389008&#45;&gt;140012867081680* 140012866762768* * 140012867389008&#45;&gt;140012866762768* 140012867126864 data &#45;0.8702 grad 0.0189 140012867126864&#45;&gt;140012867127120+ 140012867126864*&#45;&gt;140012867126864 140012867389200 data 0.8249 grad 0.9078 140012867389200&#45;&gt;140012867224592* 140012866777424* * 140012867389200&#45;&gt;140012866777424* 140012867143376* * 140012867389200&#45;&gt;140012867143376* 140012867974608* * 140012867389200&#45;&gt;140012867974608* 140012867159824 data &#45;0.0502 grad &#45;0.0185 140012867159824&#45;&gt;140012867160080+ 140012867159824* * 140012867159824*&#45;&gt;140012867159824 140012867225360 data &#45;0.5776 grad &#45;1.4466 140012867225872+ + 140012867225360&#45;&gt;140012867225872+ 140012867225360+&#45;&gt;140012867225360 140012867127120 data &#45;2.7833 grad 0.0189 140012867127696+ + 140012867127120&#45;&gt;140012867127696+ 140012867127120+&#45;&gt;140012867127120 140012867389328 data &#45;0.3938 grad &#45;3.5361 140012867389328&#45;&gt;140012867225104* 140012866777936* * 140012867389328&#45;&gt;140012866777936* 140012867389328&#45;&gt;140012867143888* 140012867705104* * 140012867389328&#45;&gt;140012867705104* 140012867389456 data &#45;0.5332 grad 2.0686 140012867389456&#45;&gt;140012867224080* 140012867142864* * 140012867389456&#45;&gt;140012867142864* 140012867021200* * 140012867389456&#45;&gt;140012867021200* 140012866764560* * 140012867389456&#45;&gt;140012866764560* 140012867160080 data 0.0089 grad &#45;0.0185 140012867160592+ + 140012867160080&#45;&gt;140012867160592+ 140012867160080+&#45;&gt;140012867160080 140012867684368 data 0.6089 grad 0.3212 140012867082192+ + 140012867684368&#45;&gt;140012867082192+ 140012867684368* * 140012867684368*&#45;&gt;140012867684368 140012867225616 data &#45;0.0233 grad &#45;1.4466 140012867225616&#45;&gt;140012867225872+ 140012867225616*&#45;&gt;140012867225616 140012867389520 data 0.3354 grad &#45;0.1022 140012867389520&#45;&gt;140012867126544+ 140012867067152+ + 140012867389520&#45;&gt;140012867067152+ 140012867203664+ + 140012867389520&#45;&gt;140012867203664+ 140012866748240+ + 140012867389520&#45;&gt;140012866748240+ 140012867127376 data 0.5000 grad 0.0126 140012867127440* * 140012867127376&#45;&gt;140012867127440* 140012867127440 data 0.3338 grad 0.0189 140012867127440&#45;&gt;140012867127696+ 140012867127440*&#45;&gt;140012867127440 140012867684560 data 0.0591 grad 0.3212 140012867684560&#45;&gt;140012867082192+ 140012867684560+ + 140012867684560+&#45;&gt;140012867684560 140012867160336 data &#45;0.0887 grad &#45;0.0185 140012867160336&#45;&gt;140012867160592+ 140012867160336*&#45;&gt;140012867160336 140012867225872 data &#45;0.6009 grad &#45;1.4466 140012867226000tanh tanh 140012867225872&#45;&gt;140012867226000tanh 140012867225872+&#45;&gt;140012867225872 140012867127696 data &#45;2.4496 grad 0.0189 140012867127824tanh tanh 140012867127696&#45;&gt;140012867127824tanh 140012867127696+&#45;&gt;140012867127696 140012867226000 data &#45;0.5377 grad &#45;2.0348 140012867226000&#45;&gt;140012867235664* 140012867226000tanh&#45;&gt;140012867226000 140012867389904 data 0.5278 grad &#45;1.2022 140012867128464+ + 140012867389904&#45;&gt;140012867128464+ 140012867205584+ + 140012867389904&#45;&gt;140012867205584+ 140012867081424+ + 140012867389904&#45;&gt;140012867081424+ 140012866762512+ + 140012867389904&#45;&gt;140012866762512+ 140012867389968 data 0.9332 grad &#45;0.3701 140012867389968&#45;&gt;140012867159056* 140012867389968&#45;&gt;140012867684368* 140012867232080* * 140012867389968&#45;&gt;140012867232080* 140012866780752* * 140012867389968&#45;&gt;140012866780752* 140012867160592 data &#45;0.0798 grad &#45;0.0185 140012867161104+ + 140012867160592&#45;&gt;140012867161104+ 140012867160592+&#45;&gt;140012867160592 140012867127824 data &#45;0.9852 grad 0.6423 140012867127824&#45;&gt;140012867159056* 140012867127824&#45;&gt;140012867161360* 140012867142096* * 140012867127824&#45;&gt;140012867142096* 140012867127824&#45;&gt;140012867144400* 140012867127824tanh&#45;&gt;140012867127824 140012867160848 data &#45;0.2460 grad &#45;0.0185 140012867160848&#45;&gt;140012867161104+ 140012867160848*&#45;&gt;140012867160848 140012867226384 data 0.0298 grad &#45;0.6648 140012867226640+ + 140012867226384&#45;&gt;140012867226640+ 140012867226384*&#45;&gt;140012867226384 140012867390288 data &#45;0.6393 grad &#45;0.6031 140012867390288&#45;&gt;140012867158544* 140012867227408* * 140012867390288&#45;&gt;140012867227408* 140012867694864* * 140012867390288&#45;&gt;140012867694864* 140012866780240* * 140012867390288&#45;&gt;140012866780240* 140012867128208 data 0.0424 grad &#45;0.8904 140012867128208&#45;&gt;140012867128464+ 140012867128208* * 140012867128208*&#45;&gt;140012867128208 140012867390416 data &#45;0.7343 grad 0.1670 140012867206864* * 140012867390416&#45;&gt;140012867206864* 140012867390416&#45;&gt;140012867142096* 140012867021904* * 140012867390416&#45;&gt;140012867021904* 140012866763792* * 140012867390416&#45;&gt;140012866763792* 140012867161104 data &#45;0.3258 grad &#45;0.0185 140012867161616+ + 140012867161104&#45;&gt;140012867161616+ 140012867161104+&#45;&gt;140012867161104 140012867226640 data &#45;0.5557 grad &#45;0.6648 140012867227152+ + 140012867226640&#45;&gt;140012867227152+ 140012867226640+&#45;&gt;140012867226640 140012867390608 data &#45;0.5273 grad 0.0330 140012867390608&#45;&gt;140012867159824* 140012867232848* * 140012867390608&#45;&gt;140012867232848* 140012867082704* * 140012867390608&#45;&gt;140012867082704* 140012866789776* * 140012867390608&#45;&gt;140012866789776* 140012867128464 data 0.5701 grad &#45;0.8904 140012867128976+ + 140012867128464&#45;&gt;140012867128976+ 140012867128464+&#45;&gt;140012867128464 140012867161360 data &#45;0.2941 grad &#45;0.0185 140012867161360&#45;&gt;140012867161616+ 140012867161360*&#45;&gt;140012867161360 140012867226896 data &#45;0.1045 grad &#45;0.6648 140012867226896&#45;&gt;140012867227152+ 140012867226896* * 140012867226896*&#45;&gt;140012867226896 140012867390800 data &#45;0.5855 grad &#45;0.2620 140012867390800&#45;&gt;140012867226640+ 140012866779472+ + 140012867390800&#45;&gt;140012866779472+ 140012867145424+ + 140012867390800&#45;&gt;140012867145424+ 140012867704016+ + 140012867390800&#45;&gt;140012867704016+ 140012867128720 data &#45;0.1364 grad &#45;0.8904 140012867128720&#45;&gt;140012867128976+ 140012867128720*&#45;&gt;140012867128720 140012867161616 data &#45;0.6199 grad &#45;0.0185 140012867161744tanh tanh 140012867161616&#45;&gt;140012867161744tanh 140012867161616+&#45;&gt;140012867161616 140012867227152 data &#45;0.6602 grad &#45;0.6648 140012867231824+ + 140012867227152&#45;&gt;140012867231824+ 140012867227152+&#45;&gt;140012867227152 140012867391056 data 0.6947 grad 0.3629 140012867391056&#45;&gt;140012867226896* 140012866779728* * 140012867391056&#45;&gt;140012866779728* 140012867145680* * 140012867391056&#45;&gt;140012867145680* 140012867703632* * 140012867391056&#45;&gt;140012867703632* 140012867391120 data &#45;0.5343 grad &#45;1.9179 140012867097360* * 140012867391120&#45;&gt;140012867097360* 140012867391120&#45;&gt;140012867235152* 140012867174480* * 140012867391120&#45;&gt;140012867174480* 140012866792080* * 140012867391120&#45;&gt;140012866792080* 140012867161744 data &#45;0.5511 grad &#45;0.0266 140012867161744&#45;&gt;140012867176016* 140012867161744tanh&#45;&gt;140012867161744 140012867128976 data 0.4337 grad &#45;0.8904 140012867141840+ + 140012867128976&#45;&gt;140012867141840+ 140012867128976+&#45;&gt;140012867128976 140012867391184 data 0.4450 grad 0.7938 140012867391184&#45;&gt;140012867128208* 140012867205328* * 140012867391184&#45;&gt;140012867205328* 140012867081168* * 140012867391184&#45;&gt;140012867081168* 140012866762256* * 140012867391184&#45;&gt;140012866762256* 140012867227408 data &#45;0.2166 grad &#45;0.6648 140012867227408&#45;&gt;140012867231824+ 140012867227408*&#45;&gt;140012867227408 140012867391376 data &#45;0.5942 grad &#45;1.8668 140012867391376&#45;&gt;140012867129232* 140012867206352* * 140012867391376&#45;&gt;140012867206352* 140012867022800* * 140012867391376&#45;&gt;140012867022800* 140012866763280* * 140012867391376&#45;&gt;140012866763280* 140012867129232 data &#45;0.5831 grad &#45;0.8904 140012867129232&#45;&gt;140012867141840+ 140012867129232*&#45;&gt;140012867129232 140012867064016 data &#45;1.0000 grad 0.0226 140012867064080* * 140012867064016&#45;&gt;140012867064080* 140012867064080 data &#45;0.0964 grad 0.2345 140012867064336+ + 140012867064080&#45;&gt;140012867064336+ 140012867064080*&#45;&gt;140012867064080 140012867096848 data &#45;0.2656 grad 0.0188 140012867096976tanh tanh 140012867096848&#45;&gt;140012867096976tanh 140012867096848+ + 140012867096848+&#45;&gt;140012867096848 140012867096976 data &#45;0.2595 grad 0.0201 140012867096976&#45;&gt;140012867098896* 140012867096976tanh&#45;&gt;140012867096976 140012867064336 data 1.4366 grad 0.2345 140012867064464tanh tanh 140012867064336&#45;&gt;140012867064464tanh 140012867064336+&#45;&gt;140012867064336 140012867064464 data 0.8930 grad 1.1577 140012867064464&#45;&gt;140012867703632* 140012867064464&#45;&gt;140012867081680* 140012867064464&#45;&gt;140012867083216* 140012867064464&#45;&gt;140012867974608* 140012867064464tanh&#45;&gt;140012867064464 140012867097360 data &#45;0.3215 grad &#45;1.7231 140012867097360&#45;&gt;140012867097616+ 140012867097360*&#45;&gt;140012867097360 140012867064848 data 2.0000 grad &#45;0.0190 140012867064912* * 140012867064848&#45;&gt;140012867064912* 140012867097616 data &#45;0.5287 grad &#45;1.7231 140012867098128+ + 140012867097616&#45;&gt;140012867098128+ 140012867097616+&#45;&gt;140012867097616 140012867064912 data 0.3149 grad &#45;0.1208 140012867065168+ + 140012867064912&#45;&gt;140012867065168+ 140012867064912*&#45;&gt;140012867064912 140012867097872 data &#45;0.1823 grad &#45;1.7231 140012867097872&#45;&gt;140012867098128+ 140012867097872*&#45;&gt;140012867097872 140012867065168 data 1.1463 grad &#45;0.1208 140012867065744+ + 140012867065168&#45;&gt;140012867065744+ 140012867065168+&#45;&gt;140012867065168 140012867098128 data &#45;0.7110 grad &#45;1.7231 140012867098640+ + 140012867098128&#45;&gt;140012867098640+ 140012867098128+&#45;&gt;140012867098128 140012867065424 data 3.0000 grad 0.1051 140012867065488* * 140012867065424&#45;&gt;140012867065488* 140012867065488 data &#45;2.6113 grad &#45;0.1208 140012867065488&#45;&gt;140012867065744+ 140012867065488*&#45;&gt;140012867065488 140012867098384 data &#45;0.1650 grad &#45;1.7231 140012867098384&#45;&gt;140012867098640+ 140012867098384*&#45;&gt;140012867098384 140012867065744 data &#45;1.4651 grad &#45;0.1208 140012867066320+ + 140012867065744&#45;&gt;140012867066320+ 140012867065744+&#45;&gt;140012867065744 140012867098640 data &#45;0.8760 grad &#45;1.7231 140012867099152+ + 140012867098640&#45;&gt;140012867099152+ 140012867098640+&#45;&gt;140012867098640 140012867066000 data &#45;1.0000 grad &#45;0.0378 140012867066064* * 140012867066000&#45;&gt;140012867066064* 140012867066064 data &#45;0.3132 grad &#45;0.1208 140012867066064&#45;&gt;140012867066320+ 140012867066064*&#45;&gt;140012867066064 140012867098896 data 0.0030 grad &#45;1.7231 140012867098896&#45;&gt;140012867099152+ 140012867098896*&#45;&gt;140012867098896 140012867066320 data &#45;1.7783 grad &#45;0.1208 140012867066448tanh tanh 140012867066320&#45;&gt;140012867066448tanh 140012867066320+&#45;&gt;140012867066320 140012867099152 data &#45;0.8730 grad &#45;1.7231 140012867099280tanh tanh 140012867099152&#45;&gt;140012867099280tanh 140012867099152+&#45;&gt;140012867099152 140012867066448 data &#45;0.9445 grad &#45;1.1195 140012867066448&#45;&gt;140012867694864* 140012867066448&#45;&gt;140012867705104* 140012867066448&#45;&gt;140012867083728* 140012867066448&#45;&gt;140012867022800* 140012867066448tanh&#45;&gt;140012867066448 140012867099280 data &#45;0.7029 grad &#45;3.4058 140012866808400+ + 140012867099280&#45;&gt;140012866808400+ 140012867099280tanh&#45;&gt;140012867099280 140012867066832 data 2.0000 grad 0.0700 140012867066896* * 140012867066832&#45;&gt;140012867066896* 140012867099664 data 3.0000 grad 0.1369 140012867099728* * 140012867099664&#45;&gt;140012867099728* 140012867066896 data &#45;1.4990 grad &#45;0.0934 140012867066896&#45;&gt;140012867067152+ 140012867066896*&#45;&gt;140012867066896 140012867099728 data 0.8532 grad 0.4815 140012867099984+ + 140012867099728&#45;&gt;140012867099984+ 140012867099728*&#45;&gt;140012867099728 140012867067152 data &#45;1.1636 grad &#45;0.0934 140012867067728+ + 140012867067152&#45;&gt;140012867067728+ 140012867067152+&#45;&gt;140012867067152 140012867099984 data 0.9858 grad 0.4815 140012867100560+ + 140012867099984&#45;&gt;140012867100560+ 140012867099984+&#45;&gt;140012867099984 140012867067408 data 3.0000 grad &#45;0.0812 140012867067472* * 140012867067408&#45;&gt;140012867067472* 140012867100240 data &#45;1.0000 grad 0.2335 140012867100304* * 140012867100240&#45;&gt;140012867100304* 140012867067472 data 2.6107 grad &#45;0.0934 140012867067472&#45;&gt;140012867067728+ 140012867067472*&#45;&gt;140012867067472 140012867100304 data &#45;0.4848 grad 0.4815 140012867100304&#45;&gt;140012867100560+ 140012867100304*&#45;&gt;140012867100304 140012867067728 data 1.4471 grad &#45;0.0934 140012867080656+ + 140012867067728&#45;&gt;140012867080656+ 140012867067728+&#45;&gt;140012867067728 140012867100560 data 0.5009 grad 0.4815 140012867109392+ + 140012867100560&#45;&gt;140012867109392+ 140012867100560+&#45;&gt;140012867100560 140012867231824 data &#45;0.8769 grad &#45;0.6648 140012867232336+ + 140012867231824&#45;&gt;140012867232336+ 140012867231824+&#45;&gt;140012867231824 140012866805968 data 0.0048 grad &#45;1.6304 140012866806224+ + 140012866805968&#45;&gt;140012866806224+ 140012866805968*&#45;&gt;140012866805968 140012867232080 data 0.8444 grad &#45;0.6648 140012867232080&#45;&gt;140012867232336+ 140012867232080*&#45;&gt;140012867232080 140012866806224 data 0.1641 grad &#45;1.6304 140012866806352tanh tanh 140012866806224&#45;&gt;140012866806352tanh 140012866806224+&#45;&gt;140012866806224 140012866806352 data 0.1626 grad &#45;1.6747 140012866815120+ + 140012866806352&#45;&gt;140012866815120+ 140012866806352tanh&#45;&gt;140012866806352 140012867232336 data &#45;0.0324 grad &#45;0.6648 140012867232464tanh tanh 140012867232336&#45;&gt;140012867232464tanh 140012867232336+&#45;&gt;140012867232336 140012867232464 data &#45;0.0324 grad &#45;0.6655 140012867232464&#45;&gt;140012867248528* 140012867232464tanh&#45;&gt;140012867232464 140012867036112 data &#45;0.8776 grad 1.4232 140012867926160+ + 140012867036112&#45;&gt;140012867926160+ 140012867036112+&#45;&gt;140012867036112 140012867232848 data 0.0269 grad &#45;0.0250 140012867232848&#45;&gt;140012867233104+ 140012867232848*&#45;&gt;140012867232848 140012867233104 data 0.0860 grad &#45;0.0250 140012867233616+ + 140012867233104&#45;&gt;140012867233616+ 140012867233104+&#45;&gt;140012867233104 140012867233360 data 0.0391 grad &#45;0.0250 140012867233360&#45;&gt;140012867233616+ 140012867233360*&#45;&gt;140012867233360 140012867233616 data 0.1251 grad &#45;0.0250 140012867234128+ + 140012867233616&#45;&gt;140012867234128+ 140012867233616+&#45;&gt;140012867233616 140012867233872 data &#45;0.0850 grad &#45;0.0250 140012867233872&#45;&gt;140012867234128+ 140012867233872*&#45;&gt;140012867233872 140012867234128 data 0.0401 grad &#45;0.0250 140012867234640+ + 140012867234128&#45;&gt;140012867234640+ 140012867234128+&#45;&gt;140012867234128 140012866808336 data 0.0000 grad 1.0000 140012866808656+ + 140012866808336&#45;&gt;140012866808656+ 140012866808400 data &#45;1.7029 grad &#45;3.4058 140012866808592**2 **2 140012866808400&#45;&gt;140012866808592**2 140012866808400+&#45;&gt;140012866808400 140012867234384 data 0.2701 grad &#45;0.0250 140012867234384&#45;&gt;140012867234640+ 140012867234384*&#45;&gt;140012867234384 140012866808592 data 2.8999 grad 1.0000 140012866808592&#45;&gt;140012866808656+ 140012866808592**2&#45;&gt;140012866808592 140012866808656 data 2.8999 grad 1.0000 140012866809488+ + 140012866808656&#45;&gt;140012866809488+ 140012866808656+&#45;&gt;140012866808656 140012867234640 data 0.3103 grad &#45;0.0250 140012867234768tanh tanh 140012867234640&#45;&gt;140012867234768tanh 140012867234640+&#45;&gt;140012867234640 140012867234768 data 0.3007 grad &#45;0.0275 140012867234768&#45;&gt;140012867249040* 140012867234768tanh&#45;&gt;140012867234768 140012866808912 data &#45;1.0000 grad &#45;3.4058 140012866808912&#45;&gt;140012866808400+ 140012866809104 data 1.1758 grad 2.3516 140012866809808**2 **2 140012866809104&#45;&gt;140012866809808**2 140012866809104+ + 140012866809104+&#45;&gt;140012866809104 140012867235152 data 0.1560 grad 2.3540 140012867235152&#45;&gt;140012867235408+ 140012867235152*&#45;&gt;140012867235152 140012866809232 data 1.0000 grad 2.3516 140012866809232&#45;&gt;140012866809104+ 140012867235408 data &#45;0.0512 grad 2.3540 140012867248272+ + 140012867235408&#45;&gt;140012867248272+ 140012867235408+&#45;&gt;140012867235408 140012866809488 data 4.2823 grad 1.0000 140012866814800+ + 140012866809488&#45;&gt;140012866814800+ 140012866809488+&#45;&gt;140012866809488 140012867235664 data 0.4648 grad 2.3540 140012867235664&#45;&gt;140012867248272+ 140012867235664*&#45;&gt;140012867235664 140012866809808 data 1.3824 grad 1.0000 140012866809808&#45;&gt;140012866809488+ 140012866809808**2&#45;&gt;140012866809808 140012866777168 data &#45;0.8469 grad 0.7787 140012866777680+ + 140012866777168&#45;&gt;140012866777680+ 140012866777168+&#45;&gt;140012866777168 140012866744528 data 1.0000 grad 0.2839 140012866744592* * 140012866744528&#45;&gt;140012866744592* 140012866744592 data 0.4524 grad 0.6275 140012866744848+ + 140012866744592&#45;&gt;140012866744848+ 140012866744592*&#45;&gt;140012866744592 140012867694864 data 0.6038 grad 0.3212 140012867694864&#45;&gt;140012867684560+ 140012867694864*&#45;&gt;140012867694864 140012867203344 data 0.5000 grad &#45;0.0345 140012867203408* * 140012867203344&#45;&gt;140012867203408* 140012866777424 data &#45;0.0431 grad 0.7787 140012866777424&#45;&gt;140012866777680+ 140012866777424*&#45;&gt;140012866777424 140012867203408 data &#45;0.3747 grad 0.0461 140012867203408&#45;&gt;140012867203664+ 140012867203408*&#45;&gt;140012867203408 140012866744848 data 0.0441 grad 0.6275 140012866745424+ + 140012866744848&#45;&gt;140012866745424+ 140012866744848+&#45;&gt;140012866744848 140012866777680 data &#45;0.8899 grad 0.7787 140012866778192+ + 140012866777680&#45;&gt;140012866778192+ 140012866777680+&#45;&gt;140012866777680 140012867203664 data &#45;0.0394 grad 0.0461 140012867204240+ + 140012867203664&#45;&gt;140012867204240+ 140012867203664+&#45;&gt;140012867203664 140012866745104 data &#45;1.0000 grad 0.0605 140012866745168* * 140012866745104&#45;&gt;140012866745168* 140012866745168 data &#45;0.0964 grad 0.6275 140012866745168&#45;&gt;140012866745424+ 140012866745168*&#45;&gt;140012866745168 140012866777936 data 0.0758 grad 0.7787 140012866777936&#45;&gt;140012866778192+ 140012866777936*&#45;&gt;140012866777936 140012867203920 data 1.0000 grad 0.0401 140012867203984* * 140012867203920&#45;&gt;140012867203984* 140012867203984 data 0.8702 grad 0.0461 140012867203984&#45;&gt;140012867204240+ 140012867203984*&#45;&gt;140012867203984 140012866745424 data &#45;0.0523 grad 0.6275 140012866745552tanh tanh 140012866745424&#45;&gt;140012866745552tanh 140012866745424+&#45;&gt;140012866745424 140012866778192 data &#45;0.8142 grad 0.7787 140012866778704+ + 140012866778192&#45;&gt;140012866778704+ 140012866778192+&#45;&gt;140012866778192 140012867204240 data 0.8309 grad 0.0461 140012867204816+ + 140012867204240&#45;&gt;140012867204816+ 140012867204240+&#45;&gt;140012867204240 140012866745552 data &#45;0.0522 grad 0.6292 140012866745552&#45;&gt;140012866777424* 140012866745552&#45;&gt;140012866779728* 140012866745552&#45;&gt;140012866790288* 140012866745552&#45;&gt;140012866762768* 140012866745552tanh&#45;&gt;140012866745552 140012866778448 data 0.0054 grad 0.7787 140012866778448&#45;&gt;140012866778704+ 140012866778448*&#45;&gt;140012866778448 140012867204496 data 1.0000 grad 0.0308 140012867204560* * 140012867204496&#45;&gt;140012867204560* 140012867204560 data 0.6675 grad 0.0461 140012867204560&#45;&gt;140012867204816+ 140012867204560*&#45;&gt;140012867204560 140012866745936 data 1.0000 grad &#45;0.0825 140012866746000* * 140012866745936&#45;&gt;140012866746000* 140012866778704 data &#45;0.8088 grad 0.7787 140012866778832tanh tanh 140012866778704&#45;&gt;140012866778832tanh 140012866778704+&#45;&gt;140012866778704 140012866746000 data 0.1575 grad &#45;0.5240 140012866746256+ + 140012866746000&#45;&gt;140012866746256+ 140012866746000*&#45;&gt;140012866746000 140012866778832 data &#45;0.6689 grad 1.4093 140012866778832&#45;&gt;140012866792592* 140012866778832tanh&#45;&gt;140012866778832 140012867204816 data 1.4984 grad 0.0461 140012867204944tanh tanh 140012867204816&#45;&gt;140012867204944tanh 140012867204816+&#45;&gt;140012867204816 140012867204944 data 0.9049 grad 0.2542 140012867204944&#45;&gt;140012867225616* 140012867204944&#45;&gt;140012867232080* 140012867204944&#45;&gt;140012867234384* 140012867204944&#45;&gt;140012867206864* 140012867204944tanh&#45;&gt;140012867204944 140012866746256 data 0.9888 grad &#45;0.5240 140012866746832+ + 140012866746256&#45;&gt;140012866746832+ 140012866746256+&#45;&gt;140012866746256 140012866779216 data &#45;0.5460 grad 0.1317 140012866779216&#45;&gt;140012866779472+ 140012866779216*&#45;&gt;140012866779216 140012866746512 data 1.0000 grad 0.4561 140012866746576* * 140012866746512&#45;&gt;140012866746576* 140012867926160 data &#45;0.1410 grad 1.4232 140012867703760+ + 140012867926160&#45;&gt;140012867703760+ 140012867926160+&#45;&gt;140012867926160 140012866746576 data &#45;0.8704 grad &#45;0.5240 140012866746576&#45;&gt;140012866746832+ 140012866746576*&#45;&gt;140012866746576 140012867205328 data &#45;0.0227 grad &#45;1.1506 140012867205328&#45;&gt;140012867205584+ 140012867205328*&#45;&gt;140012867205328 140012866779472 data &#45;1.1315 grad 0.1317 140012866779984+ + 140012866779472&#45;&gt;140012866779984+ 140012866779472+&#45;&gt;140012866779472 140012866746832 data 0.1184 grad &#45;0.5240 140012866747408+ + 140012866746832&#45;&gt;140012866747408+ 140012866746832+&#45;&gt;140012866746832 140012867205584 data 0.5050 grad &#45;1.1506 140012867206096+ + 140012867205584&#45;&gt;140012867206096+ 140012867205584+&#45;&gt;140012867205584 140012866779728 data &#45;0.0363 grad 0.1317 140012866779728&#45;&gt;140012866779984+ 140012866779728*&#45;&gt;140012866779728 140012866747088 data &#45;1.0000 grad &#45;0.1641 140012866747152* * 140012866747088&#45;&gt;140012866747152* 140012867205840 data 0.0601 grad &#45;1.1506 140012867205840&#45;&gt;140012867206096+ 140012867205840*&#45;&gt;140012867205840 140012866747152 data &#45;0.3132 grad &#45;0.5240 140012866747152&#45;&gt;140012866747408+ 140012866747152*&#45;&gt;140012866747152 140012866779984 data &#45;1.1677 grad 0.1317 140012866780496+ + 140012866779984&#45;&gt;140012866780496+ 140012866779984+&#45;&gt;140012866779984 140012867206096 data 0.5651 grad &#45;1.1506 140012867206608+ + 140012867206096&#45;&gt;140012867206608+ 140012867206096+&#45;&gt;140012867206096 140012866747408 data &#45;0.1948 grad &#45;0.5240 140012866747536tanh tanh 140012866747408&#45;&gt;140012866747536tanh 140012866747408+&#45;&gt;140012866747408 140012866780240 data 0.1230 grad 0.1317 140012866780240&#45;&gt;140012866780496+ 140012866780240*&#45;&gt;140012866780240 140012866747536 data &#45;0.1924 grad &#45;0.5442 140012866747536&#45;&gt;140012866777936* 140012866747536&#45;&gt;140012866780240* 140012866747536&#45;&gt;140012866790800* 140012866747536&#45;&gt;140012866763280* 140012866747536tanh&#45;&gt;140012866747536 140012867206352 data &#45;0.2014 grad &#45;1.1506 140012867206352&#45;&gt;140012867206608+ 140012867206352*&#45;&gt;140012867206352 140012866780496 data &#45;1.0447 grad 0.1317 140012866781008+ + 140012866780496&#45;&gt;140012866781008+ 140012866780496+&#45;&gt;140012866780496 140012867206608 data 0.3638 grad &#45;1.1506 140012867207120+ + 140012867206608&#45;&gt;140012867207120+ 140012867206608+&#45;&gt;140012867206608 140012866747920 data 1.0000 grad 0.0553 140012866747984* * 140012866747920&#45;&gt;140012866747984* 140012866747984 data &#45;0.7495 grad &#45;0.0738 140012866747984&#45;&gt;140012866748240+ 140012866747984*&#45;&gt;140012866747984 140012866780752 data &#45;0.1944 grad 0.1317 140012866780752&#45;&gt;140012866781008+ 140012866780752*&#45;&gt;140012866780752 140012867206864 data &#45;0.6645 grad &#45;1.1506 140012867206864&#45;&gt;140012867207120+ 140012867206864*&#45;&gt;140012867206864 140012866748240 data &#45;0.4141 grad &#45;0.0738 140012866761168+ + 140012866748240&#45;&gt;140012866761168+ 140012866748240+&#45;&gt;140012866748240 140012866781008 data &#45;1.2391 grad 0.1317 140012866781136tanh tanh 140012866781008&#45;&gt;140012866781136tanh 140012866781008+&#45;&gt;140012866781008 140012866781136 data &#45;0.8452 grad 0.4609 140012866781136&#45;&gt;140012866793104* 140012866781136tanh&#45;&gt;140012866781136 140012867207120 data &#45;0.3007 grad &#45;1.1506 140012867207120&#45;&gt;140012867223696tanh 140012867207120+&#45;&gt;140012867207120 140012867174480 data &#45;0.2770 grad 2.2789 140012867174480&#45;&gt;140012867174736+ 140012867174480*&#45;&gt;140012867174480 140012867109072 data 0.5000 grad &#45;0.3904 140012867109136* * 140012867109072&#45;&gt;140012867109136* 140012867141840 data &#45;0.1494 grad &#45;0.8904 140012867142352+ + 140012867141840&#45;&gt;140012867142352+ 140012867141840+&#45;&gt;140012867141840 140012866814160 data 1.0000 grad 2.7925 140012866814224+ + 140012866814160&#45;&gt;140012866814224+ 140012867109136 data &#45;0.4054 grad 0.4815 140012867109136&#45;&gt;140012867109392+ 140012867109136*&#45;&gt;140012867109136 140012866814224 data 1.3962 grad 2.7925 140012866814480**2 **2 140012866814224&#45;&gt;140012866814480**2 140012866814224+&#45;&gt;140012866814224 140012867174736 data &#45;0.4842 grad 2.2789 140012867175248+ + 140012867174736&#45;&gt;140012867175248+ 140012867174736+&#45;&gt;140012867174736 140012867142096 data 0.7235 grad &#45;0.8904 140012867142096&#45;&gt;140012867142352+ 140012867142096*&#45;&gt;140012867142096 140012867109392 data 0.0955 grad 0.4815 140012867109520tanh tanh 140012867109392&#45;&gt;140012867109520tanh 140012867109392+&#45;&gt;140012867109392 140012866814480 data 1.9495 grad 1.0000 140012866814480&#45;&gt;140012866814800+ 140012866814480**2&#45;&gt;140012866814480 140012867174992 data 0.3839 grad 2.2789 140012867174992&#45;&gt;140012867175248+ 140012867174992*&#45;&gt;140012867174992 140012867109520 data 0.0952 grad 0.4859 140012867109520&#45;&gt;140012867159824* 140012867109520&#45;&gt;140012867128208* 140012867109520&#45;&gt;140012867142864* 140012867109520&#45;&gt;140012867145168* 140012867109520tanh&#45;&gt;140012867109520 140012867142352 data 0.5741 grad &#45;0.8904 140012867142480tanh tanh 140012867142352&#45;&gt;140012867142480tanh 140012867142352+&#45;&gt;140012867142352 140012866814800 data 6.2318 grad 1.0000 140012866815696+ + 140012866814800&#45;&gt;140012866815696+ 140012866814800+&#45;&gt;140012866814800 140012867175248 data &#45;0.1003 grad 2.2789 140012867175760+ + 140012867175248&#45;&gt;140012867175760+ 140012867175248+&#45;&gt;140012867175248 140012867142480 data 0.5184 grad &#45;1.2176 140012867142480&#45;&gt;140012867174480* 140012867142480tanh&#45;&gt;140012867142480 140012867109904 data 3.0000 grad &#45;0.5051 140012867109968* * 140012867109904&#45;&gt;140012867109968* 140012866815056 data &#45;1.0000 grad &#45;1.6747 140012866815056&#45;&gt;140012866815120+ 140012867109968 data 1.7525 grad &#45;0.8646 140012867110224+ + 140012867109968&#45;&gt;140012867110224+ 140012867109968*&#45;&gt;140012867109968 140012867175504 data 0.2715 grad 2.2789 140012867175504&#45;&gt;140012867175760+ 140012867175504*&#45;&gt;140012867175504 140012867044432 data 2.0000 grad &#45;0.0021 140012867044496* * 140012867044432&#45;&gt;140012867044496* 140012866815120 data &#45;0.8374 grad &#45;1.6747 140012866815376**2 **2 140012866815120&#45;&gt;140012866815376**2 140012866815120+&#45;&gt;140012866815120 140012867044496 data 0.5688 grad &#45;0.0073 140012867044752+ + 140012867044496&#45;&gt;140012867044752+ 140012867044496*&#45;&gt;140012867044496 140012867142864 data &#45;0.0508 grad &#45;1.5814 140012867142864&#45;&gt;140012867143120+ 140012867142864*&#45;&gt;140012867142864 140012867110224 data 0.7601 grad &#45;0.8646 140012867110800+ + 140012867110224&#45;&gt;140012867110800+ 140012867110224+&#45;&gt;140012867110224 140012867175760 data 0.1712 grad 2.2789 140012867176272+ + 140012867175760&#45;&gt;140012867176272+ 140012867175760+&#45;&gt;140012867175760 140012866815376 data 0.7012 grad 1.0000 140012866815376&#45;&gt;140012866815696+ 140012866815376**2&#45;&gt;140012866815376 140012867044752 data 0.7014 grad &#45;0.0073 140012867045328+ + 140012867044752&#45;&gt;140012867045328+ 140012867044752+&#45;&gt;140012867044752 140012867143120 data &#45;0.3980 grad &#45;1.5814 140012867143632+ + 140012867143120&#45;&gt;140012867143632+ 140012867143120+&#45;&gt;140012867143120 140012867176016 data 0.0064 grad 2.2789 140012867176016&#45;&gt;140012867176272+ 140012867176016*&#45;&gt;140012867176016 140012867110480 data &#45;1.0000 grad &#45;0.3911 140012867110544* * 140012867110480&#45;&gt;140012867110544* 140012867110544 data &#45;0.4524 grad &#45;0.8646 140012867110544&#45;&gt;140012867110800+ 140012867110544*&#45;&gt;140012867110544 140012867045008 data 3.0000 grad &#45;0.0036 140012867045072* * 140012867045008&#45;&gt;140012867045072* 140012866815696 data 6.9330 grad 1.0000 140012866815696+&#45;&gt;140012866815696 140012867143376 data 0.2818 grad &#45;1.5814 140012867143376&#45;&gt;140012867143632+ 140012867143376*&#45;&gt;140012867143376 140012867045072 data 1.4545 grad &#45;0.0073 140012867045072&#45;&gt;140012867045328+ 140012867045072*&#45;&gt;140012867045072 140012867176272 data 0.1776 grad 2.2789 140012867176400tanh tanh 140012867176272&#45;&gt;140012867176400tanh 140012867176272+&#45;&gt;140012867176272 140012867110800 data 0.3077 grad &#45;0.8646 140012867111376+ + 140012867110800&#45;&gt;140012867111376+ 140012867110800+&#45;&gt;140012867110800 140012867176400 data 0.1758 grad 2.3516 140012867176400&#45;&gt;140012866809104+ 140012867176400tanh&#45;&gt;140012867176400 140012867143632 data &#45;0.1163 grad &#45;1.5814 140012867144144+ + 140012867143632&#45;&gt;140012867144144+ 140012867143632+&#45;&gt;140012867143632 140012867045328 data 2.1559 grad &#45;0.0073 140012867045904+ + 140012867045328&#45;&gt;140012867045904+ 140012867045328+&#45;&gt;140012867045328 140012867111056 data 0.5000 grad &#45;0.0833 140012867111120* * 140012867111056&#45;&gt;140012867111120* 140012867111120 data 0.0482 grad &#45;0.8646 140012867111120&#45;&gt;140012867111376+ 140012867111120*&#45;&gt;140012867111120 140012867143888 data &#45;0.3864 grad &#45;1.5814 140012867143888&#45;&gt;140012867144144+ 140012867143888*&#45;&gt;140012867143888 140012867045584 data &#45;1.0000 grad 0.0059 140012867045648* * 140012867045584&#45;&gt;140012867045648* 140012867045648 data 0.8108 grad &#45;0.0073 140012867045648&#45;&gt;140012867045904+ 140012867045648*&#45;&gt;140012867045648 140012867176784 data 0.5000 grad 0.1872 140012867176848* * 140012867176784&#45;&gt;140012867176848* 140012867176848 data 0.1422 grad 0.6582 140012867177104+ + 140012867176848&#45;&gt;140012867177104+ 140012867176848*&#45;&gt;140012867176848 140012867111376 data 0.3559 grad &#45;0.8646 140012867111504tanh tanh 140012867111376&#45;&gt;140012867111504tanh 140012867111376+&#45;&gt;140012867111376 140012867144144 data &#45;0.5027 grad &#45;1.5814 140012867144656+ + 140012867144144&#45;&gt;140012867144656+ 140012867144144+&#45;&gt;140012867144144 140012867045904 data 2.9667 grad &#45;0.0073 140012867046032tanh tanh 140012867045904&#45;&gt;140012867046032tanh 140012867045904+&#45;&gt;140012867045904 140012867111504 data 0.3416 grad &#45;0.9788 140012867111504&#45;&gt;140012867160336* 140012867111504&#45;&gt;140012867128720* 140012867111504&#45;&gt;140012867143376* 140012867111504&#45;&gt;140012867145680* 140012867111504tanh&#45;&gt;140012867111504 140012867046032 data 0.9947 grad &#45;0.6946 140012867046032&#45;&gt;140012867702992* 140012867046032&#45;&gt;140012867081168* 140012867046032&#45;&gt;140012867082704* 140012867046032&#45;&gt;140012867021200* 140012867046032tanh&#45;&gt;140012867046032 140012867177104 data 0.2748 grad 0.6582 140012867177680+ + 140012867177104&#45;&gt;140012867177680+ 140012867177104+&#45;&gt;140012867177104 140012867144400 data 0.0254 grad &#45;1.5814 140012867144400&#45;&gt;140012867144656+ 140012867144400*&#45;&gt;140012867144400 140012867177360 data 1.0000 grad 0.3191 140012867177424* * 140012867177360&#45;&gt;140012867177424* 140012867111888 data 3.0000 grad 0.0069 140012867111952* * 140012867111888&#45;&gt;140012867111952* 140012867144656 data &#45;0.4773 grad &#45;1.5814 140012867144784tanh tanh 140012867144656&#45;&gt;140012867144784tanh 140012867144656+&#45;&gt;140012867144656 140012867177424 data 0.4848 grad 0.6582 140012867177424&#45;&gt;140012867177680+ 140012867177424*&#45;&gt;140012867177424 140012867111952 data 0.4724 grad 0.0441 140012867112208+ + 140012867111952&#45;&gt;140012867112208+ 140012867111952*&#45;&gt;140012867111952 140012867046416 data 2.0000 grad 0.1370 140012867046480* * 140012867046416&#45;&gt;140012867046480* 140012867144784 data &#45;0.4441 grad &#45;1.9699 140012867144784&#45;&gt;140012867174992* 140012867144784tanh&#45;&gt;140012867144784 140012867046480 data 1.1683 grad 0.2345 140012867046736+ + 140012867046480&#45;&gt;140012867046736+ 140012867046480*&#45;&gt;140012867046480 140012867177680 data 0.7596 grad 0.6582 140012867178256+ + 140012867177680&#45;&gt;140012867178256+ 140012867177680+&#45;&gt;140012867177680 140012867112208 data 1.3037 grad 0.0441 140012867112784+ + 140012867112208&#45;&gt;140012867112784+ 140012867112208+&#45;&gt;140012867112208 140012867046736 data 0.1759 grad 0.2345 140012867047312+ + 140012867046736&#45;&gt;140012867047312+ 140012867046736+&#45;&gt;140012867046736 140012867145168 data &#45;0.0555 grad &#45;0.0501 140012867145168&#45;&gt;140012867145424+ 140012867145168*&#45;&gt;140012867145168 140012867177936 data 1.0000 grad &#45;0.5337 140012867178000* * 140012867177936&#45;&gt;140012867178000* 140012867112464 data &#45;1.0000 grad &#45;0.0384 140012867112528* * 140012867112464&#45;&gt;140012867112528* 140012867178000 data &#45;0.8108 grad 0.6582 140012867178000&#45;&gt;140012867178256+ 140012867178000*&#45;&gt;140012867178000 140012867112528 data 0.8704 grad 0.0441 140012867112528&#45;&gt;140012867112784+ 140012867112528*&#45;&gt;140012867112528 140012867046992 data 3.0000 grad 0.1061 140012867047056* * 140012867046992&#45;&gt;140012867047056* 140012867047056 data 1.3571 grad 0.2345 140012867047056&#45;&gt;140012867047312+ 140012867047056*&#45;&gt;140012867047056 140012867145424 data &#45;0.6410 grad &#45;0.0501 140012867145424&#45;&gt;140012867158288+ 140012867145424+&#45;&gt;140012867145424 140012867178256 data &#45;0.0511 grad 0.6582 140012867178384tanh tanh 140012867178256&#45;&gt;140012867178384tanh 140012867178256+&#45;&gt;140012867178256 140012867112784 data 2.1742 grad 0.0441 140012867112784&#45;&gt;140012867125712+ 140012867112784+&#45;&gt;140012867112784 140012867047312 data 1.5330 grad 0.2345 140012867047312&#45;&gt;140012867064336+ 140012867047312+&#45;&gt;140012867047312 140012867178384 data &#45;0.0511 grad 0.6599 140012867178384&#45;&gt;140012867224080* 140012867178384&#45;&gt;140012867226384* 140012867178384&#45;&gt;140012867232848* 140012867178384&#45;&gt;140012867205328* 140012867178384tanh&#45;&gt;140012867178384 140012867145680 data 0.2373 grad &#45;0.0501 140012867145680&#45;&gt;140012867158288+ 140012867145680*&#45;&gt;140012867145680 140012867080336 data &#45;1.0000 grad &#45;0.0623 140012867080400* * 140012867080336&#45;&gt;140012867080400* 140012867080400 data &#45;0.6675 grad &#45;0.0934 140012867080400&#45;&gt;140012867080656+ 140012867080400*&#45;&gt;140012867080400 140012867702992 data &#45;0.5796 grad 0.3212 140012867702992&#45;&gt;140012867704016+ 140012867702992*&#45;&gt;140012867702992 140012867703248 data 0.2141 grad 1.4232 140012867704080tanh tanh 140012867703248&#45;&gt;140012867704080tanh 140012867703248+ + 140012867703248+&#45;&gt;140012867703248 140012867080656 data 0.7796 grad &#45;0.0934 140012867080784tanh tanh 140012867080656&#45;&gt;140012867080784tanh 140012867080656+&#45;&gt;140012867080656 140012867080784 data 0.6525 grad &#45;0.1626 140012867080784&#45;&gt;140012867684368* 140012867080784&#45;&gt;140012867704592* 140012867080784&#45;&gt;140012867084240* 140012867080784&#45;&gt;140012867021904* 140012867080784tanh&#45;&gt;140012867080784 140012867703632 data 0.6204 grad 0.3212 140012867705744+ + 140012867703632&#45;&gt;140012867705744+ 140012867703632*&#45;&gt;140012867703632 140012867703760 data 0.2309 grad 1.4232 140012867703760&#45;&gt;140012867703248+ 140012867703760+&#45;&gt;140012867703760 140012867081168 data 0.4426 grad 0.5873 140012867081168&#45;&gt;140012867081424+ 140012867081168*&#45;&gt;140012867081168 140012867081424 data 0.9704 grad 0.5873 140012867081936+ + 140012867081424&#45;&gt;140012867081936+ 140012867081424+&#45;&gt;140012867081424 140012867704016 data &#45;1.1651 grad 0.3212 140012867704016&#45;&gt;140012867705744+ 140012867704016+&#45;&gt;140012867704016 140012867704080 data 0.2109 grad 1.4895 140012867704080&#45;&gt;140012867097872* 140012867704080tanh&#45;&gt;140012867704080 140012867081680 data &#45;0.3567 grad 0.5873 140012867081680&#45;&gt;140012867081936+ 140012867081680*&#45;&gt;140012867081680 140012867081936 data 0.6137 grad 0.5873 140012867022032+ + 140012867081936&#45;&gt;140012867022032+ 140012867081936+&#45;&gt;140012867081936 140012867704592 data &#45;0.0168 grad 1.4232 140012867704592&#45;&gt;140012867703248+ 140012867704592*&#45;&gt;140012867704592 140012867082192 data 0.6680 grad 0.3212 140012867082320tanh tanh 140012867082192&#45;&gt;140012867082320tanh 140012867082192+&#45;&gt;140012867082192 140012867082320 data 0.5836 grad 0.4871 140012867082320&#45;&gt;140012867098384* 140012867082320tanh&#45;&gt;140012867082320 140012867705104 data 0.3719 grad 1.4232 140012867705104&#45;&gt;140012867703760+ 140012867705104*&#45;&gt;140012867705104 140012867082704 data &#45;0.5245 grad 0.0188 140012867082704&#45;&gt;140012867082960+ 140012867082704*&#45;&gt;140012867082704 140012867082960 data &#45;0.4654 grad 0.0188 140012867083472+ + 140012867082960&#45;&gt;140012867083472+ 140012867082960+&#45;&gt;140012867082960 140012867705744 data &#45;0.5447 grad 0.3212 140012867705744&#45;&gt;140012867684560+ 140012867705744+&#45;&gt;140012867705744 140012867083216 data &#45;0.2318 grad 0.0188 140012867083216&#45;&gt;140012867083472+ 140012867083216*&#45;&gt;140012867083216 140012867083472 data &#45;0.6972 grad 0.0188 140012867083984+ + 140012867083472&#45;&gt;140012867083984+ 140012867083472+&#45;&gt;140012867083472 140012867083728 data 0.2368 grad 0.0188 140012867083728&#45;&gt;140012867083984+ 140012867083728*&#45;&gt;140012867083728 140012867083984 data &#45;0.4604 grad 0.0188 140012867083984&#45;&gt;140012867096848+ 140012867083984+&#45;&gt;140012867083984 140012867084240 data 0.1948 grad 0.0188 140012867084240&#45;&gt;140012867096848+ 140012867084240*&#45;&gt;140012867084240 140012867248272 data 0.4135 grad 2.3540 140012867248784+ + 140012867248272&#45;&gt;140012867248784+ 140012867248272+&#45;&gt;140012867248272 140012866789776 data &#45;0.4940 grad 0.0159 140012866789776&#45;&gt;140012866790032+ 140012866789776*&#45;&gt;140012866789776 140012867248528 data 0.0092 grad 2.3540 140012867248528&#45;&gt;140012867248784+ 140012867248528*&#45;&gt;140012867248528 140012866790032 data &#45;0.4349 grad 0.0159 140012866790544+ + 140012866790032&#45;&gt;140012866790544+ 140012866790032+&#45;&gt;140012866790032 140012867248784 data 0.4227 grad 2.3540 140012867249296+ + 140012867248784&#45;&gt;140012867249296+ 140012867248784+&#45;&gt;140012867248784 140012866790288 data 0.0136 grad 0.0159 140012866790288&#45;&gt;140012866790544+ 140012866790288*&#45;&gt;140012866790288 140012867249040 data &#45;0.0035 grad 2.3540 140012867249040&#45;&gt;140012867249296+ 140012867249040*&#45;&gt;140012867249040 140012866790544 data &#45;0.4214 grad 0.0159 140012866791056+ + 140012866790544&#45;&gt;140012866791056+ 140012866790544+&#45;&gt;140012866790544 140012867249296 data 0.4192 grad 2.3540 140012867249424tanh tanh 140012867249296&#45;&gt;140012867249424tanh 140012867249296+&#45;&gt;140012867249296 140012867249424 data 0.3962 grad 2.7925 140012867249424&#45;&gt;140012866814224+ 140012867249424tanh&#45;&gt;140012867249424 140012866790800 data 0.0482 grad 0.0159 140012866790800&#45;&gt;140012866791056+ 140012866790800*&#45;&gt;140012866790800 140012867249808 data 1.0000 grad &#45;0.0135 140012867249872* * 140012867249808&#45;&gt;140012867249872* 140012866791056 data &#45;0.3731 grad 0.0159 140012866791568+ + 140012866791056&#45;&gt;140012866791568+ 140012866791056+&#45;&gt;140012866791056 140012867249872 data 0.2844 grad &#45;0.0474 140012867250128+ + 140012867249872&#45;&gt;140012867250128+ 140012867249872*&#45;&gt;140012867249872 140012866791312 data &#45;0.0622 grad 0.0159 140012866791312&#45;&gt;140012866791568+ 140012866791312*&#45;&gt;140012866791312 140012867250128 data 0.4170 grad &#45;0.0474 140012867250704+ + 140012867250128&#45;&gt;140012867250704+ 140012867250128+&#45;&gt;140012867250128 140012867020752 data 0.6958 grad 0.5873 140012867021712tanh tanh 140012867020752&#45;&gt;140012867021712tanh 140012867020752+ + 140012867020752+&#45;&gt;140012867020752 140012866791568 data &#45;0.4353 grad 0.0159 140012866791696tanh tanh 140012866791568&#45;&gt;140012866791696tanh 140012866791568+&#45;&gt;140012866791568 140012867250384 data 1.0000 grad &#45;0.0230 140012867250448* * 140012867250384&#45;&gt;140012867250448* 140012867250448 data 0.4848 grad &#45;0.0474 140012867250448&#45;&gt;140012867250704+ 140012867250448*&#45;&gt;140012867250448 140012866791696 data &#45;0.4098 grad 0.0191 140012866791696&#45;&gt;140012866805968* 140012866791696tanh&#45;&gt;140012866791696 140012867021200 data &#45;0.5304 grad 1.4232 140012867021200&#45;&gt;140012867036112+ 140012867021200*&#45;&gt;140012867021200 140012867250704 data 0.9018 grad &#45;0.0474 140012867251280+ + 140012867250704&#45;&gt;140012867251280+ 140012867250704+&#45;&gt;140012867250704 140012866792080 data &#45;0.4506 grad &#45;1.6304 140012866792080&#45;&gt;140012866792336+ 140012866792080*&#45;&gt;140012866792080 140012867250960 data &#45;1.0000 grad 0.0384 140012867251024* * 140012867250960&#45;&gt;140012867251024* 140012867251024 data 0.8108 grad &#45;0.0474 140012867251024&#45;&gt;140012867251280+ 140012867251024*&#45;&gt;140012867251024 140012866792336 data &#45;0.6579 grad &#45;1.6304 140012866792848+ + 140012866792336&#45;&gt;140012866792848+ 140012866792336+&#45;&gt;140012866792336 140012867021712 data 0.6017 grad 0.9207 140012867021712&#45;&gt;140012867097360* 140012867021712tanh&#45;&gt;140012867021712 140012867251280 data 1.7126 grad &#45;0.0474 140012867251408tanh tanh 140012867251280&#45;&gt;140012867251408tanh 140012867251280+&#45;&gt;140012867251280 140012867021904 data &#45;0.4791 grad 0.5873 140012867021904&#45;&gt;140012867020752+ 140012867021904*&#45;&gt;140012867021904 140012866792592 data 0.5782 grad &#45;1.6304 140012866792592&#45;&gt;140012866792848+ 140012866792592*&#45;&gt;140012866792592 140012867251408 data 0.9370 grad &#45;0.3884 140012867251408&#45;&gt;140012866779216* 140012867251408&#45;&gt;140012866789776* 140012867251408&#45;&gt;140012866762256* 140012867251408&#45;&gt;140012866764560* 140012867251408tanh&#45;&gt;140012867251408 140012867022032 data 1.1750 grad 0.5873 140012867022032&#45;&gt;140012867020752+ 140012867022032+&#45;&gt;140012867022032 140012866792848 data &#45;0.0796 grad &#45;1.6304 140012866793360+ + 140012866792848&#45;&gt;140012866793360+ 140012866792848+&#45;&gt;140012866792848 140012867251792 data 1.0000 grad 0.3666 140012867251856* * 140012867251792&#45;&gt;140012867251856* 140012867251856 data 0.5842 grad 0.6275 140012867252112+ + 140012867251856&#45;&gt;140012867252112+ 140012867251856*&#45;&gt;140012867251856 140012866793104 data 0.2389 grad &#45;1.6304 140012866793104&#45;&gt;140012866793360+ 140012866793104*&#45;&gt;140012866793104 140012867252112 data &#45;0.4083 grad 0.6275 140012867252112&#45;&gt;140012866744848+ 140012867252112+&#45;&gt;140012867252112 140012866793360 data 0.1593 grad &#45;1.6304 140012866793360&#45;&gt;140012866806224+ 140012866793360+&#45;&gt;140012866793360 140012867022800 data 0.5612 grad 0.5873 140012867022800&#45;&gt;140012867022032+ 140012867022800*&#45;&gt;140012867022800 140012866760848 data 1.0000 grad &#45;0.0642 140012866760912* * 140012866760848&#45;&gt;140012866760912* 140012866760912 data 0.8702 grad &#45;0.0738 140012866760912&#45;&gt;140012866761168+ 140012866760912*&#45;&gt;140012866760912 140012867187024 data 0.5000 grad &#45;0.6789 140012867187088* * 140012867187024&#45;&gt;140012867187088* 140012867187088 data 0.2921 grad &#45;1.1622 140012867187344+ + 140012867187088&#45;&gt;140012867187344+ 140012867187088*&#45;&gt;140012867187088 140012866761168 data 0.4561 grad &#45;0.0738 140012866761744+ + 140012866761168&#45;&gt;140012866761744+ 140012866761168+&#45;&gt;140012866761168 140012867973712 data 0.2844 grad 1.7116 140012867973712&#45;&gt;140012867099728* 140012867973712&#45;&gt;140012867044496* 140012867973712&#45;&gt;140012867176848* 140012867973712&#45;&gt;140012867249872* 140012867187344 data &#45;0.7004 grad &#45;1.1622 140012867187920+ + 140012867187344&#45;&gt;140012867187920+ 140012867187344+&#45;&gt;140012867187344 140012866761424 data &#45;1.0000 grad &#45;0.0492 140012866761488* * 140012866761424&#45;&gt;140012866761488* 140012866761488 data &#45;0.6675 grad &#45;0.0738 140012866761488&#45;&gt;140012866761744+ 140012866761488*&#45;&gt;140012866761488 140012867973968 data 0.4524 grad 1.0333 140012867973968&#45;&gt;140012866744592* 140012867973968&#45;&gt;140012867110544* 140012867973968&#45;&gt;140012867047056* 140012867187664* * 140012867973968&#45;&gt;140012867187664* 140012867187600 data 1.0000 grad &#45;0.5257 140012867187600&#45;&gt;140012867187664* 140012867187664 data 0.4524 grad &#45;1.1622 140012867187664&#45;&gt;140012867187920+ 140012867187664*&#45;&gt;140012867187664 140012867974160 data 0.1326 grad 1.0850 140012867974160&#45;&gt;140012867099984+ 140012867974160&#45;&gt;140012867044752+ 140012867974160&#45;&gt;140012867177104+ 140012867974160&#45;&gt;140012867250128+ 140012866761744 data &#45;0.2114 grad &#45;0.0738 140012866761872tanh tanh 140012866761744&#45;&gt;140012866761872tanh 140012866761744+&#45;&gt;140012866761744 140012866761872 data &#45;0.2083 grad &#45;0.0771 140012866761872&#45;&gt;140012866778448* 140012866761872&#45;&gt;140012866780752* 140012866761872&#45;&gt;140012866791312* 140012866761872&#45;&gt;140012866763792* 140012866761872tanh&#45;&gt;140012866761872 140012867187920 data &#45;0.2480 grad &#45;1.1622 140012867188496+ + 140012867187920&#45;&gt;140012867188496+ 140012867187920+&#45;&gt;140012867187920 140012867974608 data 0.7366 grad 1.4232 140012867974608&#45;&gt;140012867926160+ 140012867974608*&#45;&gt;140012867974608 140012867188176 data 1.0000 grad &#45;0.1120 140012867188240* * 140012867188176&#45;&gt;140012867188240* 140012866762256 data 0.4169 grad 0.2515 140012866762256&#45;&gt;140012866762512+ 140012866762256*&#45;&gt;140012866762256 140012867188240 data 0.0964 grad &#45;1.1622 140012867188240&#45;&gt;140012867188496+ 140012867188240*&#45;&gt;140012867188240 140012866762512 data 0.9447 grad 0.2515 140012866763024+ + 140012866762512&#45;&gt;140012866763024+ 140012866762512+&#45;&gt;140012866762512 140012867188496 data &#45;0.1516 grad &#45;1.1622 140012867188624tanh tanh 140012867188496&#45;&gt;140012867188624tanh 140012867188496+&#45;&gt;140012867188496 140012867188624 data &#45;0.1505 grad &#45;1.1891 140012867188624&#45;&gt;140012867224592* 140012867188624&#45;&gt;140012867226896* 140012867188624&#45;&gt;140012867233360* 140012867188624&#45;&gt;140012867205840* 140012867188624tanh&#45;&gt;140012867188624 140012867975120 data &#45;0.8108 grad 0.9537 140012867975120&#45;&gt;140012867109136* 140012867975120&#45;&gt;140012867045648* 140012867975120&#45;&gt;140012867178000* 140012867975120&#45;&gt;140012867251024* 140012866762768 data 0.0209 grad 0.2515 140012866762768&#45;&gt;140012866763024+ 140012866762768*&#45;&gt;140012866762768 140012866763024 data 0.9656 grad 0.2515 140012866763536+ + 140012866763024&#45;&gt;140012866763536+ 140012866763024+&#45;&gt;140012866763024 140012867975440 data 0.3132 grad 2.1580 140012867975440&#45;&gt;140012867125456* 140012867975440&#45;&gt;140012867066064* 140012867975440&#45;&gt;140012866747152* 140012867190224* * 140012867975440&#45;&gt;140012867190224* 140012867189008 data 0.5000 grad 0.2348 140012867189072* * 140012867189008&#45;&gt;140012867189072* 140012867189072 data 0.0787 grad 1.4911 140012867189328+ + 140012867189072&#45;&gt;140012867189328+ 140012867189072*&#45;&gt;140012867189072 140012867975632 data &#45;0.8704 grad 0.5606 140012867975632&#45;&gt;140012867065488* 140012867975632&#45;&gt;140012866746576* 140012867975632&#45;&gt;140012867112528* 140012867189648* * 140012867975632&#45;&gt;140012867189648* 140012866763280 data 0.1143 grad 0.2515 140012866763280&#45;&gt;140012866763536+ 140012866763280*&#45;&gt;140012866763280 140012867189328 data 0.9101 grad 1.4911 140012867189904+ + 140012867189328&#45;&gt;140012867189904+ 140012867189328+&#45;&gt;140012867189328 140012866763536 data 1.0799 grad 0.2515 140012866764048+ + 140012866763536&#45;&gt;140012866764048+ 140012866763536+&#45;&gt;140012866763536 140012867189584 data 1.0000 grad &#45;1.2979 140012867189584&#45;&gt;140012867189648* 140012867189648 data &#45;0.8704 grad 1.4911 140012867189648&#45;&gt;140012867189904+ 140012867189648*&#45;&gt;140012867189648 140012866763792 data 0.1530 grad 0.2515 140012866763792&#45;&gt;140012866764048+ 140012866763792*&#45;&gt;140012866763792 140012867976272 data &#45;0.9924 grad &#45;1.1648 140012867976272&#45;&gt;140012867110224+ 140012867976272&#45;&gt;140012867046736+ 140012867976272&#45;&gt;140012867252112+ 140012867976272&#45;&gt;140012867187344+ 140012867189904 data 0.0396 grad 1.4911 140012867190480+ + 140012867189904&#45;&gt;140012867190480+ 140012867189904+&#45;&gt;140012867189904 140012866764048 data 1.2328 grad 0.2515 140012866764176tanh tanh 140012866764048&#45;&gt;140012866764176tanh 140012866764048+&#45;&gt;140012866764048 140012867976464 data 0.8702 grad &#45;0.3266 140012867976464&#45;&gt;140012867126864* 140012867976464&#45;&gt;140012867067472* 140012867976464&#45;&gt;140012867203984* 140012867976464&#45;&gt;140012866760912* 140012867976528 data 0.5842 grad &#45;2.0784 140012867976528&#45;&gt;140012867109968* 140012867976528&#45;&gt;140012867046480* 140012867976528&#45;&gt;140012867251856* 140012867976528&#45;&gt;140012867187088* 140012866764176 data 0.8434 grad 0.8711 140012866764176&#45;&gt;140012866792080* 140012866764176tanh&#45;&gt;140012866764176 140012867976592 data 0.0964 grad &#45;2.4564 140012867976592&#45;&gt;140012867064080* 140012867976592&#45;&gt;140012866745168* 140012867976592&#45;&gt;140012867111120* 140012867976592&#45;&gt;140012867188240* 140012867190160 data 1.0000 grad 0.4670 140012867190160&#45;&gt;140012867190224* 140012867190224 data 0.3132 grad 1.4911 140012867190224&#45;&gt;140012867190480+ 140012867190224*&#45;&gt;140012867190224 140012867976720 data &#45;0.7495 grad &#45;0.1808 140012867976720&#45;&gt;140012867126288* 140012867976720&#45;&gt;140012867066896* 140012867976720&#45;&gt;140012867203408* 140012867976720&#45;&gt;140012866747984* 140012867976848 data 0.8313 grad 0.8904 140012867976848&#45;&gt;140012867065168+ 140012867976848&#45;&gt;140012866746256+ 140012867976848&#45;&gt;140012867112208+ 140012867976848&#45;&gt;140012867189328+ 140012867976912 data 0.6675 grad 0.2226 140012867976912&#45;&gt;140012867127440* 140012867976912&#45;&gt;140012867204560* 140012867976912&#45;&gt;140012867080400* 140012867976912&#45;&gt;140012866761488* 140012867190480 data 0.3528 grad 1.4911 140012867190608tanh tanh 140012867190480&#45;&gt;140012867190608tanh 140012867190480+&#45;&gt;140012867190480 140012867976976 data 0.1575 grad 0.1122 140012867976976&#45;&gt;140012867064912* 140012867976976&#45;&gt;140012866746000* 140012867976976&#45;&gt;140012867111952* 140012867976976&#45;&gt;140012867189072* 140012866764560 data &#45;0.4996 grad 0.7787 140012866764560&#45;&gt;140012866777168+ 140012866764560*&#45;&gt;140012866764560 140012867190608 data 0.3389 grad 1.6846 140012867190608&#45;&gt;140012867225104* 140012867190608&#45;&gt;140012867227408* 140012867190608&#45;&gt;140012867233872* 140012867190608&#45;&gt;140012867206352* 140012867190608tanh&#45;&gt;140012867190608 140012867977168 data 0.4848 grad 0.1073 140012867977168&#45;&gt;140012867100304* 140012867977168&#45;&gt;140012867045072* 140012867977168&#45;&gt;140012867177424* 140012867977168&#45;&gt;140012867250448* We can update our parameters by multiplying by substracting them by their gradients multiplied by a learning rate (the desired impact of the gradient has in updating our parameters, here we use 0.01 arbitrarily, but there are many techniques to find an optimal learning rate and to decay the rate as training continues, 0.1 is also a good rule of thumb, too big and you can overstep too small and its costly to train . for p in n.parameters(): p.data -= 0.01 * p.grad . n.layers[0].neurons[0].w[0].data . 0.2672732181430417 . After updating our parameters we can confirm our loss decreased . ypred = [n(x) for x in xs] loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) loss . Value(data=6.317575661771169) . What we have done is gradient decent. Forward pass -&gt; backward pass -&gt; update the parameters. Now we just have to iterate this process, lets turn this manual process into a training loop . for k in range(10): # forward pass ypred = [n(x) for x in xs] loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) #backward pass loss.backward() #update for p in n.parameters(): p.data -= 0.05 * p.grad p.grad = 0 print(k, loss.data) . 0 0.10345239945960188 1 0.0941558588278075 2 0.0862952014023131 3 0.07956846088984432 4 0.07375215882362336 5 0.06867739459504289 6 0.06421419054843136 7 0.06026093580621215 8 0.056737086525177276 9 0.05357800634860667 . ypred . [Value(data=0.8741761793258574), Value(data=-0.8786120732704685), Value(data=-0.8952118169896238), Value(data=0.8903150856076475)] .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/07/micrograd.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/07/micrograd.html",
            "date": " ‚Ä¢ Oct 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Spaceship Titanic üö¢",
            "content": "Content: . Data Preprocessing | Binary Splits | Decision Tree | Random Forest | XGBoost | Results | File and Data Field Descriptions . train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data. PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. | HomePlanet - The planet the passenger departed from, typically their planet of permanent residence. | CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. | Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. | Destination - The planet the passenger will be debarking to. | Age - The age of the passenger. | VIP - Whether the passenger has paid for special VIP service during the voyage. | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic&#39;s many luxury amenities. | Name - The first and last names of the passenger. | Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. | . | test.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set. sample_submission.csv - A submission file in the correct format. PassengerId - Id for each passenger in the test set. | Transported - The target. For each passenger, predict either True or False. | . | . . Image made using stable diffusion in the dreamstudio beta . code to help grab the dataset if outside of kaggle, I&#39;ll comment this out so kaggle doesn&#39;t have any issues . from fastkaggle import setup_comp path = setup_comp(&#39;spaceship-titanic&#39;) . Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run &#39;chmod 600 /home/terps/.kaggle/kaggle.json&#39; Downloading spaceship-titanic.zip to /home/terps/.git/anything/_notebooks . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299k/299k [00:00&lt;00:00, 21.1MB/s] . . . Grabbing the training and test data and replacing the Transported boolean column with 0s and 1s . import pandas as pd df = pd.read_csv(&#39;spaceship-titanic/train.csv&#39;) tst_df = pd.read_csv(&#39;spaceship-titanic/test.csv&#39;) modes = df.mode().iloc[0] df[&#39;Transported&#39;] = df[&#39;Transported&#39;].apply(lambda x: 1 if x == True else 0) . . Data preprocessing . To smooth the outliers in RoomService, FoodCourt, ShoppingMall, and Spa; lets take the log of these columns | Both the Cabin and Name column contain extra information that may be useful to our model if we split them into seperate columns, lets do that also | . import numpy as np def add_features(df): df.fillna(modes, inplace=True) df[&#39;LogRoomService&#39;] = np.log(df[&#39;RoomService&#39;]+1) df[&#39;LogFoodCourt&#39;] = np.log(df[&#39;FoodCourt&#39;]+1) df[&#39;LogShoppingMall&#39;] = np.log(df[&#39;ShoppingMall&#39;]+1) df[&#39;LogSpa&#39;] = np.log(df[&#39;Spa&#39;]+1) df[&#39;LogVRDeck&#39;] = np.log(df[&#39;VRDeck&#39;]+1) df[[&#39;Deck&#39;,&#39;CabinNumber&#39;,&#39;Side&#39;]] = df.Cabin.str.split(&#39;/&#39;, expand=True) df[[&#39;Group&#39;, &#39;PassengerNumber&#39;]] = df.PassengerId.str.split(&#39;_&#39;, expand=True) df[[&#39;FirstName&#39;, &#39;LastName&#39;]] = df.Name.str.split(&#39; &#39;, expand=True) add_features(df) add_features(tst_df) . Here we are just using pandas Categorical method to assign numerical codes to the columns . def proc_data(df): df.fillna(modes, inplace=True) df[&#39;HomePlanet&#39;] = pd.Categorical(df.HomePlanet) df[&quot;CryoSleep&quot;] = pd.Categorical(df.CryoSleep) df[&#39;Destination&#39;] = pd.Categorical(df.Destination) df[&#39;Deck&#39;] = pd.Categorical(df.Deck) df[&#39;CabinNumber&#39;] = pd.Categorical(df.HomePlanet) df[&#39;Side&#39;] = pd.Categorical(df.Side) df[&#39;Group&#39;] = pd.Categorical(df.Group) df[&#39;PassengerNumber&#39;] = pd.Categorical(df.PassengerNumber) df[&#39;FirstName&#39;] = pd.Categorical(df.FirstName) df[&#39;LastName&#39;] = pd.Categorical(df.LastName) proc_data(df) proc_data(tst_df) . df.CryoSleep.cat.codes.head() . 0 0 1 0 2 0 3 0 4 0 dtype: int8 . cats=[&quot;HomePlanet&quot;, &quot;CryoSleep&quot;, &quot;Destination&quot;, &quot;Deck&quot;, &quot;CabinNumber&quot;, &quot;Side&quot;, &quot;Group&quot;, &quot;PassengerNumber&quot;, &quot;FirstName&quot;, &quot;LastName&quot;] conts=[&#39;Age&#39;, &#39;LogRoomService&#39;, &#39;LogFoodCourt&#39;, &#39;LogShoppingMall&#39;, &#39;LogSpa&#39;, &#39;LogVRDeck&#39;] dep=&quot;Transported&quot; . . Binary splits . It may be useful to look at a histogram of the transportation rate of a given column and the respective overall count for the column | . import seaborn as sns import matplotlib.pyplot as plt fix,axs = plt.subplots(1, 2, figsize=(11, 5)) sns.barplot(data=df, y=dep, x=&quot;CryoSleep&quot;, ax=axs[0]).set(title=&quot;Transportation rate&quot;) sns.countplot(data=df, x=&#39;CryoSleep&#39;, ax=axs[1]).set(title=&#39;Histogram&#39;) . [Text(0.5, 1.0, &#39;Histogram&#39;)] . Now that we have our dataset cleaned up a bit and columns split into categories, continous, and dependent columns we can split our data randomly into training and validation sets . from numpy import random from sklearn.model_selection import train_test_split random.seed(42) trn_df, val_df = train_test_split(df, test_size=0.25) trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes) val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes) . def xs_y(df): xs = df[cats+conts].copy() return xs, df[dep] if dep in df else None trn_xs, trn_y = xs_y(trn_df) val_xs, val_y = xs_y(val_df) . Lets look at predictions for just passengers who where in cryosleep . preds = val_xs.CryoSleep==1 val_y . 304 1 2697 0 8424 0 1672 1 8458 1 .. 4478 1 2996 1 7760 1 8181 0 8420 0 Name: Transported, Length: 2174, dtype: int64 . We can look at the MAE using these cryosleep predictions . from sklearn.metrics import mean_absolute_error mean_absolute_error(val_y, preds) . 0.2755289788408464 . Not bad, looks like cryosleep is a good indicator of being transported . We can look at numeric columns with boxen and kde plots to see if there are anything that stands out . df_age = trn_df[trn_df.Age&gt;0] fig, axs = plt.subplots(1,2, figsize=(11, 5)) sns.boxenplot(data=df_age, x=dep, y=&quot;Age&quot;, ax=axs[0]) sns.kdeplot(data=df_age, x=&quot;Age&quot;, ax=axs[1]); . Not really conclusive quantiles for age and transportation . We can write a few functions to get scores for our columns . def _side_score(side, y): tot = side.sum() if tot&lt;=1: return 0 return y[side].std()*tot . def score(col, y, split): lhs = col&lt;=split return (_side_score(lhs, y) + _side_score(~lhs, y))/len(y) . score(trn_xs[&quot;CryoSleep&quot;], trn_y, 0.5) . 0.4427274084379482 . score(trn_xs[&#39;Age&#39;], trn_y, 20) . 0.49837479646647287 . def iscore(nm, split): col = trn_xs[nm] return score(col, trn_y, split) from ipywidgets import interact interact(nm=conts, split=15.5)(iscore); . interact(nm=cats, split=2)(iscore); . Instead of manually doing this lets write a function that can automatically find the best split point for all our columns . def min_col(df, nm): col, y = df[nm], df[dep] unq = col.dropna().unique() scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)]) idx = scores.argmin() return unq[idx], scores[idx] min_col(trn_df, &quot;Age&quot;) . (5.0, 0.49588222217088634) . cols = cats+conts {o:min_col(trn_df, o) for o in cols} . {&#39;HomePlanet&#39;: (0, 0.49262732535926257), &#39;CryoSleep&#39;: (0, 0.4427274084379482), &#39;Destination&#39;: (0, 0.4975146234286769), &#39;Deck&#39;: (2, 0.49146596918858604), &#39;CabinNumber&#39;: (0, 0.49262732535926257), &#39;Side&#39;: (0, 0.4972039930912765), &#39;Group&#39;: (2054, 0.4988487272764558), &#39;PassengerNumber&#39;: (0, 0.4982305020633713), &#39;FirstName&#39;: (66, 0.4995954899592851), &#39;LastName&#39;: (2184, 0.49916642378415255), &#39;Age&#39;: (5.0, 0.49588222217088634), &#39;LogRoomService&#39;: (0.0, 0.4712139445780011), &#39;LogFoodCourt&#39;: (0.0, 0.4864171323280328), &#39;LogShoppingMall&#39;: (0.0, 0.4838374399256237), &#39;LogSpa&#39;: (0.0, 0.46960050421496596), &#39;LogVRDeck&#39;: (0.0, 0.47094236980183707)} . . Decision Tree . Lets take a look at a simple decision tree classifier | . from sklearn.tree import DecisionTreeClassifier, export_graphviz m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y); . import graphviz import re def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(&#39;Tree {&#39;, f&#39;Tree {{ size={size}; ratio={ratio}&#39;, s)) . draw_tree(m, trn_xs, size=10) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 CryoSleep ‚â§ 0.5 gini = 0.5 samples = 6519 value = [3233, 3286] 1 LogRoomService ‚â§ 5.85 gini = 0.45 samples = 4267 value = [2825, 1442] 0&#45;&gt;1 True 2 gini = 0.3 samples = 2252 value = [408, 1844] 0&#45;&gt;2 False 3 LogSpa ‚â§ 5.33 gini = 0.48 samples = 3188 value = [1919, 1269] 1&#45;&gt;3 4 gini = 0.27 samples = 1079 value = [906, 173] 1&#45;&gt;4 5 gini = 0.5 samples = 2131 value = [1067, 1064] 3&#45;&gt;5 6 gini = 0.31 samples = 1057 value = [852, 205] 3&#45;&gt;6 We can see that CryoSleep is a important feature in predicting transportation . def gini(cond): act = df.loc[cond, dep] return 1 - act.mean()**2 - (1-act).mean()**2 . gini(df.CryoSleep==0), gini(df.CryoSleep==1) . (0.4455780020566211, 0.2982818967776309) . mae_tree = mean_absolute_error(val_y, m.predict(val_xs)) mae_tree . 0.22309107635694572 . Lets add more nodes to our tree and see if that lowers our MAE . m = DecisionTreeClassifier(min_samples_leaf=50) m.fit(trn_xs, trn_y) . DecisionTreeClassifier(min_samples_leaf=50) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(min_samples_leaf=50) . mean_absolute_error(val_y, m.predict(val_xs)) . 0.22309107635694572 . Nice it does! Lets submit this model . tst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes) tst_xs,_ = xs_y(tst_df) def subm(preds, suff): tst_df[&#39;Transported&#39;] = list(map(lambda x: True if x == 1 else False, preds)) sub_df = tst_df[[&#39;PassengerId&#39;, &#39;Transported&#39;]] sub_df.to_csv(f&#39;sub-{suff}.csv&#39;, index=False) subm(m.predict(tst_xs), &#39;tree&#39;) . . . Random Forest . Lets use an ensemble of decision trees to build a predictive model | . def get_tree(prop=0.75): n = len(trn_y) idxs = random.choice(n, int(n*prop)) return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs]) . trees = [get_tree() for t in range(100)] . all_probs = [t.predict(val_xs) for t in trees] avg_probs = np.stack(all_probs).mean(0) mean_absolute_error(val_y, avg_probs) . 0.26108095676172954 . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(100, min_samples_leaf=10) rf.fit(trn_xs, trn_y) mae = mean_absolute_error(val_y, rf.predict(val_xs)) mae . 0.21205151793928242 . subm(rf.predict(tst_xs), &#39;rf&#39;) . . Cool thing about random forests is the featureimportances method, that allows us to see what our model thinks are good predictive features for our dependent variable . pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . from sklearn.model_selection import cross_val_score . . XG Boost . import xgboost as xgb xg_reg = xgb.XGBClassifier(colsample_bytree = 0.7, learning_rate = 0.001, max_depth = 8, alpha = 1, gamma = 3, n_estimators = 400) scores = cross_val_score(xg_reg, trn_xs, trn_y, cv=5) scores.mean() . 0.8001234762628948 . This prediction score looks solid! Lets submit it to kaggle. . xg_reg.fit(trn_xs, trn_y) subm(xg_reg.predict(tst_xs), &#39;xgb&#39;) . . Lets compare the our models . pd.DataFrame(dict(cols=[&#39;Decision Tree&#39;, &#39;XBG&#39;, &#39;Random Forest&#39;], imp=[1-mae_tree, scores.mean(), 1 - mae])).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . As expected looks like XGB produced the most accurate model .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "date": " ‚Ä¢ Sep 9, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lesson 5 / Chapter 9 questions",
            "content": "What is a continuous variable? . numerical data that have a wide range of &#39;continuous&#39; values (ex: age) | . What is a categorical variable? . discrete levels that correspond to different categories | . Provide two of the words that are used for the possible values of a categorical variable. . Levels or categories | . What is a &quot;dense layer&quot;? . linear layer and one-hot encoding layers represent inputs | . How do entity embeddings reduce memory usage and speed up neural networks? . Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model. | . What kinds of datasets are entity embeddings especially useful for? . When the datasets have features with high levels of cardinality (high dimensionality) | . What are the two main families of machine learning algorithms? . Ensembles of decision trees | Multilayered neural networks learned with SGD(i.e, shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) | . Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas? . The data is ordinal | df.cat.set_categories([&#39;columns&#39;]*, ordered=True, inplace=True) | . Summarize what a decision tree algorithm does. . Asks a series of binary questions about the data and splits until a leaf node where the prediction is returned | . Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? . While a date can be treated as ordinal, some dates are qualitatively different from others in ways that might be useful to the systems we are modeling. | We replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful. | . Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? . No, for time-series data we need our model to be able to predict the future | We want the validation set to be later in time than the training set. | To do this is use np.where, a useful function that returns(as the first element of a tuple) the indices of all True values | . What is pickle and what is it useful for? . Python&#39;s system to save a Python object | Allows of to &#39;freeze&#39; a file | Side note: not a secure process and should trust files you unpickle | . How are mse, samples, and values calculated in the decision tree drawn in this chapter? . mse is the square root of the mean of the pred minus actual squared | samples where just 500 random permutations of indexes | Values is average value for all the records in the node | . How do we deal with outliers, before building a decision tree? . Check for errors in your data, the outliers might be an error(ie. in the bulldozer competition a large number of reported sales happened during 1000) | We can take the log of the numerical / continuous column to smooth the outlier | . How do we handle categorical variables in a decision tree? . Decision trees naturally split data in a way that gives the categorical variables context, but we can also use pandas get_dummies to replace a single categorical variable with multiple one_hot_encoded columns | . What is bagging? . An ensemble technique that aggregators multiple predictions from many models | . What is the difference between max_samples and max_features when creating a random forest? . max_samples defines how many rows to sample for training each tree | max_features defines how many columns to sample at each split point | . If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? . n_estimators is the number of trees we want | You can set the number as high as you have time to train, it will get more accurate with diminishing returns | Yes it can lead to over fitting | . In the section &quot;Creating a Random Forest&quot;, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest? . What is &quot;out-of-bag-error&quot;? . OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. | . Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? . Beneficial when we have only a small amount of training data | We can compare then to the training labels | . Explain why random forests are well suited to answering each of the following question: . How confident are we in our predictions using a particular row of data? We can use the standard deviation of predictions across the trees, instead of just the means | . | For predicting with a particular row of data, what were the most important factors, and how did they influence that - prediction? Year Made and Product Size - use treeinterpreter package to check how the prediction changes as it goes - waterfall plots | . | Which columns are the strongest predictors? The feature importance for our model show that the first few important columns have much higher importance scores that the rest. | . | How do predictions vary as we vary these columns? Look at partial dependence plots | . | . What&#39;s the purpose of removing unimportant variables? . Makes for a simpler, more interpretable model that is easier to maintain and roll out. | . What&#39;s a good type of plot for showing tree interpreter results? . cluster_columns | waterfall plots | . What is the &quot;extrapolation problem&quot;? . Hard fora model to extrapolate to data that&#39;s outside the domain of the training data. This is particularly important to random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better. | . How can you tell if your test or validation set is distributed in a different way than your training set? . We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets. | . Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? . This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. | . What is &quot;boosting&quot;? . We train a model that under-fits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of al the models to get the final prediction. | . How could we use embeddings with a random forest? Would we expect this to help? . Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. | . Why might we not always use a neural net for tabular modeling? . We might not use a NN because they are the hardest and longest to train, and more opaque. Instead, random forests should be the first choice/baseline, and neural networks could be tried to improve these results or add to an ensemble. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "relUrl": "/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "date": " ‚Ä¢ Sep 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lesson 4 / Chapter 10 questions",
            "content": "What is &quot;self-supervised learning&quot;? . Training s model using &#39;labels&#39; that are naturally part of the input data, rather than requiring separate external labels. | For instance: training a model to predict the next word in text. | . test = 4 test . 4 . What is a &quot;language model&quot;? . A model trained to predict the next word in text | . Why is a language model considered self-supervised? . We are not using external labels, but the input data itself. | . What are self-supervised models usually used for? . NLP, Computer Vision | . Why do we fine-tune language models? . Task specific corpus of information | . What are the three steps to create a state-of-the-art text classifier? . Pipeline: Transfer Learning &gt; Train on our task specific data &gt; classification | . How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? . To fine-tune our language model to predict the next word of a movie review | . What are the three steps to prepare your data for a language model? . Training Set, Validation Set, and Testing Set | . What is &quot;tokenization&quot;? Why do we need it? . Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model) | . Name three different approaches to tokenization. . Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning &quot;don&#39;t&quot; into &quot;do n&#39;t&quot;). Generally, punctuation marks are also split into separate tokens. | Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, &quot;occasion&quot; might be tokenized as &quot;o c ca sion.&quot; | Character-based: Split a sentence into its individual characters. | . What is xxbos? . Denotes the &#39;beginning of stream&#39; aka the start of the sentence or text block. | . List four rules that fastai applies to text during tokenization. . use defaults.text_proc_rules to check out the default rules | fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses) | replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it&#39;s repeated, then the character | replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it&#39;s repeated, then the word. | spec_add_spaces: Adds spaces around / and # | . Why are repeated characters replaced with a token showing the number of repetitions and the character that&#39;s repeated? . Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. | . What is &quot;numericalization&quot;? . Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab. | . Why might there be words that are replaced with the &quot;unknown word&quot; token? . All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with &quot;unknown word&quot; | . With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book&#39;s website.) . a. The dataset is split into 64 mini-streams (batch size) | b. Each batch has 64 rows (batch size) and 64 columns (sequence length) | c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64) | d. The second row of the first batch contains the beginning of the second mini-stream | e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128) | . Why do we need padding for text classification? Why don&#39;t we need it for language modeling? . To collate the batch, in language models it is not needed since all documents are concatenated. | . What does an embedding matrix for NLP contain? What is its shape? . Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. | . What is &quot;perplexity&quot;? . Metric in NLP for language models. It is the exponential of the loss. | . Why do we have to pass the vocabulary of the language model to the classifier data block? . To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning. | . What is &quot;gradual unfreezing&quot;? . This refers to unfreezing one laying at a time and fine-tuning the pre-trained model. | . Why is text generation always likely to be ahead of automatic identification of machine-generated texts? . The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/30/lesson4.html",
            "relUrl": "/fastpages/jupyter/2022/08/30/lesson4.html",
            "date": " ‚Ä¢ Aug 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lesson 3 / Chapter 4 questions",
            "content": "How is a grayscale image represented on a computer? How about a color image? . Images are represented by arrays of pixels values representing content for the image | Greyscale values range from 0-255, from white-black | . How are the files and folders in the MNIST_SAMPLE dataset structured? Why? . Train &amp; Valid -&gt; 3 &amp; 7 | To test our model on out of sample data | . Explain how the &quot;pixel similarity&quot; approach to classifying digits works. . Create an archetype for the class we want to identify (Defined as the pixel wise mean of all the values in the training set) | Calculate the pixel mean wise absolute difference -&gt; classify image based upon lowest distance | . What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. . odd_double = [x * 2 for x in [1,2,3,4] if x % 2 == 0 ] odd_double . [4, 8] . What is a &quot;rank-3 tensor&quot;? . Rank is the dimensionality of the tensor | The easiest way to identify the dimensionality is the number of indices you would need to reference a number within a tensor | a &quot;rank-3 tensor&quot; is a cuboid or stack of matrices | . What is the difference between tensor rank and shape? How do you get the rank from the shape? . Rank is the number of axis or dimensions in a tensor while shape is the size of each axis of a tensor. | . How do you get the rank from the shape? . The length of the tensor&#39;s shape is its rank. | . What are RMSE and L1 norm? . Root Mean Squared Error aka the L2 norm, and Mean absolute difference aka L1 norm, are two commonly used methods of measuring distance, focusing on the magnitudes of the differences. | . How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? . Pytorch and fastai libraries utilize C and CUDA to provide these | . Create a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. . What is broadcasting? . dimension matching to allow operations, PyTorch implements these features to make it easier to perform | . Are metrics generally calculated using the training set, or the validation set? Why? . Validation set, you want to test on out of sample data to see how the model will perform. | . What is SGD? . Stochastic Gradient Decent is the process we use to traverse a function to find the local minimum for space. | . Why does SGD use mini-batches? . computationally efficient | . What are the seven steps in SGD for machine learning? . Initialize the parameters | Calculate the predictions | Calculate the loss | Calculate the gradients | Step the weights | Repeat the process | Stop | . How do we initialize the weights in a model? . Randomness is recommended | . What is &quot;loss&quot;? . A loss is a function that measures your predictions versus the results of the data, large deviations would indicate a large loss while the opposite would also be true. | . Why can&#39;t we always use a high learning rate? . A high learning rate might skip over the minima for our loss space, while fast and computationally less expensive, it may be useful to dynamically adjust your step ** might be explaining step size fix later | . What is a &quot;gradient&quot;? . The first derivative of the loss function | . Do you need to know how to calculate gradients yourself? . No we use our handy tools provided by PyTorch, but it is helpful to understand whats happening under the hood. | . Why can&#39;t we use accuracy as a loss function? . A loss for binomial outcomes (0, 1) would have create a loss function with a derivative 0 | We need to be able to turn the dials and knobs on the weights and see the effect on the predictions | . Draw the sigmoid function. What is special about its shape? . # plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . What is the difference between a loss function and a metric? . The metric is the accuracy of the actual result we want to achieve, the loss tells us how wel | . What is the function to calculate new weights using a learning rate? . The optimizer step method | . What does the DataLoader class do? . Takes any python collection and turns it into an iterator over many batches. | . Write pseudocode showing the basic steps taken in each epoch for SGD. . # pred = model(x) # loss = loss_func(pred, y) # loss.backward() # parameters -= parameters.grad * lr . Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? . def create_tuple(a,b): return list(zip(a,b)) . What does view do in PyTorch? . Changes the shape of the tensor without changing its content | . What are the &quot;bias&quot; parameters in a neural network? Why do we need them? . The margin or padding on the for our output, keeps our model from predicting a zero value | . What does the @ operator do in Python? . Matrix multiplication | . What does the backward method do? . Return the gradients | . Why do we have to zero the gradients? . To avoid adding the previously stored gradient to the current gradient | . What information do we have to pass to Learner? . DataLoader, the model, optimization function, and the loss function | . Show Python or pseudocode for the basic steps of a training loop. . Show Python or pseudocode for the basic steps of a training loop. def train_epoch(model, lr, params): for xb, yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. . Rectified Linear Unit, basically just replaces any negative numbers with 0s in a linear function | . What is an &quot;activation function&quot;? . function that adds non-linearity to our model to decouple each linear equation from the rest, allowing for more complex functions, see universal approximation theorem | . What&#39;s the difference between F.relu and nn.ReLU? . Python function versus a PyTorch module. Can be called as a function | . The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? . Allows for deeper models with less parameters | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/27/lesson-3.html",
            "relUrl": "/fastpages/jupyter/2022/08/27/lesson-3.html",
            "date": " ‚Ä¢ Aug 27, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Hotdog or Not Hotdog",
            "content": "WATCH: @EmilyChangTV chats w/ Jian-Yang about his &quot;Not Hotdog&quot; app. Download it today (really): https://t.co/7N6a1Asfge #SiliconValleyHBO pic.twitter.com/99TBhaiHYk . &mdash; Tech At Bloomberg (@TechAtBloomberg) May 15, 2017 . # This code is only here to check that your internet is enabled. It doesn&#39;t do anything else. # Here&#39;s a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367 import socket,warnings try: socket.setdefaulttimeout(1) socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((&#39;1.1.1.1&#39;, 53)) except socket.error as ex: raise Exception(&quot;STOP: No internet. Click &#39;&gt;|&#39; in top right and set &#39;Internet&#39; switch to on&quot;) . # `!pip install -Uqq &lt;libraries&gt;` upgrades to the latest version of &lt;libraries&gt; # NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities import os iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: !pip install -Uqq fastai duckduckgo_search . In 2015 the idea of creating a computer system that could recognise birds was considered so outrageously challenging that it was the basis of this XKCD joke: . But today, we can do exactly that, in just a few minutes, using entirely free resources! . The basic steps we&#39;ll take are: . Use DuckDuckGo to search for images of &quot;hot dogs&quot; | Use DuckDuckGo to search for images of &quot;not hot dogs&quot; aka &quot;pizza&quot; | Fine-tune a pretrained neural network to recognise these two groups | Try running this model on a picture of a bird and see if it works. | Step 1: Download images of hot dog and not hotdog &#39;pizza&#39; . #import shutil #hutil.rmtree(&quot;/kaggle/working/hotdog_or_not&quot;) #shutil.rmtree(&quot;/kaggle/working/&quot;) . from duckduckgo_search import ddg_images from fastcore.all import * def search_images(term, max_images=30): print(f&quot;Searching for &#39;{term}&#39;&quot;) return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . Let&#39;s start by searching for a bird photo and seeing what kind of result we get. We&#39;ll start by getting URLs from a search: . # If you get a JSON error, just try running it again (it may take a couple of tries). urls = search_images(&#39;hotdog&#39;, max_images=1) urls[0] . ...and then download a URL and take a look at it: . from fastdownload import download_url dest = &#39;hotdog.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Now let&#39;s do the same with &quot;pizza photos&quot;: . download_url(search_images(&#39;pizza&#39;, max_images=1)[0], &#39;pizza.jpg&#39;, show_progress=False) Image.open(&#39;pizza.jpg&#39;).to_thumb(256,256) . Our searches seem to be giving reasonable results, so let&#39;s grab a few examples of each of &quot;hotdog&quot; and &quot;pizza&quot; photos, and save each group of photos to a different folder (I&#39;m also trying to grab a range of lighting conditions here): . searches = &#39;pizza&#39;,&#39;hotdog&#39; path = Path(&#39;hotdog_or_not&#39;) from time import sleep for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) # Pause between searches to avoid over-loading server download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) resize_images(path/o, max_size=400, dest=path/o) . Step 2: Train our model . Some photos might not download correctly which could cause our model training to fail, so we&#39;ll remove them: . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . To train a model, we&#39;ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model -- not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it: . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path, bs=32) dls.show_batch(max_n=8) . Here what each of the DataBlock parameters means: . blocks=(ImageBlock, CategoryBlock), . The inputs to our model are images, and the outputs are categories (in this case, &quot;hotdog&quot; or &quot;pizza&quot;). . get_items=get_image_files, . To find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path). . splitter=RandomSplitter(valid_pct=0.2, seed=42), . Split the data into training and validation sets randomly, using 20% of the data for the validation set. . get_y=parent_label, . The labels (y values) is the name of the parent of each file (i.e. the name of the folder they&#39;re in, which will be bird or forest). . item_tfms=[Resize(192, method=&#39;squish&#39;)] . Before training, resize each image to 192x192 pixels by &quot;squishing&quot; it (as opposed to cropping it). . Now we&#39;re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds...) . fastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we&#39;ll use that. . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) . Generally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run). . &quot;Fine-tuning&quot; a model means that we&#39;re starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it&#39;s important, check out the free fast.ai course. . Step 3: Use our model (and build your own!) . Let&#39;s see what our model thinks about that bird we downloaded at the start: . is_hotdog,_,probs = learn.predict(PILImage.create(&#39;hotdog.jpg&#39;)) print(f&quot;This is a: {is_hotdog}.&quot;) print(f&quot;Probability it&#39;s a hotdog: {probs[0]:.4f}&quot;) . Good job, resnet18. :) . So, as you see, in the space of a few years, creating computer vision classification models has gone from &quot;so hard it&#39;s a joke&quot; to &quot;trivially easy and free&quot;! . It&#39;s not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. It&#39;s moving so fast that even experts in the field have trouble predicting how it&#39;s going to impact society in the coming years. . One thing is clear -- it&#39;s important that we all do our best to understand this technology, because otherwise we&#39;ll get left behind! . Now it&#39;s your turn. Click &quot;Copy &amp; Edit&quot; and try creating your own image classifier using your own image searches! . If you enjoyed this, please consider clicking the &quot;upvote&quot; button in the top-right -- it&#39;s very encouraging to us notebook authors to know when people appreciate our work. .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "relUrl": "/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "date": " ‚Ä¢ Aug 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Greetings traveler, my name is terps and I will be your guide today. Blogging about stuff I don‚Äôt know so I can get to know it better üòÖ data-science -&gt; machine learning -&gt; deep learning, cyber-security research &gt; cryptography &gt; block-chain technology, meta-skills of decision making &amp; problem solving and any other fun topics that I stumble upon. Thanks for visiting you can reach me at admin@terpsfi.xyz if you wanna chat. This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://terpsfi.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://terpsfi.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}