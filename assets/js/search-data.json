{
  
    
        "post0": {
            "title": "Pytorch üî•",
            "content": "Quick shout-out to FSDL for the awesome course, check them out if you want to get right into the weeds with professional full-stack deep learning. .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/09/pytorch.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/09/pytorch.html",
            "date": " ‚Ä¢ Oct 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch Lightning üå©Ô∏è",
            "content": "",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/09/lightning.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/09/lightning.html",
            "date": " ‚Ä¢ Oct 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Micrograd üîß",
            "content": "A blog implementation of Andrej Karpathy&#39;s micrograd video . Work in progress will continually add to this blog . Content: . Derivative of a function | Core Value Object | Manual Backpropagation | Neuron Example | Backward function | More operations | Pytorch | . Lets get an intuitive understanding of what a derivative is . import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline . Lets use a simple quadratic function as an example . $f(x) = 3x^2 - 4x + 5$ . def f(x): return 3*x**2 - 4*x + 5 . f(3.0) . 20.0 . Take a look at the shape of the function, we can expect a parabola since we know its a quadratic function . xs = np.arange(-5, 5, 0.25) ys = f(xs) plt.plot(xs, ys) . [&lt;matplotlib.lines.Line2D at 0x7fe34e9aef90&gt;] . We know want to think through what is the derivative of this function at different points x, let refresh with the definition of a derivative . $f&#39;(x) = lim_{h to 0} frac{f(x+h)-f(x)}{h}$ . You are basically trying to see the level of sensitivty the function responds with by bumping any x value at any point slightly by this small number h &lt;/br&gt; Intuitively how would you expect this function to respond if we nudged x = 3.0 by this small postitive number h? The amount the x value responds tells you the strength of the slope . h = 0.0001 x = 3.0 print(f&#39;slope of function at x = {x}, slope = {(f(x + h) - f(x)) / h}&#39;) . slope of function at x = 3.0, slope = 14.000300000063248 . Lets do a hacky implementation with more variables &lt;/br&gt; Look at the function a*b + c in relation to the variables we assigned, imagine if you nudged each variables by a tiny amount would that result in our output being increased or decreased? &lt;/br&gt; If we were to slightly nudge each of our input varibles by the tiny amount h(amount approaching 0) we can approximate the instataneous rate of change by looking at the difference before and after over the amount we nudged by, this will give us the slope. . h = 0.0001 #inputs a = 2.0 b = -3.0 c = 10.0 #We wanna find the derivative of d with respect to a,b,c d1 = a*b + c a += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 4.0 d2 3.999699999999999 slope -3.000000000010772 . Lets do it with b now . d1 = a*b + c b += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 3.999699999999999 d2 3.99990001 slope 2.0001000000124947 . And c... . d1 = a*b + c c += h d2 = a*b + c print(&#39;d1&#39;, d1) print(&#39;d2&#39;, d2) print(&#39;slope&#39;, (d2 - d1)/h) . d1 3.99990001 d2 4.00000001 slope 0.9999999999976694 . Hopefully this has helped build an inuitive sense of what this derivative is telling you about the function, but now we want to move to neural networks, which will be massive mathmatical expressions, so we need some structures to maintain these expressions, so we will build out a value object that can keep track of state and allow us to do expressions . . Core Value Object . class Value: def __init__(self, data): self.data = data def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data) return out def __mul__(self, other): out = Value(self.data * other.data) return out a = Value(2.0) b = Value(-3.0) a + b a * b . Value(data=-6.0) . We use double underscore or dunder methods so python knows what to interally when we use operators such as print, +, -, &lt;/br&gt; So when we call a + b above what is happening interally is a.__add__(b) with self as a and b as other. Similarly when we do a b, python is callinga.mul(b) . Cool so now we can do basic arthmetic now we need to add pointers to what see values produce other values and by what operations . class Value: def __init__(self, data, _children=(), _op=&#39;&#39;, label=&#39;&#39;): self.data = data self._prev = set(_children) self._op = _op self.label = label self.grad = 0.0 def __repr__(self): return f&quot;Value(data={self.data})&quot; def __add__(self, other): out = Value(self.data + other.data, (self, other), &#39;+&#39;) return out def __mul__(self, other): out = Value(self.data * other.data, (self, other), &#39;*&#39;) return out a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L . Value(data=-8.0) . Now that we have the a way to store the parents, labels and operations we can visualize them with an expression graph, code below allows us to do that using a library called graphviz . from graphviz import Digraph def trace(root): # builds a set of all nodes and edges in a graph nodes, edges = set(), set() def build(v): if v not in nodes: nodes.add(v) for child in v._prev: edges.add((child, v)) build(child) build(root) return nodes, edges def draw_dot(root): dot = Digraph(format=&#39;svg&#39;, graph_attr={&#39;rankdir&#39;: &#39;LR&#39;}) # LR = left to right nodes, edges = trace(root) for n in nodes: uid = str(id(n)) # for any value in the graph, create a rectangular (&#39;record&#39;) node for it dot.node(name = uid, label = &quot;{ %s | data %.4f | grad %.4f}&quot; % (n.label, n.data, n.grad), shape=&#39;record&#39;) if n._op: # if this value is a result of some operation, create an op node for it dot.node(name = uid + n._op, label = n._op) # and connect this node to it dot.edge(uid + n._op, uid) for n1, n2 in edges: # connect n1 to the op node of n2 dot.edge(str(id(n1)), str(id(n2)) + n2._op) return dot . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140614114641424 L data &#45;8.0000 grad 0.0000 140614114641424* * 140614114641424*&#45;&gt;140614114641424 140614114641488 f data &#45;2.0000 grad 0.0000 140614114641488&#45;&gt;140614114641424* 140614114647248 c data 10.0000 grad 0.0000 140614114641232+ + 140614114647248&#45;&gt;140614114641232+ 140614114647312 e data &#45;6.0000 grad 0.0000 140614114647312&#45;&gt;140614114641232+ 140614114647312* * 140614114647312*&#45;&gt;140614114647312 140614114641232 d data 4.0000 grad 0.0000 140614114641232&#45;&gt;140614114641424* 140614114641232+&#45;&gt;140614114641232 140614114640720 a data 2.0000 grad 0.0000 140614114640720&#45;&gt;140614114647312* 140614114640784 b data &#45;3.0000 grad 0.0000 140614114640784&#45;&gt;140614114647312* Lets recap: . we are able to build out mathematical expressions using + and * | Added and tracked grad so we can calculate and update this state later when we do backpropagtion | Forward pass that produces output L and visualized | Now we want to do backpropagation | . . Manual Backpropagation . Lets manually nudge the variable a and manually calculate the derivative of L with respect to a, lets create a gating function lol so we don&#39;t pollute the global scope. We can do this for each variable to calculate their derivative with respect to L . . $L = d * f$ . $ frac{dL}{dd} =? f$ . $lim_{h to 0} frac{(d+h)*f - d*f}{h}$ . $lim_{h to 0} frac{d*f + h*f - d*f}{h}$ . $lim_{h to 0} frac{h*f}{h}$ . $f$ . So we can see that d.grad is just the value of f which is -2.0 and by the property of symmetry f.grad is just the value of d which is 4.0, lets go ahead and manually set these . f.grad = 4.0 d.grad = -2.0 . L.grad = 1 . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data # this is the variable we are nudging by h a = Value(2.0 , label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; d.data += h f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . -1.9999999999953388 . So we have just derived the derivates of f and d with respect to L in a step by step way, now next will uncover the core of backpropagation, we want derive the derivative of L with respect to c and e. &lt;/br&gt; We now know how L is sensitive to d and we know how e and c are sensitive to d, we can know put that together to figure out how L is sensitive to e and c. . If a variable z depends on the variable y, which itself depends on the variable x (that is, y and z are dependent variables), then z depends on x as well, via the intermediate variable y. In this case, the chain rule is expressed as . $ frac{dz}{dx} = frac{dz}{dy} * frac{dy}{dx}$ . The chain rule is fundamentally telling you how we chain these derivatives together correctly so to differentiate through a function composition we have to apply a multiplication of those derivatives &lt;/br&gt; The inuitive explanation here is that knowing the instantaneous rate of change of z with respect to y and y relative to x allows one to calculate the instantaneous rate of change of z . If a car travels twice as fast as a bicyle and the cycle is four times as fast as a walking man then the car is 2 * 4 = 8 times faster than the man &lt;/br&gt; . We know the derivative of $ frac{dL}{dd}$ and $ frac{dd}{dc}$ and want to find $ frac{dL}{dc}$ the chain rule tells us that $ frac{dL}{dc} = frac{dL}{dd} * frac{dd}{dc}$ $1.0 * -2.0 = -2.0$ . The chain rule is telling us for plus nodes &quot;+&quot; we are just routing the gradient because the local derivative is just 1.0 . e.grad = -2.0 c.grad = -2.0 . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140614114641424 L data &#45;8.0000 grad 1.0000 140614114641424* * 140614114641424*&#45;&gt;140614114641424 140614114641488 f data &#45;2.0000 grad 4.0000 140614114641488&#45;&gt;140614114641424* 140614114647248 c data 10.0000 grad &#45;2.0000 140614114641232+ + 140614114647248&#45;&gt;140614114641232+ 140614114647312 e data &#45;6.0000 grad &#45;2.0000 140614114647312&#45;&gt;140614114641232+ 140614114647312* * 140614114647312*&#45;&gt;140614114647312 140614114641232 d data 4.0000 grad &#45;2.0000 140614114641232&#45;&gt;140614114641424* 140614114641232+&#45;&gt;140614114641232 140614114640720 a data 2.0000 grad 0.0000 140614114640720&#45;&gt;140614114647312* 140614114640784 b data &#45;3.0000 grad 0.0000 140614114640784&#45;&gt;140614114647312* Lets manually check our work by nudging c . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data a = Value(2.0 , label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) # this is the variable we are nudging by h c = Value(10.0, label=&#39;c&#39;) c.data += h e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . -1.9999999999953388 . As we expected c.grad equals -2.0* . Now we will recurse our way backwards again and going to do our second application of the chain rule . $ frac{dL}{de} = -2.0$ &lt;/br&gt; . $ frac{de}{da} = b$ &lt;/br&gt; . $ frac{dL}{da} = frac{dL}{de} * frac{de}{da}$ . We are multiplying the derivative of e with respect to L with the local gradients . a.grad = -2.0 * -3.0 b.grad = -2.0 * 2.0 . draw_dot(L) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 140614114641424 L data &#45;8.0000 grad 1.0000 140614114641424* * 140614114641424*&#45;&gt;140614114641424 140614114641488 f data &#45;2.0000 grad 4.0000 140614114641488&#45;&gt;140614114641424* 140614114647248 c data 10.0000 grad &#45;2.0000 140614114641232+ + 140614114647248&#45;&gt;140614114641232+ 140614114647312 e data &#45;6.0000 grad &#45;2.0000 140614114647312&#45;&gt;140614114641232+ 140614114647312* * 140614114647312*&#45;&gt;140614114647312 140614114641232 d data 4.0000 grad &#45;2.0000 140614114641232&#45;&gt;140614114641424* 140614114641232+&#45;&gt;140614114641232 140614114640720 a data 2.0000 grad 6.0000 140614114640720&#45;&gt;140614114647312* 140614114640784 b data &#45;3.0000 grad &#45;4.0000 140614114640784&#45;&gt;140614114647312* Lets verify . def lol(): h = 0.0001 a = Value(2.0, label=&#39;a&#39;) b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L1 = L.data # this is the variable we are nudging by h a = Value(2.0 , label=&#39;a&#39;) a.data += h b = Value(-3.0, label=&#39;b&#39;) c = Value(10.0, label=&#39;c&#39;) e = a*b; e.label = &#39;e&#39; d = e + c; d.label = &#39;d&#39; f = Value(-2.0, label=&#39;f&#39;) L = d * f; L.label = &#39;L&#39; L2 = L.data print((L2-L1)/h) lol() . 6.000000000021544 . Checks out . We know know what back propagation is; a recursive application of the chain rule backwards through the computational graph . Neuron Example .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/10/07/micrograd.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/10/07/micrograd.html",
            "date": " ‚Ä¢ Oct 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Spaceship Titanic üö¢",
            "content": "Content: . Data Preprocessing | Binary Splits | Decision Tree | Random Forest | XGBoost | Results | File and Data Field Descriptions . train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data. PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. | HomePlanet - The planet the passenger departed from, typically their planet of permanent residence. | CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. | Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. | Destination - The planet the passenger will be debarking to. | Age - The age of the passenger. | VIP - Whether the passenger has paid for special VIP service during the voyage. | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic&#39;s many luxury amenities. | Name - The first and last names of the passenger. | Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. | . | test.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set. sample_submission.csv - A submission file in the correct format. PassengerId - Id for each passenger in the test set. | Transported - The target. For each passenger, predict either True or False. | . | . . Image made using stable diffusion in the dreamstudio beta . code to help grab the dataset if outside of kaggle, I&#39;ll comment this out so kaggle doesn&#39;t have any issues . from fastkaggle import setup_comp path = setup_comp(&#39;spaceship-titanic&#39;) . Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run &#39;chmod 600 /home/terps/.kaggle/kaggle.json&#39; Downloading spaceship-titanic.zip to /home/terps/.git/anything/_notebooks . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299k/299k [00:00&lt;00:00, 21.1MB/s] . . . Grabbing the training and test data and replacing the Transported boolean column with 0s and 1s . import pandas as pd df = pd.read_csv(&#39;spaceship-titanic/train.csv&#39;) tst_df = pd.read_csv(&#39;spaceship-titanic/test.csv&#39;) modes = df.mode().iloc[0] df[&#39;Transported&#39;] = df[&#39;Transported&#39;].apply(lambda x: 1 if x == True else 0) . . Data preprocessing . To smooth the outliers in RoomService, FoodCourt, ShoppingMall, and Spa; lets take the log of these columns | Both the Cabin and Name column contain extra information that may be useful to our model if we split them into seperate columns, lets do that also | . import numpy as np def add_features(df): df.fillna(modes, inplace=True) df[&#39;LogRoomService&#39;] = np.log(df[&#39;RoomService&#39;]+1) df[&#39;LogFoodCourt&#39;] = np.log(df[&#39;FoodCourt&#39;]+1) df[&#39;LogShoppingMall&#39;] = np.log(df[&#39;ShoppingMall&#39;]+1) df[&#39;LogSpa&#39;] = np.log(df[&#39;Spa&#39;]+1) df[&#39;LogVRDeck&#39;] = np.log(df[&#39;VRDeck&#39;]+1) df[[&#39;Deck&#39;,&#39;CabinNumber&#39;,&#39;Side&#39;]] = df.Cabin.str.split(&#39;/&#39;, expand=True) df[[&#39;Group&#39;, &#39;PassengerNumber&#39;]] = df.PassengerId.str.split(&#39;_&#39;, expand=True) df[[&#39;FirstName&#39;, &#39;LastName&#39;]] = df.Name.str.split(&#39; &#39;, expand=True) add_features(df) add_features(tst_df) . Here we are just using pandas Categorical method to assign numerical codes to the columns . def proc_data(df): df.fillna(modes, inplace=True) df[&#39;HomePlanet&#39;] = pd.Categorical(df.HomePlanet) df[&quot;CryoSleep&quot;] = pd.Categorical(df.CryoSleep) df[&#39;Destination&#39;] = pd.Categorical(df.Destination) df[&#39;Deck&#39;] = pd.Categorical(df.Deck) df[&#39;CabinNumber&#39;] = pd.Categorical(df.HomePlanet) df[&#39;Side&#39;] = pd.Categorical(df.Side) df[&#39;Group&#39;] = pd.Categorical(df.Group) df[&#39;PassengerNumber&#39;] = pd.Categorical(df.PassengerNumber) df[&#39;FirstName&#39;] = pd.Categorical(df.FirstName) df[&#39;LastName&#39;] = pd.Categorical(df.LastName) proc_data(df) proc_data(tst_df) . df.CryoSleep.cat.codes.head() . 0 0 1 0 2 0 3 0 4 0 dtype: int8 . cats=[&quot;HomePlanet&quot;, &quot;CryoSleep&quot;, &quot;Destination&quot;, &quot;Deck&quot;, &quot;CabinNumber&quot;, &quot;Side&quot;, &quot;Group&quot;, &quot;PassengerNumber&quot;, &quot;FirstName&quot;, &quot;LastName&quot;] conts=[&#39;Age&#39;, &#39;LogRoomService&#39;, &#39;LogFoodCourt&#39;, &#39;LogShoppingMall&#39;, &#39;LogSpa&#39;, &#39;LogVRDeck&#39;] dep=&quot;Transported&quot; . . Binary splits . It may be useful to look at a histogram of the transportation rate of a given column and the respective overall count for the column | . import seaborn as sns import matplotlib.pyplot as plt fix,axs = plt.subplots(1, 2, figsize=(11, 5)) sns.barplot(data=df, y=dep, x=&quot;CryoSleep&quot;, ax=axs[0]).set(title=&quot;Transportation rate&quot;) sns.countplot(data=df, x=&#39;CryoSleep&#39;, ax=axs[1]).set(title=&#39;Histogram&#39;) . [Text(0.5, 1.0, &#39;Histogram&#39;)] . Now that we have our dataset cleaned up a bit and columns split into categories, continous, and dependent columns we can split our data randomly into training and validation sets . from numpy import random from sklearn.model_selection import train_test_split random.seed(42) trn_df, val_df = train_test_split(df, test_size=0.25) trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes) val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes) . def xs_y(df): xs = df[cats+conts].copy() return xs, df[dep] if dep in df else None trn_xs, trn_y = xs_y(trn_df) val_xs, val_y = xs_y(val_df) . Lets look at predictions for just passengers who where in cryosleep . preds = val_xs.CryoSleep==1 val_y . 304 1 2697 0 8424 0 1672 1 8458 1 .. 4478 1 2996 1 7760 1 8181 0 8420 0 Name: Transported, Length: 2174, dtype: int64 . We can look at the MAE using these cryosleep predictions . from sklearn.metrics import mean_absolute_error mean_absolute_error(val_y, preds) . 0.2755289788408464 . Not bad, looks like cryosleep is a good indicator of being transported . We can look at numeric columns with boxen and kde plots to see if there are anything that stands out . df_age = trn_df[trn_df.Age&gt;0] fig, axs = plt.subplots(1,2, figsize=(11, 5)) sns.boxenplot(data=df_age, x=dep, y=&quot;Age&quot;, ax=axs[0]) sns.kdeplot(data=df_age, x=&quot;Age&quot;, ax=axs[1]); . Not really conclusive quantiles for age and transportation . We can write a few functions to get scores for our columns . def _side_score(side, y): tot = side.sum() if tot&lt;=1: return 0 return y[side].std()*tot . def score(col, y, split): lhs = col&lt;=split return (_side_score(lhs, y) + _side_score(~lhs, y))/len(y) . score(trn_xs[&quot;CryoSleep&quot;], trn_y, 0.5) . 0.4427274084379482 . score(trn_xs[&#39;Age&#39;], trn_y, 20) . 0.49837479646647287 . def iscore(nm, split): col = trn_xs[nm] return score(col, trn_y, split) from ipywidgets import interact interact(nm=conts, split=15.5)(iscore); . interact(nm=cats, split=2)(iscore); . Instead of manually doing this lets write a function that can automatically find the best split point for all our columns . def min_col(df, nm): col, y = df[nm], df[dep] unq = col.dropna().unique() scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)]) idx = scores.argmin() return unq[idx], scores[idx] min_col(trn_df, &quot;Age&quot;) . (5.0, 0.49588222217088634) . cols = cats+conts {o:min_col(trn_df, o) for o in cols} . {&#39;HomePlanet&#39;: (0, 0.49262732535926257), &#39;CryoSleep&#39;: (0, 0.4427274084379482), &#39;Destination&#39;: (0, 0.4975146234286769), &#39;Deck&#39;: (2, 0.49146596918858604), &#39;CabinNumber&#39;: (0, 0.49262732535926257), &#39;Side&#39;: (0, 0.4972039930912765), &#39;Group&#39;: (2054, 0.4988487272764558), &#39;PassengerNumber&#39;: (0, 0.4982305020633713), &#39;FirstName&#39;: (66, 0.4995954899592851), &#39;LastName&#39;: (2184, 0.49916642378415255), &#39;Age&#39;: (5.0, 0.49588222217088634), &#39;LogRoomService&#39;: (0.0, 0.4712139445780011), &#39;LogFoodCourt&#39;: (0.0, 0.4864171323280328), &#39;LogShoppingMall&#39;: (0.0, 0.4838374399256237), &#39;LogSpa&#39;: (0.0, 0.46960050421496596), &#39;LogVRDeck&#39;: (0.0, 0.47094236980183707)} . . Decision Tree . Lets take a look at a simple decision tree classifier | . from sklearn.tree import DecisionTreeClassifier, export_graphviz m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y); . import graphviz import re def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(&#39;Tree {&#39;, f&#39;Tree {{ size={size}; ratio={ratio}&#39;, s)) . draw_tree(m, trn_xs, size=10) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 CryoSleep ‚â§ 0.5 gini = 0.5 samples = 6519 value = [3233, 3286] 1 LogRoomService ‚â§ 5.85 gini = 0.45 samples = 4267 value = [2825, 1442] 0&#45;&gt;1 True 2 gini = 0.3 samples = 2252 value = [408, 1844] 0&#45;&gt;2 False 3 LogSpa ‚â§ 5.33 gini = 0.48 samples = 3188 value = [1919, 1269] 1&#45;&gt;3 4 gini = 0.27 samples = 1079 value = [906, 173] 1&#45;&gt;4 5 gini = 0.5 samples = 2131 value = [1067, 1064] 3&#45;&gt;5 6 gini = 0.31 samples = 1057 value = [852, 205] 3&#45;&gt;6 We can see that CryoSleep is a important feature in predicting transportation . def gini(cond): act = df.loc[cond, dep] return 1 - act.mean()**2 - (1-act).mean()**2 . gini(df.CryoSleep==0), gini(df.CryoSleep==1) . (0.4455780020566211, 0.2982818967776309) . mae_tree = mean_absolute_error(val_y, m.predict(val_xs)) mae_tree . 0.22309107635694572 . Lets add more nodes to our tree and see if that lowers our MAE . m = DecisionTreeClassifier(min_samples_leaf=50) m.fit(trn_xs, trn_y) . DecisionTreeClassifier(min_samples_leaf=50) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(min_samples_leaf=50) . mean_absolute_error(val_y, m.predict(val_xs)) . 0.22309107635694572 . Nice it does! Lets submit this model . tst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes) tst_xs,_ = xs_y(tst_df) def subm(preds, suff): tst_df[&#39;Transported&#39;] = list(map(lambda x: True if x == 1 else False, preds)) sub_df = tst_df[[&#39;PassengerId&#39;, &#39;Transported&#39;]] sub_df.to_csv(f&#39;sub-{suff}.csv&#39;, index=False) subm(m.predict(tst_xs), &#39;tree&#39;) . . . Random Forest . Lets use an ensemble of decision trees to build a predictive model | . def get_tree(prop=0.75): n = len(trn_y) idxs = random.choice(n, int(n*prop)) return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs]) . trees = [get_tree() for t in range(100)] . all_probs = [t.predict(val_xs) for t in trees] avg_probs = np.stack(all_probs).mean(0) mean_absolute_error(val_y, avg_probs) . 0.26108095676172954 . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(100, min_samples_leaf=10) rf.fit(trn_xs, trn_y) mae = mean_absolute_error(val_y, rf.predict(val_xs)) mae . 0.21205151793928242 . subm(rf.predict(tst_xs), &#39;rf&#39;) . . Cool thing about random forests is the featureimportances method, that allows us to see what our model thinks are good predictive features for our dependent variable . pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . from sklearn.model_selection import cross_val_score . . XG Boost . import xgboost as xgb xg_reg = xgb.XGBClassifier(colsample_bytree = 0.7, learning_rate = 0.001, max_depth = 8, alpha = 1, gamma = 3, n_estimators = 400) scores = cross_val_score(xg_reg, trn_xs, trn_y, cv=5) scores.mean() . 0.8001234762628948 . This prediction score looks solid! Lets submit it to kaggle. . xg_reg.fit(trn_xs, trn_y) subm(xg_reg.predict(tst_xs), &#39;xgb&#39;) . . Lets compare the our models . pd.DataFrame(dict(cols=[&#39;Decision Tree&#39;, &#39;XBG&#39;, &#39;Random Forest&#39;], imp=[1-mae_tree, scores.mean(), 1 - mae])).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . As expected looks like XGB produced the most accurate model .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "date": " ‚Ä¢ Sep 9, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lesson 5 / Chapter 9 questions",
            "content": "What is a continuous variable? . numerical data that have a wide range of &#39;continuous&#39; values (ex: age) | . What is a categorical variable? . discrete levels that correspond to different categories | . Provide two of the words that are used for the possible values of a categorical variable. . Levels or categories | . What is a &quot;dense layer&quot;? . linear layer and one-hot encoding layers represent inputs | . How do entity embeddings reduce memory usage and speed up neural networks? . Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model. | . What kinds of datasets are entity embeddings especially useful for? . When the datasets have features with high levels of cardinality (high dimensionality) | . What are the two main families of machine learning algorithms? . Ensembles of decision trees | Multilayered neural networks learned with SGD(i.e, shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) | . Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas? . The data is ordinal | df.cat.set_categories([&#39;columns&#39;]*, ordered=True, inplace=True) | . Summarize what a decision tree algorithm does. . Asks a series of binary questions about the data and splits until a leaf node where the prediction is returned | . Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? . While a date can be treated as ordinal, some dates are qualitatively different from others in ways that might be useful to the systems we are modeling. | We replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful. | . Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? . No, for time-series data we need our model to be able to predict the future | We want the validation set to be later in time than the training set. | To do this is use np.where, a useful function that returns(as the first element of a tuple) the indices of all True values | . What is pickle and what is it useful for? . Python&#39;s system to save a Python object | Allows of to &#39;freeze&#39; a file | Side note: not a secure process and should trust files you unpickle | . How are mse, samples, and values calculated in the decision tree drawn in this chapter? . mse is the square root of the mean of the pred minus actual squared | samples where just 500 random permutations of indexes | Values is average value for all the records in the node | . How do we deal with outliers, before building a decision tree? . Check for errors in your data, the outliers might be an error(ie. in the bulldozer competition a large number of reported sales happened during 1000) | We can take the log of the numerical / continuous column to smooth the outlier | . How do we handle categorical variables in a decision tree? . Decision trees naturally split data in a way that gives the categorical variables context, but we can also use pandas get_dummies to replace a single categorical variable with multiple one_hot_encoded columns | . What is bagging? . An ensemble technique that aggregators multiple predictions from many models | . What is the difference between max_samples and max_features when creating a random forest? . max_samples defines how many rows to sample for training each tree | max_features defines how many columns to sample at each split point | . If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? . n_estimators is the number of trees we want | You can set the number as high as you have time to train, it will get more accurate with diminishing returns | Yes it can lead to over fitting | . In the section &quot;Creating a Random Forest&quot;, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest? . What is &quot;out-of-bag-error&quot;? . OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. | . Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? . Beneficial when we have only a small amount of training data | We can compare then to the training labels | . Explain why random forests are well suited to answering each of the following question: . How confident are we in our predictions using a particular row of data? We can use the standard deviation of predictions across the trees, instead of just the means | . | For predicting with a particular row of data, what were the most important factors, and how did they influence that - prediction? Year Made and Product Size - use treeinterpreter package to check how the prediction changes as it goes - waterfall plots | . | Which columns are the strongest predictors? The feature importance for our model show that the first few important columns have much higher importance scores that the rest. | . | How do predictions vary as we vary these columns? Look at partial dependence plots | . | . What&#39;s the purpose of removing unimportant variables? . Makes for a simpler, more interpretable model that is easier to maintain and roll out. | . What&#39;s a good type of plot for showing tree interpreter results? . cluster_columns | waterfall plots | . What is the &quot;extrapolation problem&quot;? . Hard fora model to extrapolate to data that&#39;s outside the domain of the training data. This is particularly important to random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better. | . How can you tell if your test or validation set is distributed in a different way than your training set? . We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets. | . Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? . This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. | . What is &quot;boosting&quot;? . We train a model that under-fits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of al the models to get the final prediction. | . How could we use embeddings with a random forest? Would we expect this to help? . Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. | . Why might we not always use a neural net for tabular modeling? . We might not use a NN because they are the hardest and longest to train, and more opaque. Instead, random forests should be the first choice/baseline, and neural networks could be tried to improve these results or add to an ensemble. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "relUrl": "/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "date": " ‚Ä¢ Sep 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lesson 4 / Chapter 10 questions",
            "content": "What is &quot;self-supervised learning&quot;? . Training s model using &#39;labels&#39; that are naturally part of the input data, rather than requiring separate external labels. | For instance: training a model to predict the next word in text. | . test = 4 test . 4 . What is a &quot;language model&quot;? . A model trained to predict the next word in text | . Why is a language model considered self-supervised? . We are not using external labels, but the input data itself. | . What are self-supervised models usually used for? . NLP, Computer Vision | . Why do we fine-tune language models? . Task specific corpus of information | . What are the three steps to create a state-of-the-art text classifier? . Pipeline: Transfer Learning &gt; Train on our task specific data &gt; classification | . How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? . To fine-tune our language model to predict the next word of a movie review | . What are the three steps to prepare your data for a language model? . Training Set, Validation Set, and Testing Set | . What is &quot;tokenization&quot;? Why do we need it? . Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model) | . Name three different approaches to tokenization. . Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning &quot;don&#39;t&quot; into &quot;do n&#39;t&quot;). Generally, punctuation marks are also split into separate tokens. | Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, &quot;occasion&quot; might be tokenized as &quot;o c ca sion.&quot; | Character-based: Split a sentence into its individual characters. | . What is xxbos? . Denotes the &#39;beginning of stream&#39; aka the start of the sentence or text block. | . List four rules that fastai applies to text during tokenization. . use defaults.text_proc_rules to check out the default rules | fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses) | replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it&#39;s repeated, then the character | replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it&#39;s repeated, then the word. | spec_add_spaces: Adds spaces around / and # | . Why are repeated characters replaced with a token showing the number of repetitions and the character that&#39;s repeated? . Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. | . What is &quot;numericalization&quot;? . Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab. | . Why might there be words that are replaced with the &quot;unknown word&quot; token? . All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with &quot;unknown word&quot; | . With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful‚Äîstudents often get this one wrong! Be sure to check your answer on the book&#39;s website.) . a. The dataset is split into 64 mini-streams (batch size) | b. Each batch has 64 rows (batch size) and 64 columns (sequence length) | c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64) | d. The second row of the first batch contains the beginning of the second mini-stream | e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128) | . Why do we need padding for text classification? Why don&#39;t we need it for language modeling? . To collate the batch, in language models it is not needed since all documents are concatenated. | . What does an embedding matrix for NLP contain? What is its shape? . Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. | . What is &quot;perplexity&quot;? . Metric in NLP for language models. It is the exponential of the loss. | . Why do we have to pass the vocabulary of the language model to the classifier data block? . To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning. | . What is &quot;gradual unfreezing&quot;? . This refers to unfreezing one laying at a time and fine-tuning the pre-trained model. | . Why is text generation always likely to be ahead of automatic identification of machine-generated texts? . The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/30/lesson4.html",
            "relUrl": "/fastpages/jupyter/2022/08/30/lesson4.html",
            "date": " ‚Ä¢ Aug 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lesson 3 / Chapter 4 questions",
            "content": "How is a grayscale image represented on a computer? How about a color image? . Images are represented by arrays of pixels values representing content for the image | Greyscale values range from 0-255, from white-black | . How are the files and folders in the MNIST_SAMPLE dataset structured? Why? . Train &amp; Valid -&gt; 3 &amp; 7 | To test our model on out of sample data | . Explain how the &quot;pixel similarity&quot; approach to classifying digits works. . Create an archetype for the class we want to identify (Defined as the pixel wise mean of all the values in the training set) | Calculate the pixel mean wise absolute difference -&gt; classify image based upon lowest distance | . What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. . odd_double = [x * 2 for x in [1,2,3,4] if x % 2 == 0 ] odd_double . [4, 8] . What is a &quot;rank-3 tensor&quot;? . Rank is the dimensionality of the tensor | The easiest way to identify the dimensionality is the number of indices you would need to reference a number within a tensor | a &quot;rank-3 tensor&quot; is a cuboid or stack of matrices | . What is the difference between tensor rank and shape? How do you get the rank from the shape? . Rank is the number of axis or dimensions in a tensor while shape is the size of each axis of a tensor. | . How do you get the rank from the shape? . The length of the tensor&#39;s shape is its rank. | . What are RMSE and L1 norm? . Root Mean Squared Error aka the L2 norm, and Mean absolute difference aka L1 norm, are two commonly used methods of measuring distance, focusing on the magnitudes of the differences. | . How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? . Pytorch and fastai libraries utilize C and CUDA to provide these | . Create a 3√ó3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. . What is broadcasting? . dimension matching to allow operations, PyTorch implements these features to make it easier to perform | . Are metrics generally calculated using the training set, or the validation set? Why? . Validation set, you want to test on out of sample data to see how the model will perform. | . What is SGD? . Stochastic Gradient Decent is the process we use to traverse a function to find the local minimum for space. | . Why does SGD use mini-batches? . computationally efficient | . What are the seven steps in SGD for machine learning? . Initialize the parameters | Calculate the predictions | Calculate the loss | Calculate the gradients | Step the weights | Repeat the process | Stop | . How do we initialize the weights in a model? . Randomness is recommended | . What is &quot;loss&quot;? . A loss is a function that measures your predictions versus the results of the data, large deviations would indicate a large loss while the opposite would also be true. | . Why can&#39;t we always use a high learning rate? . A high learning rate might skip over the minima for our loss space, while fast and computationally less expensive, it may be useful to dynamically adjust your step ** might be explaining step size fix later | . What is a &quot;gradient&quot;? . The first derivative of the loss function | . Do you need to know how to calculate gradients yourself? . No we use our handy tools provided by PyTorch, but it is helpful to understand whats happening under the hood. | . Why can&#39;t we use accuracy as a loss function? . A loss for binomial outcomes (0, 1) would have create a loss function with a derivative 0 | We need to be able to turn the dials and knobs on the weights and see the effect on the predictions | . Draw the sigmoid function. What is special about its shape? . # plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . What is the difference between a loss function and a metric? . The metric is the accuracy of the actual result we want to achieve, the loss tells us how wel | . What is the function to calculate new weights using a learning rate? . The optimizer step method | . What does the DataLoader class do? . Takes any python collection and turns it into an iterator over many batches. | . Write pseudocode showing the basic steps taken in each epoch for SGD. . # pred = model(x) # loss = loss_func(pred, y) # loss.backward() # parameters -= parameters.grad * lr . Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? . def create_tuple(a,b): return list(zip(a,b)) . What does view do in PyTorch? . Changes the shape of the tensor without changing its content | . What are the &quot;bias&quot; parameters in a neural network? Why do we need them? . The margin or padding on the for our output, keeps our model from predicting a zero value | . What does the @ operator do in Python? . Matrix multiplication | . What does the backward method do? . Return the gradients | . Why do we have to zero the gradients? . To avoid adding the previously stored gradient to the current gradient | . What information do we have to pass to Learner? . DataLoader, the model, optimization function, and the loss function | . Show Python or pseudocode for the basic steps of a training loop. . Show Python or pseudocode for the basic steps of a training loop. def train_epoch(model, lr, params): for xb, yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. . Rectified Linear Unit, basically just replaces any negative numbers with 0s in a linear function | . What is an &quot;activation function&quot;? . function that adds non-linearity to our model to decouple each linear equation from the rest, allowing for more complex functions, see universal approximation theorem | . What&#39;s the difference between F.relu and nn.ReLU? . Python function versus a PyTorch module. Can be called as a function | . The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? . Allows for deeper models with less parameters | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/27/lesson-3.html",
            "relUrl": "/fastpages/jupyter/2022/08/27/lesson-3.html",
            "date": " ‚Ä¢ Aug 27, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Hotdog or Not Hotdog",
            "content": "WATCH: @EmilyChangTV chats w/ Jian-Yang about his &quot;Not Hotdog&quot; app. Download it today (really): https://t.co/7N6a1Asfge #SiliconValleyHBO pic.twitter.com/99TBhaiHYk . &mdash; Tech At Bloomberg (@TechAtBloomberg) May 15, 2017 . # This code is only here to check that your internet is enabled. It doesn&#39;t do anything else. # Here&#39;s a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367 import socket,warnings try: socket.setdefaulttimeout(1) socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((&#39;1.1.1.1&#39;, 53)) except socket.error as ex: raise Exception(&quot;STOP: No internet. Click &#39;&gt;|&#39; in top right and set &#39;Internet&#39; switch to on&quot;) . # `!pip install -Uqq &lt;libraries&gt;` upgrades to the latest version of &lt;libraries&gt; # NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities import os iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: !pip install -Uqq fastai duckduckgo_search . In 2015 the idea of creating a computer system that could recognise birds was considered so outrageously challenging that it was the basis of this XKCD joke: . But today, we can do exactly that, in just a few minutes, using entirely free resources! . The basic steps we&#39;ll take are: . Use DuckDuckGo to search for images of &quot;hot dogs&quot; | Use DuckDuckGo to search for images of &quot;not hot dogs&quot; aka &quot;pizza&quot; | Fine-tune a pretrained neural network to recognise these two groups | Try running this model on a picture of a bird and see if it works. | Step 1: Download images of hot dog and not hotdog &#39;pizza&#39; . #import shutil #hutil.rmtree(&quot;/kaggle/working/hotdog_or_not&quot;) #shutil.rmtree(&quot;/kaggle/working/&quot;) . from duckduckgo_search import ddg_images from fastcore.all import * def search_images(term, max_images=30): print(f&quot;Searching for &#39;{term}&#39;&quot;) return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . Let&#39;s start by searching for a bird photo and seeing what kind of result we get. We&#39;ll start by getting URLs from a search: . # If you get a JSON error, just try running it again (it may take a couple of tries). urls = search_images(&#39;hotdog&#39;, max_images=1) urls[0] . ...and then download a URL and take a look at it: . from fastdownload import download_url dest = &#39;hotdog.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Now let&#39;s do the same with &quot;pizza photos&quot;: . download_url(search_images(&#39;pizza&#39;, max_images=1)[0], &#39;pizza.jpg&#39;, show_progress=False) Image.open(&#39;pizza.jpg&#39;).to_thumb(256,256) . Our searches seem to be giving reasonable results, so let&#39;s grab a few examples of each of &quot;hotdog&quot; and &quot;pizza&quot; photos, and save each group of photos to a different folder (I&#39;m also trying to grab a range of lighting conditions here): . searches = &#39;pizza&#39;,&#39;hotdog&#39; path = Path(&#39;hotdog_or_not&#39;) from time import sleep for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) # Pause between searches to avoid over-loading server download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) resize_images(path/o, max_size=400, dest=path/o) . Step 2: Train our model . Some photos might not download correctly which could cause our model training to fail, so we&#39;ll remove them: . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . To train a model, we&#39;ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model -- not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it: . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path, bs=32) dls.show_batch(max_n=8) . Here what each of the DataBlock parameters means: . blocks=(ImageBlock, CategoryBlock), . The inputs to our model are images, and the outputs are categories (in this case, &quot;hotdog&quot; or &quot;pizza&quot;). . get_items=get_image_files, . To find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path). . splitter=RandomSplitter(valid_pct=0.2, seed=42), . Split the data into training and validation sets randomly, using 20% of the data for the validation set. . get_y=parent_label, . The labels (y values) is the name of the parent of each file (i.e. the name of the folder they&#39;re in, which will be bird or forest). . item_tfms=[Resize(192, method=&#39;squish&#39;)] . Before training, resize each image to 192x192 pixels by &quot;squishing&quot; it (as opposed to cropping it). . Now we&#39;re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds...) . fastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we&#39;ll use that. . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) . Generally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run). . &quot;Fine-tuning&quot; a model means that we&#39;re starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it&#39;s important, check out the free fast.ai course. . Step 3: Use our model (and build your own!) . Let&#39;s see what our model thinks about that bird we downloaded at the start: . is_hotdog,_,probs = learn.predict(PILImage.create(&#39;hotdog.jpg&#39;)) print(f&quot;This is a: {is_hotdog}.&quot;) print(f&quot;Probability it&#39;s a hotdog: {probs[0]:.4f}&quot;) . Good job, resnet18. :) . So, as you see, in the space of a few years, creating computer vision classification models has gone from &quot;so hard it&#39;s a joke&quot; to &quot;trivially easy and free&quot;! . It&#39;s not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. It&#39;s moving so fast that even experts in the field have trouble predicting how it&#39;s going to impact society in the coming years. . One thing is clear -- it&#39;s important that we all do our best to understand this technology, because otherwise we&#39;ll get left behind! . Now it&#39;s your turn. Click &quot;Copy &amp; Edit&quot; and try creating your own image classifier using your own image searches! . If you enjoyed this, please consider clicking the &quot;upvote&quot; button in the top-right -- it&#39;s very encouraging to us notebook authors to know when people appreciate our work. .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "relUrl": "/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "date": " ‚Ä¢ Aug 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Greetings traveler, my name is terps and I will be your guide today. Blogging about stuff I don‚Äôt know so I can get to know it better üòÖ data-science -&gt; machine learning -&gt; deep learning, cyber-security research &gt; cryptography &gt; block-chain technology, meta-skills of decision making &amp; problem solving and any other fun topics that I stumble upon. Thanks for visiting you can reach me at admin@terpsfi.xyz if you wanna chat. This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://terpsfi.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://terpsfi.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}