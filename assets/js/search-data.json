{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://terpsfi.xyz/jupyter/2022/09/11/test.html",
            "relUrl": "/jupyter/2022/09/11/test.html",
            "date": " • Sep 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Spaceship Titanic",
            "content": "Content: . Data Preprocessing | Binary Splits | Decision Tree | Random Forest | XGBoost | Results | File and Data Field Descriptions . train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data. PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. | HomePlanet - The planet the passenger departed from, typically their planet of permanent residence. | CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. | Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. | Destination - The planet the passenger will be debarking to. | Age - The age of the passenger. | VIP - Whether the passenger has paid for special VIP service during the voyage. | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic&#39;s many luxury amenities. | Name - The first and last names of the passenger. | Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. | . | test.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set. sample_submission.csv - A submission file in the correct format. PassengerId - Id for each passenger in the test set. | Transported - The target. For each passenger, predict either True or False. | . | . . Image made using stable diffusion in the dreamstudio beta . code to help grab the dataset if outside of kaggle, I&#39;ll comment this out so kaggle doesn&#39;t have any issues . from fastkaggle import setup_comp path = setup_comp(&#39;spaceship-titanic&#39;) . Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run &#39;chmod 600 /home/terps/.kaggle/kaggle.json&#39; Downloading spaceship-titanic.zip to /home/terps/.git/anything/_notebooks . 100%|██████████| 299k/299k [00:00&lt;00:00, 21.1MB/s] . . . Grabbing the training and test data and replacing the Transported boolean column with 0s and 1s . import pandas as pd df = pd.read_csv(&#39;spaceship-titanic/train.csv&#39;) tst_df = pd.read_csv(&#39;spaceship-titanic/test.csv&#39;) modes = df.mode().iloc[0] df[&#39;Transported&#39;] = df[&#39;Transported&#39;].apply(lambda x: 1 if x == True else 0) . . Data preprocessing . To smooth the outliers in RoomService, FoodCourt, ShoppingMall, and Spa; lets take the log of these columns | Both the Cabin and Name column contain extra information that may be useful to our model if we split them into seperate columns, lets do that also | . import numpy as np def add_features(df): df.fillna(modes, inplace=True) df[&#39;LogRoomService&#39;] = np.log(df[&#39;RoomService&#39;]+1) df[&#39;LogFoodCourt&#39;] = np.log(df[&#39;FoodCourt&#39;]+1) df[&#39;LogShoppingMall&#39;] = np.log(df[&#39;ShoppingMall&#39;]+1) df[&#39;LogSpa&#39;] = np.log(df[&#39;Spa&#39;]+1) df[&#39;LogVRDeck&#39;] = np.log(df[&#39;VRDeck&#39;]+1) df[[&#39;Deck&#39;,&#39;CabinNumber&#39;,&#39;Side&#39;]] = df.Cabin.str.split(&#39;/&#39;, expand=True) df[[&#39;Group&#39;, &#39;PassengerNumber&#39;]] = df.PassengerId.str.split(&#39;_&#39;, expand=True) df[[&#39;FirstName&#39;, &#39;LastName&#39;]] = df.Name.str.split(&#39; &#39;, expand=True) add_features(df) add_features(tst_df) . Here we are just using pandas Categorical method to assign numerical codes to the columns . def proc_data(df): df.fillna(modes, inplace=True) df[&#39;HomePlanet&#39;] = pd.Categorical(df.HomePlanet) df[&quot;CryoSleep&quot;] = pd.Categorical(df.CryoSleep) df[&#39;Destination&#39;] = pd.Categorical(df.Destination) df[&#39;Deck&#39;] = pd.Categorical(df.Deck) df[&#39;CabinNumber&#39;] = pd.Categorical(df.HomePlanet) df[&#39;Side&#39;] = pd.Categorical(df.Side) df[&#39;Group&#39;] = pd.Categorical(df.Group) df[&#39;PassengerNumber&#39;] = pd.Categorical(df.PassengerNumber) df[&#39;FirstName&#39;] = pd.Categorical(df.FirstName) df[&#39;LastName&#39;] = pd.Categorical(df.LastName) proc_data(df) proc_data(tst_df) . df.CryoSleep.cat.codes.head() . 0 0 1 0 2 0 3 0 4 0 dtype: int8 . cats=[&quot;HomePlanet&quot;, &quot;CryoSleep&quot;, &quot;Destination&quot;, &quot;Deck&quot;, &quot;CabinNumber&quot;, &quot;Side&quot;, &quot;Group&quot;, &quot;PassengerNumber&quot;, &quot;FirstName&quot;, &quot;LastName&quot;] conts=[&#39;Age&#39;, &#39;LogRoomService&#39;, &#39;LogFoodCourt&#39;, &#39;LogShoppingMall&#39;, &#39;LogSpa&#39;, &#39;LogVRDeck&#39;] dep=&quot;Transported&quot; . . Binary splits . It may be useful to look at a histogram of the transportation rate of a given column and the respective overall count for the column | . import seaborn as sns import matplotlib.pyplot as plt fix,axs = plt.subplots(1, 2, figsize=(11, 5)) sns.barplot(data=df, y=dep, x=&quot;CryoSleep&quot;, ax=axs[0]).set(title=&quot;Transportation rate&quot;) sns.countplot(data=df, x=&#39;CryoSleep&#39;, ax=axs[1]).set(title=&#39;Histogram&#39;) . [Text(0.5, 1.0, &#39;Histogram&#39;)] . Now that we have our dataset cleaned up a bit and columns split into categories, continous, and dependent columns we can split our data randomly into training and validation sets . from numpy import random from sklearn.model_selection import train_test_split random.seed(42) trn_df, val_df = train_test_split(df, test_size=0.25) trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes) val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes) . def xs_y(df): xs = df[cats+conts].copy() return xs, df[dep] if dep in df else None trn_xs, trn_y = xs_y(trn_df) val_xs, val_y = xs_y(val_df) . Lets look at predictions for just passengers who where in cryosleep . preds = val_xs.CryoSleep==1 val_y . 304 1 2697 0 8424 0 1672 1 8458 1 .. 4478 1 2996 1 7760 1 8181 0 8420 0 Name: Transported, Length: 2174, dtype: int64 . We can look at the MAE using these cryosleep predictions . from sklearn.metrics import mean_absolute_error mean_absolute_error(val_y, preds) . 0.2755289788408464 . Not bad, looks like cryosleep is a good indicator of being transported . We can look at numeric columns with boxen and kde plots to see if there are anything that stands out . df_age = trn_df[trn_df.Age&gt;0] fig, axs = plt.subplots(1,2, figsize=(11, 5)) sns.boxenplot(data=df_age, x=dep, y=&quot;Age&quot;, ax=axs[0]) sns.kdeplot(data=df_age, x=&quot;Age&quot;, ax=axs[1]); . Not really conclusive quantiles for age and transportation . We can write a few functions to get scores for our columns . def _side_score(side, y): tot = side.sum() if tot&lt;=1: return 0 return y[side].std()*tot . def score(col, y, split): lhs = col&lt;=split return (_side_score(lhs, y) + _side_score(~lhs, y))/len(y) . score(trn_xs[&quot;CryoSleep&quot;], trn_y, 0.5) . 0.4427274084379482 . score(trn_xs[&#39;Age&#39;], trn_y, 20) . 0.49837479646647287 . def iscore(nm, split): col = trn_xs[nm] return score(col, trn_y, split) from ipywidgets import interact interact(nm=conts, split=15.5)(iscore); . interact(nm=cats, split=2)(iscore); . Instead of manually doing this lets write a function that can automatically find the best split point for all our columns . def min_col(df, nm): col, y = df[nm], df[dep] unq = col.dropna().unique() scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)]) idx = scores.argmin() return unq[idx], scores[idx] min_col(trn_df, &quot;Age&quot;) . (5.0, 0.49588222217088634) . cols = cats+conts {o:min_col(trn_df, o) for o in cols} . {&#39;HomePlanet&#39;: (0, 0.49262732535926257), &#39;CryoSleep&#39;: (0, 0.4427274084379482), &#39;Destination&#39;: (0, 0.4975146234286769), &#39;Deck&#39;: (2, 0.49146596918858604), &#39;CabinNumber&#39;: (0, 0.49262732535926257), &#39;Side&#39;: (0, 0.4972039930912765), &#39;Group&#39;: (2054, 0.4988487272764558), &#39;PassengerNumber&#39;: (0, 0.4982305020633713), &#39;FirstName&#39;: (66, 0.4995954899592851), &#39;LastName&#39;: (2184, 0.49916642378415255), &#39;Age&#39;: (5.0, 0.49588222217088634), &#39;LogRoomService&#39;: (0.0, 0.4712139445780011), &#39;LogFoodCourt&#39;: (0.0, 0.4864171323280328), &#39;LogShoppingMall&#39;: (0.0, 0.4838374399256237), &#39;LogSpa&#39;: (0.0, 0.46960050421496596), &#39;LogVRDeck&#39;: (0.0, 0.47094236980183707)} . . Decision Tree . Lets take a look at a simple decision tree classifier | . from sklearn.tree import DecisionTreeClassifier, export_graphviz m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y); . import graphviz import re def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(&#39;Tree {&#39;, f&#39;Tree {{ size={size}; ratio={ratio}&#39;, s)) . draw_tree(m, trn_xs, size=10) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 CryoSleep ≤ 0.5 gini = 0.5 samples = 6519 value = [3233, 3286] 1 LogRoomService ≤ 5.85 gini = 0.45 samples = 4267 value = [2825, 1442] 0&#45;&gt;1 True 2 gini = 0.3 samples = 2252 value = [408, 1844] 0&#45;&gt;2 False 3 LogSpa ≤ 5.33 gini = 0.48 samples = 3188 value = [1919, 1269] 1&#45;&gt;3 4 gini = 0.27 samples = 1079 value = [906, 173] 1&#45;&gt;4 5 gini = 0.5 samples = 2131 value = [1067, 1064] 3&#45;&gt;5 6 gini = 0.31 samples = 1057 value = [852, 205] 3&#45;&gt;6 We can see that CryoSleep is a important feature in predicting transportation . def gini(cond): act = df.loc[cond, dep] return 1 - act.mean()**2 - (1-act).mean()**2 . gini(df.CryoSleep==0), gini(df.CryoSleep==1) . (0.4455780020566211, 0.2982818967776309) . mae_tree = mean_absolute_error(val_y, m.predict(val_xs)) mae_tree . 0.22309107635694572 . Lets add more nodes to our tree and see if that lowers our MAE . m = DecisionTreeClassifier(min_samples_leaf=50) m.fit(trn_xs, trn_y) . DecisionTreeClassifier(min_samples_leaf=50) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(min_samples_leaf=50) . mean_absolute_error(val_y, m.predict(val_xs)) . 0.22309107635694572 . Nice it does! Lets submit this model . tst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes) tst_xs,_ = xs_y(tst_df) def subm(preds, suff): tst_df[&#39;Transported&#39;] = list(map(lambda x: True if x == 1 else False, preds)) sub_df = tst_df[[&#39;PassengerId&#39;, &#39;Transported&#39;]] sub_df.to_csv(f&#39;sub-{suff}.csv&#39;, index=False) subm(m.predict(tst_xs), &#39;tree&#39;) . . . Random Forest . Lets use an ensemble of decision trees to build a predictive model | . def get_tree(prop=0.75): n = len(trn_y) idxs = random.choice(n, int(n*prop)) return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs]) . trees = [get_tree() for t in range(100)] . all_probs = [t.predict(val_xs) for t in trees] avg_probs = np.stack(all_probs).mean(0) mean_absolute_error(val_y, avg_probs) . 0.26108095676172954 . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(100, min_samples_leaf=10) rf.fit(trn_xs, trn_y) mae = mean_absolute_error(val_y, rf.predict(val_xs)) mae . 0.21205151793928242 . subm(rf.predict(tst_xs), &#39;rf&#39;) . . Cool thing about random forests is the featureimportances method, that allows us to see what our model thinks are good predictive features for our dependent variable . pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . from sklearn.model_selection import cross_val_score . . XG Boost . import xgboost as xgb xg_reg = xgb.XGBClassifier(colsample_bytree = 0.7, learning_rate = 0.001, max_depth = 8, alpha = 1, gamma = 3, n_estimators = 400) scores = cross_val_score(xg_reg, trn_xs, trn_y, cv=5) scores.mean() . 0.8001234762628948 . This prediction score looks solid! Lets submit it to kaggle. . xg_reg.fit(trn_xs, trn_y) subm(xg_reg.predict(tst_xs), &#39;xgb&#39;) . . Lets compare the our models . pd.DataFrame(dict(cols=[&#39;Decision Tree&#39;, &#39;XBG&#39;, &#39;Random Forest&#39;], imp=[1-mae_tree, scores.mean(), 1 - mae])).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "relUrl": "/fastpages/jupyter/kaggle/2022/09/09/eda-random-forest-xbg-oh-my.html",
            "date": " • Sep 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lesson 5 / Chapter 9 questions",
            "content": "What is a continuous variable? . numerical data that have a wide range of &#39;continuous&#39; values (ex: age) | . What is a categorical variable? . discrete levels that correspond to different categories | . Provide two of the words that are used for the possible values of a categorical variable. . Levels or categories | . What is a &quot;dense layer&quot;? . linear layer and one-hot encoding layers represent inputs | . How do entity embeddings reduce memory usage and speed up neural networks? . Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model. | . What kinds of datasets are entity embeddings especially useful for? . When the datasets have features with high levels of cardinality (high dimensionality) | . What are the two main families of machine learning algorithms? . Ensembles of decision trees | Multilayered neural networks learned with SGD(i.e, shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) | . Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas? . The data is ordinal | df.cat.set_categories([&#39;columns&#39;]*, ordered=True, inplace=True) | . Summarize what a decision tree algorithm does. . Asks a series of binary questions about the data and splits until a leaf node where the prediction is returned | . Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? . While a date can be treated as ordinal, some dates are qualitatively different from others in ways that might be useful to the systems we are modeling. | We replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful. | . Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? . No, for time-series data we need our model to be able to predict the future | We want the validation set to be later in time than the training set. | To do this is use np.where, a useful function that returns(as the first element of a tuple) the indices of all True values | . What is pickle and what is it useful for? . Python&#39;s system to save a Python object | Allows of to &#39;freeze&#39; a file | Side note: not a secure process and should trust files you unpickle | . How are mse, samples, and values calculated in the decision tree drawn in this chapter? . mse is the square root of the mean of the pred minus actual squared | samples where just 500 random permutations of indexes | Values is average value for all the records in the node | . How do we deal with outliers, before building a decision tree? . Check for errors in your data, the outliers might be an error(ie. in the bulldozer competition a large number of reported sales happened during 1000) | We can take the log of the numerical / continuous column to smooth the outlier | . How do we handle categorical variables in a decision tree? . Decision trees naturally split data in a way that gives the categorical variables context, but we can also use pandas get_dummies to replace a single categorical variable with multiple one_hot_encoded columns | . What is bagging? . An ensemble technique that aggregators multiple predictions from many models | . What is the difference between max_samples and max_features when creating a random forest? . max_samples defines how many rows to sample for training each tree | max_features defines how many columns to sample at each split point | . If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? . n_estimators is the number of trees we want | You can set the number as high as you have time to train, it will get more accurate with diminishing returns | Yes it can lead to over fitting | . In the section &quot;Creating a Random Forest&quot;, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest? . What is &quot;out-of-bag-error&quot;? . OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. | . Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? . Beneficial when we have only a small amount of training data | We can compare then to the training labels | . Explain why random forests are well suited to answering each of the following question: . How confident are we in our predictions using a particular row of data? We can use the standard deviation of predictions across the trees, instead of just the means | . | For predicting with a particular row of data, what were the most important factors, and how did they influence that - prediction? Year Made and Product Size - use treeinterpreter package to check how the prediction changes as it goes - waterfall plots | . | Which columns are the strongest predictors? The feature importance for our model show that the first few important columns have much higher importance scores that the rest. | . | How do predictions vary as we vary these columns? Look at partial dependence plots | . | . What&#39;s the purpose of removing unimportant variables? . Makes for a simpler, more interpretable model that is easier to maintain and roll out. | . What&#39;s a good type of plot for showing tree interpreter results? . cluster_columns | waterfall plots | . What is the &quot;extrapolation problem&quot;? . Hard fora model to extrapolate to data that&#39;s outside the domain of the training data. This is particularly important to random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better. | . How can you tell if your test or validation set is distributed in a different way than your training set? . We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets. | . Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? . This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. | . What is &quot;boosting&quot;? . We train a model that under-fits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of al the models to get the final prediction. | . How could we use embeddings with a random forest? Would we expect this to help? . Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. | . Why might we not always use a neural net for tabular modeling? . We might not use a NN because they are the hardest and longest to train, and more opaque. Instead, random forests should be the first choice/baseline, and neural networks could be tried to improve these results or add to an ensemble. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "relUrl": "/fastpages/jupyter/2022/09/02/chapter9-qa.html",
            "date": " • Sep 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Lesson 4 / Chapter 10 questions",
            "content": "What is &quot;self-supervised learning&quot;? . Training s model using &#39;labels&#39; that are naturally part of the input data, rather than requiring separate external labels. | For instance: training a model to predict the next word in text. | . test = 4 test . 4 . What is a &quot;language model&quot;? . A model trained to predict the next word in text | . Why is a language model considered self-supervised? . We are not using external labels, but the input data itself. | . What are self-supervised models usually used for? . NLP, Computer Vision | . Why do we fine-tune language models? . Task specific corpus of information | . What are the three steps to create a state-of-the-art text classifier? . Pipeline: Transfer Learning &gt; Train on our task specific data &gt; classification | . How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? . To fine-tune our language model to predict the next word of a movie review | . What are the three steps to prepare your data for a language model? . Training Set, Validation Set, and Testing Set | . What is &quot;tokenization&quot;? Why do we need it? . Convert the text into a list of words(or chars, or substrings, depending on the granularity of your model) | . Name three different approaches to tokenization. . Word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning &quot;don&#39;t&quot; into &quot;do n&#39;t&quot;). Generally, punctuation marks are also split into separate tokens. | Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, &quot;occasion&quot; might be tokenized as &quot;o c ca sion.&quot; | Character-based: Split a sentence into its individual characters. | . What is xxbos? . Denotes the &#39;beginning of stream&#39; aka the start of the sentence or text block. | . List four rules that fastai applies to text during tokenization. . use defaults.text_proc_rules to check out the default rules | fit_html: Replaces special HTML characters with a readable version(IMDB reviews have quite a few of theses) | replace_rep: Replaces any character repeated three times or more with a special token for repetition(xxrep), the number of times it&#39;s repeated, then the character | replace_wrep: Replaces any word repeated three times or more with a special token for word repetition(xxwrep), the number of times it&#39;s repeated, then the word. | spec_add_spaces: Adds spaces around / and # | . Why are repeated characters replaced with a token showing the number of repetitions and the character that&#39;s repeated? . Allows the models embedding matrix to encode information about general concepts, such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. | . What is &quot;numericalization&quot;? . Make a list of all the unique words that apepar(the vocab), and convert each word into a number, by looking up its index in the vocab. | . Why might there be words that are replaced with the &quot;unknown word&quot; token? . All the words in the embedding matrix can have a token associated with them(too large), only words with that occur more than the min_freq have an assigned token, other are replaced with &quot;unknown word&quot; | . With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book&#39;s website.) . a. The dataset is split into 64 mini-streams (batch size) | b. Each batch has 64 rows (batch size) and 64 columns (sequence length) | c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64) | d. The second row of the first batch contains the beginning of the second mini-stream | e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128) | . Why do we need padding for text classification? Why don&#39;t we need it for language modeling? . To collate the batch, in language models it is not needed since all documents are concatenated. | . What does an embedding matrix for NLP contain? What is its shape? . Contains a vector representation of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. | . What is &quot;perplexity&quot;? . Metric in NLP for language models. It is the exponential of the loss. | . Why do we have to pass the vocabulary of the language model to the classifier data block? . To ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LRM fine-tuning. | . What is &quot;gradual unfreezing&quot;? . This refers to unfreezing one laying at a time and fine-tuning the pre-trained model. | . Why is text generation always likely to be ahead of automatic identification of machine-generated texts? . The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead. | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/30/lesson4.html",
            "relUrl": "/fastpages/jupyter/2022/08/30/lesson4.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lesson 3 / Chapter 4 questions",
            "content": "How is a grayscale image represented on a computer? How about a color image? . Images are represented by arrays of pixels values representing content for the image | Greyscale values range from 0-255, from white-black | . How are the files and folders in the MNIST_SAMPLE dataset structured? Why? . Train &amp; Valid -&gt; 3 &amp; 7 | To test our model on out of sample data | . Explain how the &quot;pixel similarity&quot; approach to classifying digits works. . Create an archetype for the class we want to identify (Defined as the pixel wise mean of all the values in the training set) | Calculate the pixel mean wise absolute difference -&gt; classify image based upon lowest distance | . What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. . odd_double = [x * 2 for x in [1,2,3,4] if x % 2 == 0 ] odd_double . [4, 8] . What is a &quot;rank-3 tensor&quot;? . Rank is the dimensionality of the tensor | The easiest way to identify the dimensionality is the number of indices you would need to reference a number within a tensor | a &quot;rank-3 tensor&quot; is a cuboid or stack of matrices | . What is the difference between tensor rank and shape? How do you get the rank from the shape? . Rank is the number of axis or dimensions in a tensor while shape is the size of each axis of a tensor. | . How do you get the rank from the shape? . The length of the tensor&#39;s shape is its rank. | . What are RMSE and L1 norm? . Root Mean Squared Error aka the L2 norm, and Mean absolute difference aka L1 norm, are two commonly used methods of measuring distance, focusing on the magnitudes of the differences. | . How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? . Pytorch and fastai libraries utilize C and CUDA to provide these | . Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. . What is broadcasting? . dimension matching to allow operations, PyTorch implements these features to make it easier to perform | . Are metrics generally calculated using the training set, or the validation set? Why? . Validation set, you want to test on out of sample data to see how the model will perform. | . What is SGD? . Stochastic Gradient Decent is the process we use to traverse a function to find the local minimum for space. | . Why does SGD use mini-batches? . computationally efficient | . What are the seven steps in SGD for machine learning? . Initialize the parameters | Calculate the predictions | Calculate the loss | Calculate the gradients | Step the weights | Repeat the process | Stop | . How do we initialize the weights in a model? . Randomness is recommended | . What is &quot;loss&quot;? . A loss is a function that measures your predictions versus the results of the data, large deviations would indicate a large loss while the opposite would also be true. | . Why can&#39;t we always use a high learning rate? . A high learning rate might skip over the minima for our loss space, while fast and computationally less expensive, it may be useful to dynamically adjust your step ** might be explaining step size fix later | . What is a &quot;gradient&quot;? . The first derivative of the loss function | . Do you need to know how to calculate gradients yourself? . No we use our handy tools provided by PyTorch, but it is helpful to understand whats happening under the hood. | . Why can&#39;t we use accuracy as a loss function? . A loss for binomial outcomes (0, 1) would have create a loss function with a derivative 0 | We need to be able to turn the dials and knobs on the weights and see the effect on the predictions | . Draw the sigmoid function. What is special about its shape? . # plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . What is the difference between a loss function and a metric? . The metric is the accuracy of the actual result we want to achieve, the loss tells us how wel | . What is the function to calculate new weights using a learning rate? . The optimizer step method | . What does the DataLoader class do? . Takes any python collection and turns it into an iterator over many batches. | . Write pseudocode showing the basic steps taken in each epoch for SGD. . # pred = model(x) # loss = loss_func(pred, y) # loss.backward() # parameters -= parameters.grad * lr . Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? . def create_tuple(a,b): return list(zip(a,b)) . What does view do in PyTorch? . Changes the shape of the tensor without changing its content | . What are the &quot;bias&quot; parameters in a neural network? Why do we need them? . The margin or padding on the for our output, keeps our model from predicting a zero value | . What does the @ operator do in Python? . Matrix multiplication | . What does the backward method do? . Return the gradients | . Why do we have to zero the gradients? . To avoid adding the previously stored gradient to the current gradient | . What information do we have to pass to Learner? . DataLoader, the model, optimization function, and the loss function | . Show Python or pseudocode for the basic steps of a training loop. . Show Python or pseudocode for the basic steps of a training loop. def train_epoch(model, lr, params): for xb, yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. . Rectified Linear Unit, basically just replaces any negative numbers with 0s in a linear function | . What is an &quot;activation function&quot;? . function that adds non-linearity to our model to decouple each linear equation from the rest, allowing for more complex functions, see universal approximation theorem | . What&#39;s the difference between F.relu and nn.ReLU? . Python function versus a PyTorch module. Can be called as a function | . The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? . Allows for deeper models with less parameters | .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/27/lesson-3.html",
            "relUrl": "/fastpages/jupyter/2022/08/27/lesson-3.html",
            "date": " • Aug 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Zeroth Lecture",
            "content": "Zeroth Lecture . Meta-Learning and Set-up . This lecture was super insightful and contained some awesome tricks and tools to suceed in the practical deep learning course. . The biggest take away and process I will attempt to follow is this how to: . watch Lecture | run notebook &amp; experiment | reproduce results | repeat with different dataset | . In addition to reviewing and reflecting on the information in flashcard (aiquizzes.com), and writting this blog! . Really look forward to taking MIT’s missing semester course covering: . code concepts | editor | git &amp; gh | ssh / Linux | . Speaking of linux and ssh, towards the end of the lesson we got a look at spinning up a AWS server to get some juicy GPU’s to help us build our models in the future. I’ve fooled around with a rasberry-pi in the past but besides that have little experience with rsa, ssh, and managing remote systems. Feel like a big hackerman .",
            "url": "https://terpsfi.xyz/markdown/2022/08/25/zeroth.html",
            "relUrl": "/markdown/2022/08/25/zeroth.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Hotdog or Not Hotdog",
            "content": "WATCH: @EmilyChangTV chats w/ Jian-Yang about his &quot;Not Hotdog&quot; app. Download it today (really): https://t.co/7N6a1Asfge #SiliconValleyHBO pic.twitter.com/99TBhaiHYk . &mdash; Tech At Bloomberg (@TechAtBloomberg) May 15, 2017 . # This code is only here to check that your internet is enabled. It doesn&#39;t do anything else. # Here&#39;s a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367 import socket,warnings try: socket.setdefaulttimeout(1) socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((&#39;1.1.1.1&#39;, 53)) except socket.error as ex: raise Exception(&quot;STOP: No internet. Click &#39;&gt;|&#39; in top right and set &#39;Internet&#39; switch to on&quot;) . # `!pip install -Uqq &lt;libraries&gt;` upgrades to the latest version of &lt;libraries&gt; # NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities import os iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: !pip install -Uqq fastai duckduckgo_search . In 2015 the idea of creating a computer system that could recognise birds was considered so outrageously challenging that it was the basis of this XKCD joke: . But today, we can do exactly that, in just a few minutes, using entirely free resources! . The basic steps we&#39;ll take are: . Use DuckDuckGo to search for images of &quot;hot dogs&quot; | Use DuckDuckGo to search for images of &quot;not hot dogs&quot; aka &quot;pizza&quot; | Fine-tune a pretrained neural network to recognise these two groups | Try running this model on a picture of a bird and see if it works. | Step 1: Download images of hot dog and not hotdog &#39;pizza&#39; . #import shutil #hutil.rmtree(&quot;/kaggle/working/hotdog_or_not&quot;) #shutil.rmtree(&quot;/kaggle/working/&quot;) . from duckduckgo_search import ddg_images from fastcore.all import * def search_images(term, max_images=30): print(f&quot;Searching for &#39;{term}&#39;&quot;) return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . Let&#39;s start by searching for a bird photo and seeing what kind of result we get. We&#39;ll start by getting URLs from a search: . # If you get a JSON error, just try running it again (it may take a couple of tries). urls = search_images(&#39;hotdog&#39;, max_images=1) urls[0] . ...and then download a URL and take a look at it: . from fastdownload import download_url dest = &#39;hotdog.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Now let&#39;s do the same with &quot;pizza photos&quot;: . download_url(search_images(&#39;pizza&#39;, max_images=1)[0], &#39;pizza.jpg&#39;, show_progress=False) Image.open(&#39;pizza.jpg&#39;).to_thumb(256,256) . Our searches seem to be giving reasonable results, so let&#39;s grab a few examples of each of &quot;hotdog&quot; and &quot;pizza&quot; photos, and save each group of photos to a different folder (I&#39;m also trying to grab a range of lighting conditions here): . searches = &#39;pizza&#39;,&#39;hotdog&#39; path = Path(&#39;hotdog_or_not&#39;) from time import sleep for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) # Pause between searches to avoid over-loading server download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) download_images(dest, urls=search_images(f&#39;{o}&#39;)) sleep(10) resize_images(path/o, max_size=400, dest=path/o) . Step 2: Train our model . Some photos might not download correctly which could cause our model training to fail, so we&#39;ll remove them: . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . To train a model, we&#39;ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model -- not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it: . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path, bs=32) dls.show_batch(max_n=8) . Here what each of the DataBlock parameters means: . blocks=(ImageBlock, CategoryBlock), . The inputs to our model are images, and the outputs are categories (in this case, &quot;hotdog&quot; or &quot;pizza&quot;). . get_items=get_image_files, . To find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path). . splitter=RandomSplitter(valid_pct=0.2, seed=42), . Split the data into training and validation sets randomly, using 20% of the data for the validation set. . get_y=parent_label, . The labels (y values) is the name of the parent of each file (i.e. the name of the folder they&#39;re in, which will be bird or forest). . item_tfms=[Resize(192, method=&#39;squish&#39;)] . Before training, resize each image to 192x192 pixels by &quot;squishing&quot; it (as opposed to cropping it). . Now we&#39;re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds...) . fastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we&#39;ll use that. . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) . Generally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run). . &quot;Fine-tuning&quot; a model means that we&#39;re starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it&#39;s important, check out the free fast.ai course. . Step 3: Use our model (and build your own!) . Let&#39;s see what our model thinks about that bird we downloaded at the start: . is_hotdog,_,probs = learn.predict(PILImage.create(&#39;hotdog.jpg&#39;)) print(f&quot;This is a: {is_hotdog}.&quot;) print(f&quot;Probability it&#39;s a hotdog: {probs[0]:.4f}&quot;) . Good job, resnet18. :) . So, as you see, in the space of a few years, creating computer vision classification models has gone from &quot;so hard it&#39;s a joke&quot; to &quot;trivially easy and free&quot;! . It&#39;s not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. It&#39;s moving so fast that even experts in the field have trouble predicting how it&#39;s going to impact society in the coming years. . One thing is clear -- it&#39;s important that we all do our best to understand this technology, because otherwise we&#39;ll get left behind! . Now it&#39;s your turn. Click &quot;Copy &amp; Edit&quot; and try creating your own image classifier using your own image searches! . If you enjoyed this, please consider clicking the &quot;upvote&quot; button in the top-right -- it&#39;s very encouraging to us notebook authors to know when people appreciate our work. .",
            "url": "https://terpsfi.xyz/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "relUrl": "/fastpages/jupyter/2022/08/22/not-hotdog.html",
            "date": " • Aug 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://terpsfi.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://terpsfi.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}